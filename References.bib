@MISC{Baudelaire1955-ae,
  title        = "The Mirror of Art: Critical Studies",
  author       = "Baudelaire, Charles and (trans), Jonathon Mayne",
  booktitle    = "Phaidon Press Ltd., London",
  abstract     = "Hardcover - Phaidon Press Ltd., London - 1955 - Condition:
                  Good - The collected critical writing of Baudelaire, including
                  his Salons of 1845, 1846 and 1859 plus his Essence of Laughter
                  and other essays. With numerous b/w plates. Spine is sunned.
                  Contents clean and tight. Good+ - The Mirror of Art: Critical
                  Studies",
  year         =  1955,
  howpublished = "\url{https://www.abebooks.com/Mirror-Art-Critical-Studies-Charles-Baudelaire/30932257309/bd}",
  note         = "Accessed: 2024-12-6",
  language     = "en"
}

@MISC{UnknownUnknown-fl,
  title        = "Charles Baudelaire, “On Photography,” from The Salon of 1859",
  howpublished = "\url{https://www.csus.edu/indiv/o/obriene/art109/readings/11\%20baudelaire\%20photography.htm}",
  note         = "Accessed: 2024-12-6"
}

@INPROCEEDINGS{Zhang2023-fy,
  title     = "Towards human-centred {AI}-co-creation: A three-level framework
               for effective collaboration between human and {AI}",
  author    = "Zhang, Mingyuan and Cheng, Zhaolin and Shiu, Sheung Ting Ramona
               and Liang, Jiacheng and Fang, Cong and Ma, Zhengtao and Fang, Le
               and Wang, Stephen Jia",
  booktitle = "Computer Supported Cooperative Work and Social Computing",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  oct,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Haase2024-yp,
  title    = "Human-{AI} Co-Creativity: Exploring Synergies Across Levels of
              Creative Collaboration",
  author   = "Haase, Jennifer and Pokutta, Sebastian",
  abstract = "Human-AI co-creativity represents a transformative shift in how
              humans and generative AI tools collaborate in creative processes.
              This chapter explores the synergies between human ingenuity and AI
              capabilities across four levels of interaction: Digital Pen, AI
              Task Specialist, AI Assistant, and AI Co-Creator. While earlier
              digital tools primarily facilitated creativity, generative AI
              systems now contribute actively, demonstrating autonomous
              creativity in producing novel and valuable outcomes. Empirical
              evidence from mathematics showcases how AI can extend human
              creative potential, from computational problem-solving to
              co-creative partnerships yielding breakthroughs in longstanding
              challenges. By analyzing these collaborations, the chapter
              highlights AI's potential to enhance human creativity without
              replacing it, underscoring the importance of balancing AI's
              contributions with human oversight and contextual understanding.
              This integration pushes the boundaries of creative achievements,
              emphasizing the need for human-centered AI systems that foster
              collaboration while preserving the unique qualities of human
              creativity.",
  month    =  nov,
  year     =  2024
}

@ARTICLE{Nails2005-iq,
  title  = "Socrates",
  author = "Nails, Debra and Monoson, S Sara",
  month  =  sep,
  year   =  2005
}

@ARTICLE{Zank2004-dh,
  title  = "Martin Buber",
  author = "Zank, Michael and Braiterman, Zachary",
  month  =  apr,
  year   =  2004
}

@ARTICLE{Kobis2021-bb,
  title     = "Artificial intelligence versus Maya Angelou: Experimental
               evidence that people cannot differentiate {AI}-generated from
               human-written poetry",
  author    = "Köbis, Nils and Mossink, Luca D",
  journal   = "Comput. Human Behav.",
  publisher = "Elsevier BV",
  volume    =  114,
  number    =  106553,
  pages     =  106553,
  abstract  = "The release of openly available, robust natural language
               generation algorithms (NLG) has spurred much public attention and
               debate. One reason lies in …",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Vaccaro2024-ne,
  title     = "When combinations of humans and {AI} are useful: A systematic
               review and meta-analysis",
  author    = "Vaccaro, Michelle and Almaatouq, Abdullah and Malone, Thomas",
  journal   = "Nat. Hum. Behav.",
  publisher = "Nature Publishing Group",
  pages     = "1--11",
  abstract  = "Inspired by the increasing use of artificial intelligence (AI) to
               augment humans, researchers have studied human-AI systems
               involving different tasks, systems and populations. Despite such
               a large body of work, we lack a broad conceptual understanding of
               when combinations of humans and AI are better than either alone.
               Here we addressed this question by conducting a preregistered
               systematic review and meta-analysis of 106 experimental studies
               reporting 370 effect sizes. We searched an interdisciplinary set
               of databases (the Association for Computing Machinery Digital
               Library, the Web of Science and the Association for Information
               Systems eLibrary) for studies published between 1 January 2020
               and 30 June 2023. Each study was required to include an original
               human-participants experiment that evaluated the performance of
               humans alone, AI alone and human-AI combinations. First, we found
               that, on average, human-AI combinations performed significantly
               worse than the best of humans or AI alone (Hedges' g = -0.23;
               95\% confidence interval, -0.39 to -0.07). Second, we found
               performance losses in tasks that involved making decisions and
               significantly greater gains in tasks that involved creating
               content. Finally, when humans outperformed AI alone, we found
               performance gains in the combination, but when AI outperformed
               humans alone, we found losses. Limitations of the evidence
               assessed here include possible publication bias and variations in
               the study designs analysed. Overall, these findings highlight the
               heterogeneity of the effects of human-AI collaboration and point
               to promising avenues for improving human-AI systems.",
  month     =  oct,
  year      =  2024,
  language  = "en"
}

@BOOK{Esposito2022-ml,
  title     = "Artificial Communication: How algorithms produce social
               intelligence",
  author    = "Esposito, Elena",
  publisher = "The MIT Press",
  abstract  = "A proposal that we think about digital technologies such as
               machine learning not in terms of artificial intelligence but as
               artificial communication. Algorithms that work with deep learning
               and big data are getting so much better at doing so many things
               that it makes us uncomfortable. How can a device know what our
               favorite songs are, or what we should write in an email? Have
               machines become too smart? In Artificial Communication, Elena
               Esposito argues that drawing this sort of analogy between
               algorithms and human intelligence is misleading. If machines
               contribute to social intelligence, it will not be because they
               have learned how to think like us but because we have learned how
               to communicate with them. Esposito proposes that we think of
               “smart” machines not in terms of artificial intelligence but in
               terms of artificial communication. To do this, we need a concept
               of communication that can take into account the possibility that
               a communication partner may be not a human being but an
               algorithm—which is not random and is completely controlled,
               although not by the processes of the human mind. Esposito
               investigates this by examining the use of algorithms in different
               areas of social life. She explores the proliferation of lists
               (and lists of lists) online, explaining that the web works on the
               basis of lists to produce further lists; the use of
               visualization; digital profiling and algorithmic
               individualization, which personalize a mass medium with playlists
               and recommendations; and the implications of the “right to be
               forgotten.” Finally, she considers how photographs today seem to
               be used to escape the present rather than to preserve a memory.",
  month     =  may,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Rezwana2023-rt,
  title    = "Towards designing engaging and ethical human-centered {AI}
              partners for human-{AI} co-creativity",
  author   = "Rezwana, Jeba",
  abstract = "Human-AI co-creativity involves a human and an AI collaborating as
              partners on creative tasks such as generating music or art. This
              research domain is particularly timely as AI becomes increasingly
              prevalent in collaborative spaces. With the availability of
              ChatGPT, DALL. E 2 and other generative AI tools, co-creative AI
              is gaining popularity. Unlike general human-computer interaction,
              human-AI co-creation establishes a complex relationship where AI
              actively contributes, assumes human-like roles, and generates
              novel content blended with the user's contribution. Therefore,
              designing engaging and ethical co-creative systems poses
              challenges due to the open-ended nature of human-AI interaction.
              This dissertation contributes empirically and theoretically to the
              design of engaging and ethical human-centered co-creative AI. It
              focuses on four main areas: designing interaction, the impact of
              AI-to-human …",
  year     =  2023
}

@MISC{noauthor_2024-xb,
  title        = "Why you need a Nobot",
  booktitle    = "The Thesis Whisperer",
  abstract     = "I've been working with Claude, an AI assistant from Anthropic,
                  for about a year. We've become... close. People laugh when I
                  call Claude my 'work husband'. I'm not really joking. Like a
                  good work spouse, Claude is always there to help and never
                  gets tired of my stories. Claude cheerfully does the tasks I
                  hate,…",
  month        =  oct,
  year         =  2024,
  howpublished = "\url{https://thesiswhisperer.com/2024/10/31/why-you-need-a-nobot/}",
  note         = "Accessed: 2024-11-26",
  language     = "en"
}

@INPROCEEDINGS{Ghajargar2022-af,
  title     = "A redhead walks into a bar: Experiences of writing fiction with
               artificial intelligence",
  author    = "Ghajargar, Maliheh and Bardzell, Jeffrey and Lagerkvist, Love",
  booktitle = "Proceedings of the 25th International Academic Mindtrek
               Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  nov,
  year      =  2022
}

@MISC{Salvaggio2024-fl,
  title        = "Sounds Like Music: Toward a Multi-Modal Media Theory of
                  Gaussian Pop",
  author       = "Salvaggio, Eryk",
  booktitle    = "Cybernetic Forests.",
  abstract     = "AI generated music is a sonification of data about music. In
                  this essay, Eryk Salvaggio explores how this might inform a
                  ``multi-modal media theory'' for generated media.",
  month        =  oct,
  year         =  2024,
  howpublished = "\url{https://www.cyberneticforests.com/news/toward-a-multi-modal-media-theory}",
  note         = "Accessed: 2024-11-13",
  language     = "en"
}

@ARTICLE{Crespo2022-ty,
  title     = "Augmenting digital nature: Generative art as a constructive
               feedback loop",
  author    = "Crespo, Sofia and McCormick, Feileacan",
  journal   = "Archit. Des.",
  publisher = "Wiley",
  volume    =  92,
  number    =  3,
  pages     = "54--59",
  month     =  may,
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Tholander2023-rv,
  title     = "Design ideation with {AI} - sketching, thinking and talking with
               generative machine learning models",
  author    = "Tholander, Jakob and Jonsson, Martin",
  booktitle = "Proceedings of the 2023 ACM Designing Interactive Systems
               Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jul,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Chang2023-tv,
  title     = "The Prompt Artists",
  author    = "Chang, Minsuk and Druga, Stefania and Fiannaca, Alexander J and
               Vergani, Pedro and Kulkarni, Chinmay and Cai, Carrie J and Terry,
               Michael",
  booktitle = "Creativity and Cognition",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Grigis2024-pf,
  title     = "Playwriting with large language models: Perceived features,
               interaction strategies and outcomes",
  author    = "Grigis, Paolo and De Angeli, Antonella",
  booktitle = "Proceedings of the 2024 International Conference on Advanced
               Visual Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Zhou2024-yy,
  title     = "Generative artificial intelligence, human creativity, and art",
  author    = "Zhou, Eric and Lee, Dokyun",
  journal   = "PNAS Nexus",
  publisher = "Oxford University Press (OUP)",
  volume    =  3,
  number    =  3,
  pages     = "gae052",
  abstract  = "Recent artificial intelligence (AI) tools have demonstrated the
               ability to produce outputs traditionally considered creative. One
               such system is text-to-image generative AI (e.g. Midjourney,
               Stable Diffusion, DALL-E), which automates humans' artistic
               execution to generate digital artworks. Utilizing a dataset of
               over 4 million artworks from more than 50,000 unique users, our
               research shows that over time, text-to-image AI significantly
               enhances human creative productivity by 25\% and increases the
               value as measured by the likelihood of receiving a favorite per
               view by 50\%. While peak artwork Content Novelty, defined as
               focal subject matter and relations, increases over time, average
               Content Novelty declines, suggesting an expanding but inefficient
               idea space. Additionally, there is a consistent reduction in both
               peak and average Visual Novelty, captured by pixel-level
               stylistic elements. Importantly, AI-assisted artists who can
               successfully explore more novel ideas, regardless of their prior
               originality, may produce artworks that their peers evaluate more
               favorably. Lastly, AI adoption decreased value capture (favorites
               earned) concentration among adopters. The results suggest that
               ideation and filtering are likely necessary skills in the
               text-to-image process, thus giving rise to ``generative
               synesthesia''-the harmonious blending of human exploration and AI
               exploitation to discover new creative workflows.",
  month     =  mar,
  year      =  2024,
  keywords  = "art; creative workflow; generative AI; human-AI collaboration;
               impact of AI",
  language  = "en"
}

@INPROCEEDINGS{Palani2024-on,
  title     = "Evolving roles and workflows of creative practitioners in the age
               of generative {AI}",
  author    = "Palani, Srishti and Ramos, Gonzalo",
  booktitle = "Creativity and Cognition",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2024
}

@INPROCEEDINGS{Lin2020-ji,
  title     = "It is your turn: Collaborative ideation with a co-creative robot
               through sketch",
  author    = "Lin, Yuyu and Guo, Jiahao and Chen, Yang and Yao, Cheng and Ying,
               Fangtian",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2020
}

@INPROCEEDINGS{Davis2016-te,
  title     = "Empirically studying participatory sense-making in abstract
               drawing with a co-creative cognitive agent",
  author    = "Davis, Nicholas and Hsiao, Chih-Pin and Yashraj Singh, Kunwar and
               Li, Lisa and Magerko, Brian",
  booktitle = "Proceedings of the 21st International Conference on Intelligent
               User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This paper reports on the design and evaluation of a co-creative
               drawing partner called the Drawing Apprentice, which was designed
               to improvise and collaborate on abstract sketches with users in
               real time. The system qualifies as a new genre of creative
               technologies termed ``casual creators'' that are meant to
               creatively engage users and provide enjoyable creative
               experiences rather than necessarily helping users make a higher
               quality creative product. We introduce the conceptual framework
               of participatory sense-making and describe how it can help model
               and understand open-ended collaboration. We report the results of
               a user study comparing human-human collaboration to
               human-computer collaboration using the Drawing Apprentice system.
               Based on insights from the user study, we present a set of design
               recommendations for co-creative agents.",
  month     =  mar,
  year      =  2016
}

@INPROCEEDINGS{Peng2024-tr,
  title     = "{DesignPrompt}: Using multimodal interaction for design
               exploration with generative {AI}",
  author    = "Peng, Xiaohan and Koch, Janin and Mackay, Wendy E",
  booktitle = "Designing Interactive Systems Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jul,
  year      =  2024
}

@ARTICLE{McCormack2024-gv,
  title         = "Mimetic Poet",
  author        = "McCormack, Jon and Wilson, Elliott and Rajcic, Nina and
                   Llano, Maria Teresa",
  journal       = "arXiv [cs.HC]",
  abstract      = "This paper presents the design and initial assessment of a
                   novel device that uses generative AI to facilitate creative
                   ideation, inspiration, and reflective thought. Inspired by
                   magnetic poetry, which was originally designed to help
                   overcome writer's block, the device allows participants to
                   compose short poetic texts from a limited vocabulary by
                   physically placing words on the device's surface. Upon
                   composing the text, the system employs a large language model
                   (LLM) to generate a response, displayed on an e-ink screen.
                   We explored various strategies for internally sequencing
                   prompts to foster creative thinking, including analogy,
                   allegorical interpretations, and ideation. We installed the
                   device in our research laboratory for two weeks and held a
                   focus group at the conclusion to evaluate the design. The
                   design choice to limit interactions with the LLM to poetic
                   text, coupled with the tactile experience of assembling the
                   poem, fostered a deeper and more enjoyable engagement with
                   the LLM compared to traditional chatbot or screen-based
                   interactions. This approach gives users the opportunity to
                   reflect on the AI-generated responses in a manner conducive
                   to creative thought.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@INPROCEEDINGS{Park2024-gw,
  title     = "``we are visual thinkers, not verbal thinkers!'': A thematic
               analysis of how professional designers use generative {AI} image
               generation tools",
  author    = "Park, Hyerim and Eirich, Joscha and Luckow, Andre and Sedlmair,
               Michael",
  booktitle = "Nordic Conference on Human-Computer Interaction",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--14",
  month     =  oct,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Koch2020-gx,
  title     = "{ImageSense}: An intelligent collaborative ideation tool to
               support diverse human-computer partnerships",
  author    = "Koch, Janin and Taffin, Nicolas and Beaudouin-Lafon, Michel and
               Laine, Markku and Lucero, Andrés and Mackay, Wendy E",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  4,
  number    = "CSCW1",
  pages     = "1--27",
  abstract  = "Professional designers create mood boards to explore, visualize,
               and communicate hard-to-express ideas. We present ImageCascade,
               an intelligent, collaborative ideation tool that combines
               individual and shared work spaces, as well as collaboration with
               multiple forms of intelligent agents. In the collection phase,
               ImageCascade offers fluid transitions between serendipitous
               discovery of curated images via ImageCascade, combined text- and
               image-based Semantic search, and intelligent AI suggestions for
               finding new images. For later composition and reflection,
               ImageCascade provides semantic labels, generated color palettes,
               and multiple tag clouds to help communicate the intent of the
               mood board. A study of nine professional designers revealed
               nuances in designers' preferences for designer-led, system-led,
               and mixed-initiative approaches that evolve throughout the design
               process. We discuss the challenges in creating effective
               human-computer partnerships for creative activities, and suggest
               directions for future research.",
  month     =  may,
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Chiou2023-vr,
  title     = "Designing with {AI}: An exploration of co-ideation with image
               generators",
  author    = "Chiou, Li-Yuan and Hung, Peng-Kai and Liang, Rung-Huei and Wang,
               Chun-Teng",
  booktitle = "Proceedings of the 2023 ACM Designing Interactive Systems
               Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jul,
  year      =  2023
}

@ARTICLE{Wadinambiarachchi2024-jn,
  title         = "The effects of generative {AI} on design fixation and
                   divergent thinking",
  author        = "Wadinambiarachchi, Samangi and Kelly, Ryan M and Pareek,
                   Saumya and Zhou, Qiushi and Velloso, Eduardo",
  journal       = "arXiv [cs.HC]",
  abstract      = "Generative AI systems have been heralded as tools for
                   augmenting human creativity and inspiring divergent thinking,
                   though with little empirical evidence for these claims. This
                   paper explores the effects of exposure to AI-generated images
                   on measures of design fixation and divergent thinking in a
                   visual ideation task. Through a between-participants
                   experiment (N=60), we found that support from an AI image
                   generator during ideation leads to higher fixation on an
                   initial example. Participants who used AI produced fewer
                   ideas, with less variety and lower originality compared to a
                   baseline. Our qualitative analysis suggests that the
                   effectiveness of co-ideation with AI rests on participants'
                   chosen approach to prompt creation and on the strategies used
                   by participants to generate ideas in response to the AI's
                   suggestions. We discuss opportunities for designing
                   generative AI systems for ideation support and incorporating
                   these AI tools into ideation workflows.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@INPROCEEDINGS{Yang2022-vs,
  title     = "{AI} as an active writer: Interaction strategies with generated
               text in human-{AI} collaborative fiction writing 56-65",
  author    = "Yang, Daijin and Zhou, Yanpeng and Zhang, Zhiyuan and Li, Toby
               Jia-Jun and Ray, L C",
  booktitle = "Joint Proceedings of the ACM IUI Workshops 2022,",
  pages     = "56--65",
  abstract  = "Machine Learning (ML) has become an important part of the
               creative process for human fiction writers, allowing them to
               utilize various sources of information and be inspired by
               strategies and data previously seldom explored. To investigate
               how human writers collaborate with ML systems in fiction writing,
               we prototyped a web-based human-AI collaborative writing tool
               that allows writers to shorten, edit, summarize, and regenerate
               text produced by AI. To investigate the dynamics of human-AI
               interaction in fiction co-writing, we used a ``finish each
               other’s story'' approach where humans and machines took turns
               writing collaborative fiction. In results from a preliminary
               study with 9 users, we found that users took inspiration from
               unexpected text generated by the machine, that users expected
               reduced fluency and coherence in the machine text when allowed to
               edit the output, and that they perceived a mental model of the AI
               as an active writer in the collaborative process rather than
               simply as a passive AI writing assistant. This study provides
               design implications on supporting co-creative writing of humans
               and machines.",
  year      =  2022
}

@INPROCEEDINGS{Bougueng-Tchemeube2023-nm,
  title     = "Evaluating human-{AI} interaction via usability, user experience
               and acceptance measures for {MMM}-{C}: A creative {AI} system for
               music composition",
  author    = "Bougueng Tchemeube, Renaud and Ens, Jeffrey and Plut, Cale and
               Pasquier, Philippe and Safi, Maryam and Grabit, Yvan and Rolland,
               Jean-Baptiste",
  booktitle = "Proceedings of the Thirty-Second International Joint Conference
               on Artificial Intelligence",
  publisher = "International Joint Conferences on Artificial Intelligence
               Organization",
  address   = "California",
  pages     = "5769--5778",
  abstract  = "With the rise of artificial intelligence (AI), there has been
               increasing interest in human-AI co-creation in a variety of
               artistic domains including music as AI-driven systems are
               frequently able to generate human-competitive artifacts. Now, the
               implications of such systems for the musical practice are being
               investigated. This paper reports on a thorough evaluation of the
               user adoption of the Multi-Track Music Machine (MMM) as a minimal
               co-creative AI tool for music composers. To do this, we integrate
               MMM into Cubase, a popular Digital Audio Workstation (DAW), by
               producing a ``1-parameter'' plugin interface named MMM-Cubase,
               which enables human-AI co-composition. We conduct a 3-part mixed
               method study measuring usability, user experience and technology
               acceptance of the system across two groups of expert-level
               composers: hobbyists and professionals. Results show positive
               usability and acceptance scores. Users report experiences of
               novelty, surprise and ease of use from using the system, and
               limitations on controllability and predictability of the
               interface when generating music. Findings indicate no significant
               difference between the two user groups.",
  month     =  aug,
  year      =  2023
}

@ARTICLE{Rezwana2022-uf,
  title         = "Identifying ethical issues in {AI} partners in human-{AI}
                   co-creation",
  author        = "Rezwana, Jeba and Maher, Mary Lou",
  journal       = "arXiv [cs.HC]",
  abstract      = "Human-AI co-creativity involves humans and AI collaborating
                   on a shared creative product as partners. In many existing
                   co-creative systems, users communicate with the AI using
                   buttons or sliders. However, typically, the AI in co-creative
                   systems cannot communicate back to humans, limiting their
                   potential to be perceived as partners. This paper starts with
                   an overview of a comparative study with 38 participants to
                   explore the impact of AI-to-human communication on user
                   perception and engagement in co-creative systems and the
                   results show improved collaborative experience and user
                   engagement with the system incorporating AI-to-human
                   communication. The results also demonstrate that users
                   perceive co-creative AI as more reliable, personal and
                   intelligent when it can communicate with the users. The
                   results indicate a need to identify potential ethical issues
                   from an engaging communicating co-creative AI. Later in the
                   paper, we present some potential ethical issues in human-AI
                   co-creation and propose to use participatory design fiction
                   as the research methodology to investigate the ethical issues
                   associated with a co-creative AI that communicates with
                   users.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@INPROCEEDINGS{Lawton2023-tb,
  title     = "When is a tool a tool? User perceptions of system agency in
               human–AI co-creative drawing",
  author    = "Lawton, Tomas and Grace, Kazjon and Ibarrola, Francisco J",
  booktitle = "Proceedings of the 2023 ACM Designing Interactive Systems
               Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jul,
  year      =  2023
}

@INPROCEEDINGS{Chakrabarty2024-ov,
  title     = "Creativity support in the age of large language models: An
               empirical study involving professional writers",
  author    = "Chakrabarty, Tuhin and Padmakumar, Vishakh and Brahman, Faeze and
               Muresan, Smaranda",
  booktitle = "Creativity and Cognition",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Zhou2024-vp,
  title     = "Understanding nonlinear collaboration between human and {AI}
               agents: A co-design framework for creative design",
  author    = "Zhou, Jiayi and Li, Renzhong and Tang, Junxiu and Tang, Tan and
               Li, Haotian and Cui, Weiwei and Wu, Yingcai",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--16",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Sun2024-pb,
  title     = "Generative {AI} in the wild: Prospects, challenges, and
               strategies",
  author    = "Sun, Yuan and Jang, Eunchae and Ma, Fenglong and Wang, Ting",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  18,
  pages     = "1--16",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Han2024-cq,
  title     = "When teams embrace {AI}: Human collaboration strategies in
               generative prompting in a creative design task",
  author    = "Han, Yuanning and Qiu, Ziyi and Cheng, Jiale and Lc, Ray",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  23,
  pages     = "1--14",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Uitdenbogerd2023-no,
  title     = "Raging with the machine in the uncanny valley: {Human–AI}
               cocreativity in the eurovision-themed {AI} Song Contest",
  author    = "Uitdenbogerd, Alexandra L and Bown, Oliver and Hill, Charlton and
               Pegram, Caroline and Shave, Justin and Wright, Brendan",
  journal   = "Comput. Music J.",
  publisher = "MIT Press",
  volume    =  47,
  number    =  1,
  pages     = "44--63",
  abstract  = "Abstract We report here the processes involved in creating our
               entry in the 2020 AI Song Contest, “Beautiful the World”; the
               technical innovations from the project; and the decision-making
               that divided tasks between human and machine in a way that
               ensured that the final creation was AI-inspired but
               human-created, starting from generated melodies, lyrics, and
               timbres. Key innovations include the use of lyric stress patterns
               as queries to a stress-based melody index to a database of
               generated melodies, and the creation of a novel instrument timbre
               with differential digital signal processing, trained on
               Australian animal calls. We reflect on how human–AI cocreativity
               occurred during the process and how it may develop in the future.",
  month     =  jun,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Kim2023-zq,
  title     = "The effect of {AI}-based inspiration on human design ideation",
  author    = "Kim, Jingoog and Maher, Mary Lou",
  journal   = "Int. J. Des. Creat. Innov.",
  publisher = "Informa UK Limited",
  pages     = "1--18",
  month     =  jan,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Verheijden2023-gn,
  title     = "Collaborative diffusion: Boosting designerly co-creation with
               generative {AI}",
  author    = "Verheijden, Mathias Peter and Funk, Mathias",
  booktitle = "Extended Abstracts of the 2023 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Messer2024-gl,
  title     = "Co-creating art with generative artificial intelligence:
               Implications for artworks and artists",
  author    = "Messer, Uwe",
  journal   = "Computers in Human Behavior: Artificial Humans",
  publisher = "Elsevier BV",
  volume    =  2,
  number    =  100056,
  pages     =  100056,
  abstract  = "Synthetic visual art is becoming a commodity due to generative
               artificial intelligence (AI). The trend of using AI for
               co-creation will not spare arti…",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Nordstrom2023-ge,
  title     = "Evolving coagency between artists and {AI} in the spatial
               cocreative process of artmaking",
  author    = "Nordström, Paulina and Lundman, Riina and Hautala, Johanna",
  journal   = "Ann. Am. Assoc. Geogr.",
  publisher = "Informa UK Limited",
  volume    =  113,
  number    =  9,
  pages     = "2203--2218",
  month     =  oct,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Suh2021-cj,
  title     = "{AI} as social glue: Uncovering the roles of deep generative {AI}
               during social music composition",
  author    = "Suh, Minhyang (mia) and Youngblom, Emily and Terry, Michael and
               Cai, Carrie J",
  booktitle = "Proceedings of the 2021 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  may,
  year      =  2021,
  language  = "en"
}

@INCOLLECTION{Moura2024-zb,
  title     = "Rethinking creativity frameworks for artificial intelligence",
  author    = "Moura, Francisco Tigre",
  booktitle = "Artificial Intelligence, Co-Creation and Creativity",
  publisher = "Routledge",
  address   = "London",
  edition   = "1st Edition",
  pages     = "32--44",
  abstract  = "The rapid development and adoption of artificial intelligence
               (AI) into creative processes has brought about a transformative
               era, challenging established notions of creativity. Traditional
               creativity frameworks, such as the 4Ps, 5As, 7Cs, and 8Ps,
               grounded in human-centric assumptions, fail to address the
               impersonal and autonomous nature of intelligent systems. Also,
               AI’s efficiency in problem-solving across domains challenges the
               domain specificity often associated with human creativity. AI
               exhibits unprecedented capabilities in tasks like composing
               music, writing poetry, and designing art, often rivalling human
               creativity. However, this shift raises fundamental questions
               about the nature of creativity, including attributing
               intentionality to AI-generated outputs. This chapter reviews
               recent frameworks, such as the 5C model, specifically designed
               for human–computer co-creativity, to propose adaptations to
               existing creativity frameworks, such as the 8P model. Exploring
               missing elements in traditional frameworks, the chapter addresses
               gaps related to propulsion, problem, and purpose. As AI adoption
               accelerates, distinguishing between human- and AI-generated
               creative outputs becomes increasingly challenging, necessitating
               an inclusive framework that embraces this evolving reality. Thus,
               the chapter concludes by advocating for a paradigm shift in
               creativity understanding, acknowledging smart systems as vital
               creative agents, and suggesting future research directions that
               emphasize the dynamic nature of human–AI collaboration.",
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@ARTICLE{McGuire2024-im,
  title     = "Establishing the importance of co-creation and self-efficacy in
               creative collaboration with artificial intelligence",
  author    = "McGuire, Jack and De Cremer, David and Van de Cruys, Tim",
  journal   = "Sci. Rep.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  14,
  number    =  1,
  pages     =  18525,
  abstract  = "The emergence of generative AI technologies has led to an
               increasing number of people collaborating with AI to produce
               creative works. Across two experimental studies, in which we
               carefully designed and programmed state-of-the-art human-AI
               interfaces, we examine how the design of generative AI systems
               influences human creativity (poetry writing). First, we find that
               people were most creative when writing a poem on their own,
               compared to first receiving a poem generated by an AI system and
               using sophisticated tools to edit it (Study 1). Following this,
               we demonstrate that this creativity deficit dissipates when
               people co-create with-not edit-AI and establish creative
               self-efficacy as an important mechanism in this process (Study
               2). Thus, our findings indicate that people must occupy the role
               of a co-creator, not an editor, to reap the benefits of
               generative AI in the production of creative works.",
  month     =  aug,
  year      =  2024,
  keywords  = "Artificial intelligence; Co-creation; Creativity; Self-efficacy",
  language  = "en"
}

@MISC{Colton_undated-pm,
  title  = "Computational Creativity: The Final Frontier?",
  author = "Colton, Simon and Wiggins, Geraint A"
}

@BOOK{Bratton2015-re,
  title     = "The Stack",
  author    = "Bratton, Benjamin",
  publisher = "MIT Press",
  address   = "Cambridge, Mass.",
  series    = "Software Studies",
  year      =  2015
}

@MISC{Toner-Rodgers_undated-sk,
  title  = "Artificial Intelligence, Scientific Discovery, and Product
            Innovation",
  author = "Toner-Rodgers, Aidan"
}

@ARTICLE{Kwon2022-uc,
  title         = "Diffusion Models already have a Semantic Latent Space",
  author        = "Kwon, Mingi and Jeong, Jaeseok and Uh, Youngjung",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models achieve outstanding generative performance
                   in various domains. Despite their great success, they lack
                   semantic latent space which is essential for controlling the
                   generative process. To address the problem, we propose
                   asymmetric reverse process (Asyrp) which discovers the
                   semantic latent space in frozen pretrained diffusion models.
                   Our semantic latent space, named h-space, has nice properties
                   for accommodating semantic image manipulation: homogeneity,
                   linearity, robustness, and consistency across timesteps. In
                   addition, we introduce a principled design of the generative
                   process for versatile editing and quality boost ing by
                   quantifiable measures: editing strength of an interval and
                   quality deficiency at a timestep. Our method is applicable to
                   various architectures (DDPM++, iD- DPM, and ADM) and datasets
                   (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and
                   METFACES). Project page: https://kwonminki.github.io/Asyrp/",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Schaerf2024-gf,
  title         = "Reflections on disentanglement and the latent space",
  author        = "Schaerf, Ludovica",
  journal       = "arXiv [cs.CY]",
  abstract      = "The latent space of image generative models is a
                   multi-dimensional space of compressed hidden visual
                   knowledge. Its entity captivates computer scientists, digital
                   artists, and media scholars alike. Latent space has become an
                   aesthetic category in AI art, inspiring artistic techniques
                   such as the latent space walk, exemplified by the works of
                   Mario Klingemann and others. It is also viewed as cultural
                   snapshots, encoding rich representations of our visual world.
                   This paper proposes a double view of the latent space, as a
                   multi-dimensional archive of culture and as a
                   multi-dimensional space of potentiality. The paper discusses
                   disentanglement as a method to elucidate the double nature of
                   the space and as an interpretative direction to exploit its
                   organization in human terms. The paper compares the role of
                   disentanglement as potentiality to that of conditioning, as
                   imagination, and confronts this interpretation with the
                   philosophy of Deleuzian potentiality and Hume's imagination.
                   Lastly, this paper notes the difference between traditional
                   generative models and recent architectures.",
  month         =  oct,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Licklider1960-md,
  title     = "Man-Computer Symbiosis",
  author    = "Licklider, J C R",
  journal   = "IRE Trans. Hum. Factors Electron.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    = "HFE-1",
  number    =  1,
  pages     = "4--11",
  abstract  = "Man-computer symbiosis is an expected development in cooperative
               interaction between men and electronic computers. It will involve
               very close coupling between the human and the electronic members
               of the partnership. The main aims are 1) to let computers
               facilitate formulative thinking as they now facilitate the
               solution of formulated problems, and 2) to enable men and
               computers to cooperate in making decisions and controlling
               complex situations without inflexible dependence on predetermined
               programs. In the anticipated symbiotic partnership, men will set
               the goals, formulate the hypotheses, determine the criteria, and
               perform the evaluations. Computing machines will do the
               routinizable work that must be done to prepare the way for
               insights and decisions in technical and scientific thinking.
               Preliminary analyses indicate that the symbiotic partnership will
               perform intellectual operations much more effectively than man
               alone can perform them. Prerequisites for the achievement of the
               effective, cooperative association include developments in
               computer time sharing, in memory components, in memory
               organization, in programming languages, and in input and output
               equipment.",
  month     =  mar,
  year      =  1960,
  language  = "en"
}

@MISC{UnknownUnknown-he,
  title        = "Man-Computer Symbiosis",
  howpublished = "\url{https://groups.csail.mit.edu/medg/people/psz/Licklider.html}",
  note         = "Accessed: 2024-11-8"
}

@ARTICLE{Kantosalo2015-pk,
  title     = "Interaction evaluation for human-computer co-creativity: A case
               study",
  author    = "Kantosalo, Anna and Toivanen, Jukka M and Toivonen, Hannu (tt)",
  journal   = "Int Conf Control Commun Comput India",
  publisher = "tuhat.helsinki.fi",
  pages     = "276--283",
  abstract  = "Interaction design has been suggested as a framework for
               evaluating computational creativity by Bown (2014). Yet few
               practical accounts on using an Interaction Design based
               evaluation strategy in Computational Creativity Contexts have
               been reported in the literature. This study paper describes the
               evaluation process and results of a human-computer co-creative
               poetry writing tool intended for children in a school context. We
               specifically focus on one formative evaluation case utilizing
               Interaction Design evaluation methods, offering a suggestion on
               how to conduct Interaction Design based evaluation in a
               computational creativity context, as well as, report the results
               of the evaluation itself. The evaluation process is considered
               from the perspective of a computational creativity researcher and
               we focus on challenges and benefits of the interaction design
               evaluation approach within a computational creativity project
               context.",
  year      =  2015
}

@ARTICLE{Rombach2021-wf,
  title         = "High-resolution image synthesis with latent diffusion models",
  author        = "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and
                   Esser, Patrick and Ommer, Björn",
  journal       = "arXiv [cs.CV]",
  abstract      = "By decomposing the image formation process into a sequential
                   application of denoising autoencoders, diffusion models (DMs)
                   achieve state-of-the-art synthesis results on image data and
                   beyond. Additionally, their formulation allows for a guiding
                   mechanism to control the image generation process without
                   retraining. However, since these models typically operate
                   directly in pixel space, optimization of powerful DMs often
                   consumes hundreds of GPU days and inference is expensive due
                   to sequential evaluations. To enable DM training on limited
                   computational resources while retaining their quality and
                   flexibility, we apply them in the latent space of powerful
                   pretrained autoencoders. In contrast to previous work,
                   training diffusion models on such a representation allows for
                   the first time to reach a near-optimal point between
                   complexity reduction and detail preservation, greatly
                   boosting visual fidelity. By introducing cross-attention
                   layers into the model architecture, we turn diffusion models
                   into powerful and flexible generators for general
                   conditioning inputs such as text or bounding boxes and
                   high-resolution synthesis becomes possible in a convolutional
                   manner. Our latent diffusion models (LDMs) achieve a new
                   state of the art for image inpainting and highly competitive
                   performance on various tasks, including unconditional image
                   generation, semantic scene synthesis, and super-resolution,
                   while significantly reducing computational requirements
                   compared to pixel-based DMs. Code is available at
                   https://github.com/CompVis/latent-diffusion .",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Ho2020-zj,
  title         = "Denoising Diffusion Probabilistic Models",
  author        = "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present high quality image synthesis results using
                   diffusion probabilistic models, a class of latent variable
                   models inspired by considerations from nonequilibrium
                   thermodynamics. Our best results are obtained by training on
                   a weighted variational bound designed according to a novel
                   connection between diffusion probabilistic models and
                   denoising score matching with Langevin dynamics, and our
                   models naturally admit a progressive lossy decompression
                   scheme that can be interpreted as a generalization of
                   autoregressive decoding. On the unconditional CIFAR10
                   dataset, we obtain an Inception score of 9.46 and a
                   state-of-the-art FID score of 3.17. On 256x256 LSUN, we
                   obtain sample quality similar to ProgressiveGAN. Our
                   implementation is available at
                   https://github.com/hojonathanho/diffusion",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Sohl-Dickstein2015-sm,
  title         = "Deep Unsupervised Learning using Nonequilibrium
                   Thermodynamics",
  author        = "Sohl-Dickstein, Jascha and Weiss, Eric A and Maheswaranathan,
                   Niru and Ganguli, Surya",
  journal       = "arXiv [cs.LG]",
  abstract      = "A central problem in machine learning involves modeling
                   complex data-sets using highly flexible families of
                   probability distributions in which learning, sampling,
                   inference, and evaluation are still analytically or
                   computationally tractable. Here, we develop an approach that
                   simultaneously achieves both flexibility and tractability.
                   The essential idea, inspired by non-equilibrium statistical
                   physics, is to systematically and slowly destroy structure in
                   a data distribution through an iterative forward diffusion
                   process. We then learn a reverse diffusion process that
                   restores structure in data, yielding a highly flexible and
                   tractable generative model of the data. This approach allows
                   us to rapidly learn, sample from, and evaluate probabilities
                   in deep generative models with thousands of layers or time
                   steps, as well as to compute conditional and posterior
                   probabilities under the learned model. We additionally
                   release an open source reference implementation of the
                   algorithm.",
  month         =  mar,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Haase2023-vz,
  title         = "Artificial muses: Generative Artificial Intelligence chatbots
                   have risen to human-level creativity",
  author        = "Haase, Jennifer and Hanel, Paul H P",
  journal       = "arXiv [cs.AI]",
  abstract      = "A widespread view is that Artificial Intelligence cannot be
                   creative. We tested this assumption by comparing
                   human-generated ideas with those generated by six Generative
                   Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
                   $Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and
                   YouChat. Humans and a specifically trained AI independently
                   assessed the quality and quantity of ideas. We found no
                   qualitative difference between AI and human-generated
                   creativity, although there are differences in how ideas are
                   generated. Interestingly, 9.4 percent of humans were more
                   creative than the most creative GAI, GPT-4. Our findings
                   suggest that GAIs are valuable assistants in the creative
                   process. Continued research and development of GAI in
                   creative tasks is crucial to fully understand this
                   technology's potential benefits and drawbacks in shaping the
                   future of creativity. Finally, we discuss the question of
                   whether GAIs are capable of being truly creative.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Guzik2023-cl,
  title     = "The originality of machines: {AI} takes the Torrance Test",
  author    = "Guzik, Erik E and Byrge, Christian and Gilde, Christian",
  journal   = "Journal of Creativity",
  publisher = "Elsevier BV",
  volume    =  33,
  number    =  3,
  pages     =  100065,
  abstract  = "This exploratory research investigated the creative abilities of
               OpenAI's large language model, ChatGPT, based on the GPT-4
               architecture, as assessed …",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Jia2024-vp,
  title     = "When and how artificial intelligence augments employee creativity",
  author    = "Jia, Nan and Luo, Xueming and Fang, Zheng and Liao, Chengcheng",
  journal   = "Acad. Manage. J.",
  publisher = "Academy of Management",
  volume    =  67,
  number    =  1,
  pages     = "5--32",
  abstract  = "Can artificial intelligence (AI) assist human employees in
               increasing employee creativity? Drawing on research on AI–human
               collaboration, job design, and employee creativity, we examine AI
               assistance in the form of a sequential division of labor within
               organizations: in a task, AI handles the initial portion, which
               is well-codified and repetitive, and employees focus on the
               subsequent portion, involving higher-level problem-solving.
               First, we provide causal evidence from a field experiment
               conducted at a telemarketing company. We find that AI assistance
               in generating sales leads, on average, increases employees’
               creativity in answering customers’ questions during subsequent
               sales persuasion. Enhanced creativity leads to increased sales.
               However, this effect is much more pronounced for higher-skilled
               employees. Next, we conducted a qualitative study using
               semi-structured interviews with the employees. We found that AI
               assistance changes job design by intensifying employees’
               interactions with more serious customers. This change enables
               higher-skilled employees to generate innovative scripts and
               develop positive emotions at work, which are conducive to
               creativity. By contrast, with AI assistance, lower-skilled
               employees make limited improvements to scripts and experience
               negative emotions at work. We conclude that employees can achieve
               AI-augmented creativity, but this desirable outcome is
               skill-biased by favoring experts with greater job skills.",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Lyu2022-ny,
  title     = "Communication in human–AI co-creation: Perceptual analysis of
               paintings generated by text-to-image system",
  author    = "Lyu, Yanru and Wang, Xinxin and Lin, Rungtai and Wu, Jun",
  journal   = "Appl. Sci. (Basel)",
  publisher = "MDPI AG",
  volume    =  12,
  number    =  22,
  pages     =  11312,
  abstract  = "In recent years, art creation using artificial intelligence (AI)
               has started to become a mainstream phenomenon. One of the latest
               applications of AI is to generate visual artwork from natural
               language descriptions where anyone can interact with it to create
               thousands of artistic images with minimal effort, which provokes
               the questions: what is the essence of artistic creation, and who
               can create art in this era? Considering that, in this study, the
               theoretical communication framework was adopted to investigate
               the difference in the interaction with the text-to-image system
               between artists and nonartists. In this experiment, ten artists
               and ten nonartists were invited to co-create with Midjourney.
               Their actions and reflections were recorded, and two sets of
               generated images were collected for the visual question-answering
               task, with a painting created by the artist as a reference
               sample. A total of forty-two subjects with artistic backgrounds
               participated in the evaluated experiment. The results indicated
               differences between the two groups in their creation actions and
               their attitude toward AI, while the technology blurred the
               difference in the perception of the results caused by the
               creator’s artistic experience. In addition, attention should be
               paid to communication on the effectiveness level for a better
               perception of the artistic value.",
  month     =  nov,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Hitsuwari2023-tw,
  title     = "Does human–AI collaboration lead to more creative art? Aesthetic
               evaluation of human-made and {AI}-generated haiku poetry",
  author    = "Hitsuwari, Jimpei and Ueda, Yoshiyuki and Yun, Woojin and Nomura,
               Michio",
  journal   = "Comput. Human Behav.",
  publisher = "Elsevier BV",
  volume    =  139,
  number    =  107502,
  pages     =  107502,
  abstract  = "With the development of technology, the quality of AI-generated
               text has improved. This is relevant in the AI art field, where AI
               generates literature…",
  month     =  feb,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Haase2023-mz,
  title     = "The Art of Inspiring Creativity: Exploring the Unique Impact of
               {AI}-generated Images",
  author    = "Haase, Jennifer and Djurica, Djordje and Mendling, Jan",
  booktitle = "AMCIS 2023 Proceedings",
  abstract  = "This paper examines whether AI-generated art can serve as a
               source of inspiration to enhance individual creative performance.
               Specifically, the study investigates whether AI-generated art has
               associative potential that can stimulate idea generation and
               enhance individual creativity in terms of originality and the
               number of ideas generated by humans. To address this research
               problem, we focus on DALL-E-2, a generative AI system that can
               create images from textual descriptions. We first provide an
               overview of situational creativity support systems and then
               present the design of an online experiment in which 298
               participants used (artificially generated vs. traditional) art or
               none to ideate. The data shows that art in general, but
               AI-generated especially, has the potential to enhance creative
               performance by stimulating idea generation. We discuss the
               implications of using AI-generated art as a creative support
               tool.",
  year      =  2023
}

@ARTICLE{Wan2023-he,
  title         = "``it felt like having a second mind'': Investigating
                   human-{AI} co-creativity in prewriting with large language
                   models",
  author        = "Wan, Qian and Hu, Siying and Zhang, Yu and Wang, Piaohong and
                   Wen, Bo and Lu, Zhicong",
  journal       = "arXiv [cs.HC]",
  abstract      = "Prewriting is the process of discovering and developing ideas
                   before a first draft, which requires divergent thinking and
                   often implies unstructured strategies such as diagramming,
                   outlining, free-writing, etc. Although large language models
                   (LLMs) have been demonstrated to be useful for a variety of
                   tasks including creative writing, little is known about how
                   users would collaborate with LLMs to support prewriting. The
                   preferred collaborative role and initiative of LLMs during
                   such a creativity process is also unclear. To investigate
                   human-LLM collaboration patterns and dynamics during
                   prewriting, we conducted a three-session qualitative study
                   with 15 participants in two creative tasks: story writing and
                   slogan writing. The findings indicated that during
                   collaborative prewriting, there appears to be a three-stage
                   iterative Human-AI Co-creativity process that includes
                   Ideation, Illumination, and Implementation stages. This
                   collaborative process champions the human in a dominant role,
                   in addition to mixed and shifting levels of initiative that
                   exist between humans and LLMs. This research also reports on
                   collaboration breakdowns that occur during this process, user
                   perceptions of using existing LLMs during Human-AI
                   Co-creativity, and discusses design implications to support
                   this co-creativity process.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Doshi2023-dv,
  title     = "Generative artificial intelligence enhances creativity but
               reduces the diversity of novel content",
  author    = "Doshi, Anil Rajnikant and Hauser, Oliver",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  abstract  = "Creativity is core to being human. Generative artificial
               intelligence (GenAI) holds promise for humans to be more creative
               by offering new ideas, or less creati",
  month     =  aug,
  year      =  2023,
  keywords  = "generative artificial intelligence (GenAI), artificial
               intelligence (AI), generative AI, large language models (LLMs),
               creativity, experiment",
  language  = "en"
}

@MISC{UnknownUnknown-nx,
  title        = "Oasis",
  abstract     = "Generating Worlds in Realtime",
  howpublished = "\url{https://oasis-model.github.io/}",
  note         = "Accessed: 2024-11-1",
  language     = "en"
}

@ARTICLE{Dosovitskiy2020-wh,
  title         = "An image is worth {16x16} words: Transformers for image
                   recognition at scale",
  author        = "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,
                   Alexander and Weissenborn, Dirk and Zhai, Xiaohua and
                   Unterthiner, Thomas and Dehghani, Mostafa and Minderer,
                   Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit,
                   Jakob and Houlsby, Neil",
  journal       = "arXiv [cs.CV]",
  abstract      = "While the Transformer architecture has become the de-facto
                   standard for natural language processing tasks, its
                   applications to computer vision remain limited. In vision,
                   attention is either applied in conjunction with convolutional
                   networks, or used to replace certain components of
                   convolutional networks while keeping their overall structure
                   in place. We show that this reliance on CNNs is not necessary
                   and a pure transformer applied directly to sequences of image
                   patches can perform very well on image classification tasks.
                   When pre-trained on large amounts of data and transferred to
                   multiple mid-sized or small image recognition benchmarks
                   (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT)
                   attains excellent results compared to state-of-the-art
                   convolutional networks while requiring substantially fewer
                   computational resources to train.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@MISC{UnknownUnknown-ke,
  title        = "Paper page - {LongVU}: Spatiotemporal Adaptive Compression for
                  Long Video-Language Understanding",
  abstract     = "Join the discussion on this paper page",
  howpublished = "\url{https://huggingface.co/papers/2410.17434}",
  note         = "Accessed: 2024-10-31"
}

@ARTICLE{Colton2011-uy,
  title    = "Computational creativity theory: The {FACE} and {IDEA} descriptive
              models",
  author   = "Colton, S and Charnley, J and Pease, A",
  journal  = "Int Conf Control Commun Comput India",
  pages    = "90--95",
  abstract = "We introduce computational creativity theory (CCT) as an analogue
              in computational creativity research to computational learning
              theory in machine learning. In its current draft, CCT comprises
              the FACE descriptive model of creative acts as tuples of
              generative acts, and the IDEA descriptive model of the impact such
              creative acts may have. To introduce these, we simplify various
              assumptions about software development, background material given
              to software, how creative acts are performed by computer, and how
              audiences consume the results. We use the two descriptive models
              to perform two comparisons studies, firstly for mathematical
              discovery software, and secondly for visual art generating
              programs. We conclude by discussing possible additions,
              improvements and refinements to CCT.",
  month    =  dec,
  year     =  2011
}

@MISC{UnknownUnknown-dq,
  title        = "Self-Discovering Interpretable Diffusion Latent Directions for
                  Responsible Text-to-Image Generation",
  howpublished = "\url{https://arxiv.org/html/2311.17216v2}",
  note         = "Accessed: 2024-10-30",
  language     = "en"
}

@INPROCEEDINGS{Davis2024-ml,
  title     = "Fashioning creative expertise with generative {AI}: Graphical
               interfaces for design space exploration better support ideation
               than text prompts",
  author    = "Davis, Richard Lee and Wambsganss, Thiemo and Jiang, Wei and Kim,
               Kevin Gonyop and Käser, Tanja and Dillenbourg, Pierre",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  33,
  pages     = "1--26",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Lawton2023-gd,
  title     = "Drawing with reframer: Emergence and control in co-creative {AI}",
  author    = "Lawton, Tomas and Ibarrola, Francisco J and Ventura, Dan and
               Grace, Kazjon",
  booktitle = "Proceedings of the 28th International Conference on Intelligent
               User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  mar,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Mikolov2013-ii,
  title    = "Efficient Estimation of Word Representations in Vector Space",
  author   = "Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey",
  abstract = "We propose two novel model architectures for computing continuous
              vector representations of words from very large data sets. The
              quality of these representations is measured in a word similarity
              task, and the results are compared to the previously best
              performing techniques based on different types of neural networks.
              We observe large improvements in accuracy at much lower
              computational cost, i.e. it takes less than a day to learn high
              quality word vectors from a 1.6 billion words data set.
              Furthermore, we show that these vectors provide state-of-the-art
              performance on our test set for measuring syntactic and semantic
              word similarities.",
  month    =  jan,
  year     =  2013
}

@ARTICLE{Radford2021-hb,
  title         = "Learning transferable visual models from natural language
                   supervision",
  author        = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and
                   Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and
                   Sastry, Girish and Askell, Amanda and Mishkin, Pamela and
                   Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
  journal       = "arXiv [cs.CV]",
  abstract      = "State-of-the-art computer vision systems are trained to
                   predict a fixed set of predetermined object categories. This
                   restricted form of supervision limits their generality and
                   usability since additional labeled data is needed to specify
                   any other visual concept. Learning directly from raw text
                   about images is a promising alternative which leverages a
                   much broader source of supervision. We demonstrate that the
                   simple pre-training task of predicting which caption goes
                   with which image is an efficient and scalable way to learn
                   SOTA image representations from scratch on a dataset of 400
                   million (image, text) pairs collected from the internet.
                   After pre-training, natural language is used to reference
                   learned visual concepts (or describe new ones) enabling
                   zero-shot transfer of the model to downstream tasks. We study
                   the performance of this approach by benchmarking on over 30
                   different existing computer vision datasets, spanning tasks
                   such as OCR, action recognition in videos, geo-localization,
                   and many types of fine-grained object classification. The
                   model transfers non-trivially to most tasks and is often
                   competitive with a fully supervised baseline without the need
                   for any dataset specific training. For instance, we match the
                   accuracy of the original ResNet-50 on ImageNet zero-shot
                   without needing to use any of the 1.28 million training
                   examples it was trained on. We release our code and
                   pre-trained model weights at https://github.com/OpenAI/CLIP.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Touvron2023-sp,
  title         = "Llama 2: Open foundation and fine-tuned chat models",
  author        = "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert,
                   Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov,
                   Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale,
                   Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian
                   Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David
                   and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller,
                   Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman
                   and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and
                   Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa,
                   Madian and Kloumann, Isabel and Korenev, Artem and Koura,
                   Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and
                   Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao,
                   Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra,
                   Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew
                   and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan
                   and Schelten, Alan and Silva, Ruan and Smith, Eric Michael
                   and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang,
                   Binh and Taylor, Ross and Williams, Adina and Kuan, Jian
                   Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and
                   Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and
                   Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert
                   and Edunov, Sergey and Scialom, Thomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this work, we develop and release Llama 2, a collection of
                   pretrained and fine-tuned large language models (LLMs)
                   ranging in scale from 7 billion to 70 billion parameters. Our
                   fine-tuned LLMs, called Llama 2-Chat, are optimized for
                   dialogue use cases. Our models outperform open-source chat
                   models on most benchmarks we tested, and based on our human
                   evaluations for helpfulness and safety, may be a suitable
                   substitute for closed-source models. We provide a detailed
                   description of our approach to fine-tuning and safety
                   improvements of Llama 2-Chat in order to enable the community
                   to build on our work and contribute to the responsible
                   development of LLMs.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Gemini-Team2023-ux,
  title         = "Gemini: A family of highly capable multimodal models",
  author        = "{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and
                   Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and
                   Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and
                   Millican, Katie and Silver, David and Johnson, Melvin and
                   Antonoglou, Ioannis and Schrittwieser, Julian and Glaese,
                   Amelia and Chen, Jilin and Pitler, Emily and Lillicrap,
                   Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy,
                   James and Isard, Michael and Barham, Paul R and Hennigan, Tom
                   and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and
                   Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer,
                   Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub,
                   Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and
                   Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and
                   Kane, Patrick and Chan, Betty and Faruqui, Manaal and
                   Severyn, Aliaksei and Lin, Hanzhao and Li, Yaguang and Cheng,
                   Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia
                   and Sun, Pei and Tran, Dustin and Bagri, Sumit and
                   Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras
                   and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy,
                   Aurelien and Ganapathy, Harish and Zheng, Steven and Choe,
                   Hyunjeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and
                   Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and
                   Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey,
                   Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey,
                   Laurent El and Zhang, Yujing and Sercinoglu, Olcan and
                   Tucker, George and Piqueras, Enrique and Krikun, Maxim and
                   Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and
                   Roelofs, Becca and White, Anaïs and Andreassen, Anders and
                   von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and
                   Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and
                   Frechette, Alexandre and Smith, Charlotte and Culp, Laura and
                   Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and
                   Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and
                   Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao,
                   Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and
                   Bloniarz, Adam and Rae, Jack W and Lu, Han and Sifre, Laurent
                   and Maggioni, Marcello and Alcober, Fred and Garrette, Dan
                   and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and
                   Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and
                   Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar,
                   Gaurav Singh and Senter, Evan and Chadwick, Martin and
                   Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and
                   Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy
                   and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad,
                   Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai,
                   Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin,
                   Michael and Casas, Diego de Las and Valter, Dasha and Tao,
                   Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and
                   Reitter, David and Chen, Mianna and Brennan, Jenny and
                   Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita,
                   Gabriela and Labanowski, Jane and Rao, Abhi and Winkler,
                   Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska,
                   Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie
                   and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and
                   Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood,
                   Zoe and Briukhov, Anton and Webson, Albert and Ganapathy,
                   Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang,
                   Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun,
                   Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman,
                   Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy
                   and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and
                   Han, Kehang and Humphreys, Peter and Sellam, Thibault and
                   Bradbury, James and Godbole, Varun and Samangooei, Sina and
                   Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M R
                   and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason
                   and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan,
                   Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam,
                   Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush
                   and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh
                   and Neitz, Alexander and Abbas, Zaheer and York, Sarah and
                   Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and
                   Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy
                   and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and
                   Prost, Flavien and He, Luheng and Monteiro, Marianne and
                   Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia,
                   Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de
                   Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and
                   Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and
                   Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and
                   Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan,
                   Devendra and Amplayo, Reinald Kim and Swanson, Craig and
                   Petrova, Dessie and Narayan, Shashi and Guez, Arthur and
                   Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and
                   Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia,
                   Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg
                   and Keeling, James and Georgiev, Petko and Mincu, Diana and
                   Wu, Boxi and Haykal, Salem and Saputro, Rachel and
                   Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and
                   Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and
                   Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and
                   Agrawal, Priyanka and Castro-Ros, Alex and van den Driessche,
                   George and Wang, Tao and Yang, Fan and Chang, Shuo-Yiin and
                   Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang,
                   Guodong and Farhan, Wael and Sharman, Michael and Natsev,
                   Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and
                   Cao, Kris and Shakeri, Siamak and Butterfield, Christina and
                   Chung, Justin and Rubenstein, Paul Kishan and Agrawal,
                   Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc,
                   Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren
                   and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez,
                   Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti,
                   Andrea and Trebacz, Maja and Robinson, Kevin and Katariya,
                   Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan
                   and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and
                   Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and
                   Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee,
                   Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and
                   Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and
                   Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and
                   Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and
                   Besley, James and Chung, Da-Woon and Dozat, Timothy and
                   Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su,
                   Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and
                   Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and
                   Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and
                   Tomasev, Nenad and Xing, Jinwei and Greer, Christina and
                   Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang,
                   Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and
                   Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and
                   Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and
                   Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan,
                   Charline Le and Haridasan, Krishna and Marathe, Amit and
                   Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and
                   Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang,
                   Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund,
                   Lars Lowe and Cevey, Sébastien and Gleicher, Zach and
                   Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and
                   Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and
                   Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate
                   and Chang, Michael B and Recasens, Adrià and Caine, Ben and
                   Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and
                   Gergely, Anita and Frye, Justin and Ramasesh, Vinay and
                   Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy,
                   Subhrajit and Dyer, Ethan and Campos, Víctor Campos and
                   Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White,
                   Elspeth and Mustafa, Basil and Lang, Oran and Jindal,
                   Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles,
                   Sergi and Hemsley, Ross and Thornton, Gregory and Feng,
                   Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker,
                   Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh,
                   Mohammad and Svensson, James and Bileschi, Max and Patil,
                   Piyush and Anand, Ankesh and Ring, Roman and Tsihlas,
                   Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby
                   and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira
                   and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and
                   Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and
                   Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James
                   and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi
                   and Ives, Richard and Hasson, Yana and Noland, Eric and Cao,
                   Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and
                   Sottiaux, Thibault and Paganini, Michela and Lespiau,
                   Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and
                   Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane,
                   Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew
                   and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and
                   Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and
                   Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb
                   and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De
                   Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and
                   Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and
                   Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and
                   Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov,
                   Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran,
                   Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and
                   Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and
                   Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi,
                   Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang,
                   Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara
                   and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie
                   and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and
                   Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou,
                   Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea
                   and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and
                   Toor, Tej and Chen, Tina and Anklin, Valentin and Wang,
                   Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin
                   and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and
                   Kishore, Arun and Adamek, Jakub and Mercado, Tyler and
                   Mallinson, Jonathan and Wandekar, Siddhinita and Cagle,
                   Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser,
                   Clemens and Mukha, Maksim and Sun, Botu and Mohammad,
                   Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani,
                   Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and
                   David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii
                   and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav,
                   Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh
                   and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and
                   Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and
                   Dai, Zihang and He, Kyle and von Dincklage, Daniel and
                   Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and
                   Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G
                   and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev,
                   Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta,
                   Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan
                   Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet,
                   François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric
                   and Teixeira, Elico and Fritze, Matthew and Bertolini,
                   Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and
                   Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and
                   Chang, Max and Sanders, Jason and Wilson, Roopa and Wu,
                   Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi,
                   Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming
                   and Luong, Thang and Benjamin, Seth and Lee, Jasmine and
                   Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and
                   Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and
                   Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei
                   and Bacon, Geoff and Greene, David and Mirylenka, Daniil and
                   Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and
                   Andermatt, Samuel and Siegler, Patrick and Horn, Ben and
                   Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei
                   ``louis and Selvatici, Marco and Silva, Pedro and Wang,
                   Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey
                   and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik
                   and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien
                   and Friso, Linda and Stambler, Adam and Kurzrok, Adam and
                   Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan,
                   Z J and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and
                   Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel
                   and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi,
                   Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja,
                   Niharika and Saxena, Pranab and Dooley, Dan and Potharaju,
                   Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand
                   and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu,
                   Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and
                   Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger,
                   Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski,
                   Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and
                   Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and
                   Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford,
                   Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru
                   and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis
                   and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and
                   Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik
                   and Kwak, Chester and Lv, Pallavi and Velury, Sarmishta and
                   Choudhury, Himadri and Hall, Jamie and Shah, Premal and
                   Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou,
                   Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur,
                   Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel,
                   Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei
                   and Barua, Aditya and Ji, Colin and Park, Ji Ho and
                   Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and
                   Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and
                   Rzadkowski, Wojciech and Macintosh, Fiona and Shagin,
                   Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing
                   and Shah, Pararth and Bi, Yingying and Dankovics, Attila and
                   Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and
                   Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and
                   Chung, Raynald and Yang, Kai and Balani, Nihal and
                   Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew
                   and Alcalde, Héctor Fernández and Makarov, Peter and Chen,
                   Will and Stella, Antonio and Snijders, Liselotte and Mandl,
                   Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and
                   Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and
                   Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and
                   Mittal, Sushil and Udathu, Akhil and Christensen, Janara and
                   Verma, Vishal and Irving, Zach and Santucci, Andreas and
                   Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin
                   and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and
                   Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut
                   and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang,
                   Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang,
                   Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy
                   and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and
                   Jain, Rishub and Uesato, Jonathan and Datta, Romina and
                   Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk,
                   Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit
                   and Azzam, Michael and Johnson, Matthew and Paszke, Adam and
                   Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin,
                   Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and
                   Vieillard, Nino and Park, Jane and Zhang, Jiageng and
                   Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and
                   Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei
                   and Evens, Jonathan and Isaac, William and Irving, Geoffrey
                   and Loper, Edward and Fink, Michael and Arkatkar, Isha and
                   Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and
                   Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu,
                   Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto
                   and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman
                   and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and
                   Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and
                   Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David
                   and Goedeckemeyer, Adrian and Gierke, Willi and Jafari,
                   Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana
                   Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya,
                   Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu,
                   Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey
                   and Cui, Albert and Lin, Tian and Wu, Marcus and Aguilar,
                   Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng,
                   Ginger and Abellan, Elena Allica and Zhang, Mingyang and
                   Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and
                   Repina, Alena and Wu, Xihui and van der Weide, Tom and
                   Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and
                   Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper,
                   Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan
                   and Vijayakumar, Anitha and Andor, Daniel and Valenzuela,
                   Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi
                   and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and
                   Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy
                   and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng,
                   Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou
                   and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga
                   and Keller, Orgad and Reid, David and Finchelstein, Daniel
                   and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and
                   Dadashi, Robert and Gaffney, Colin and Franko, Ken and
                   Bulanova, Anna and Leblond, Rémi and Chung, Shirley and
                   Askham, Harry and Cobo, Luis C and Xu, Kelvin and Fischer,
                   Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris
                   and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and
                   Forbes, Hannah and Banarse, Dylan and Tung, Zora and
                   Omernick, Mark and Bishop, Colton and Sterneck, Rachel and
                   Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno,
                   Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz,
                   Daniel J and Polozov, Alex and Krakovna, Victoria and Brown,
                   Sasha and Bateni, Mohammadhossein and Duan, Dennis and
                   Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist,
                   Matthieu and Girgin, Ser Tan and Li, Hui and Ye, Jiayu and
                   Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp,
                   James and Yew, Christopher and Sinopalnikov, Danila and
                   Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu,
                   Kathy and Miller, David and Sonnerat, Nicolas and Vnukov,
                   Denis and Greig, Rory and Beattie, Jennifer and Caveness,
                   Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy,
                   Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and
                   Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan
                   and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and
                   Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and
                   Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind,
                   Chen and Woodman, Oliver and Carpenter, John and
                   Papamakarios, George and Kemp, Rupert and Kafle, Sushant and
                   Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu,
                   Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton,
                   Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li,
                   Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar
                   and Uria, Benigno and Ko, Yeongil and Knight, Laura and
                   Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi
                   and Li, Yeqing and Levine, Nir and Stolovich, Ariel and
                   Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim,
                   Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie
                   and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann,
                   Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora,
                   Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder,
                   Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian
                   and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu
                   and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and
                   Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li,
                   Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and
                   Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and
                   Choquette-Choo, Christopher A and Li, Yunjie and Lu, T J and
                   Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani
                   and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and
                   Desjardins, Guillaume and Cornero, Marco and Robenek, Brona
                   and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish
                   and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah,
                   Alireza and Rivière, Morgane and Walton, Alanna and Crepy,
                   Clément and Parrish, Alicia and Zhou, Zongwei and Farabet,
                   Clement and Radebaugh, Carey and Srinivasan, Praveen and van
                   der Salm, Claudia and Fidjeland, Andreas and Scellato,
                   Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska,
                   Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom
                   and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex
                   and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and
                   Odoom, Seth and Loher, Lucia and Cotruta, Victor and
                   Yenugula, Madhavi and Grewe, Dominik and Petrushkina,
                   Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky,
                   Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and
                   Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan
                   and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and
                   Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and
                   Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei,
                   Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty,
                   Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak,
                   Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and
                   Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma,
                   Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie
                   and Manyika, James and Amiri, Keyvan and Kim, Yelin and
                   Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni,
                   Nilesh and Madras, David and Guo, Mandy and Waters, Austin
                   and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and
                   Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng
                   and Mansour, Riham and Gelman, Jason and Xu, Yang and
                   Polovets, George and Liu, Ji and Cai, Honglong and Chen,
                   Warren and Sheng, Xianghai and Xue, Emily and Ozair, Sherjil
                   and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop
                   and Wang, Weiren and Wiesinger, Julia and Koukoumidis,
                   Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy,
                   Madhu and Goldenson, Mark and Shah, Parashar and Blake, M K
                   and Yu, Hongkun and Urbanowicz, Anthony and Palomaki,
                   Jennimaria and Fernando, Chrisantha and Durden, Ken and
                   Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and
                   Georgaki, Maria and Raul, Amit and Ruder, Sebastian and
                   Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan,
                   Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker
                   and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and
                   Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley,
                   Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua
                   and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao,
                   Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr,
                   Jean Michel and Preston, Melanie Moranski and Elish,
                   Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and
                   Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M.,
                   Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex
                   and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and
                   Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and
                   Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and
                   Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and
                   Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen,
                   Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila
                   and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer,
                   Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias,
                   Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias
                   and van de Kerkhof, Jan and Pikus, Marcin and Zaher,
                   Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec,
                   Richard and Gatsko, Vitaly and Hirnschall, Christoph and
                   Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and
                   Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania,
                   Keshav and Katyal, Manish and Gupta, Akshay and Parulekar,
                   Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan
                   and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and
                   Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova,
                   Vera and Ghosh, Abhipso and Limonchik, Ben and Urala,
                   Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and
                   Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and
                   Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and
                   Majmundar, Kushal and Alverson, Michael and Kucharski,
                   Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim
                   and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and
                   Kim, Joseph and Sankar, Swetha and Shah, Vineet and
                   Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben
                   and Weidinger, Laura and Vu, Tu and Andreev, Alek and He,
                   Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya,
                   Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu,
                   Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor
                   and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and
                   Vinyals, Oriol",
  journal       = "arXiv [cs.CL]",
  abstract      = "This report introduces a new family of multimodal models,
                   Gemini, that exhibit remarkable capabilities across image,
                   audio, video, and text understanding. The Gemini family
                   consists of Ultra, Pro, and Nano sizes, suitable for
                   applications ranging from complex reasoning tasks to
                   on-device memory-constrained use-cases. Evaluation on a broad
                   range of benchmarks shows that our most-capable Gemini Ultra
                   model advances the state of the art in 30 of 32 of these
                   benchmarks - notably being the first model to achieve
                   human-expert performance on the well-studied exam benchmark
                   MMLU, and improving the state of the art in every one of the
                   20 multimodal benchmarks we examined. We believe that the new
                   capabilities of the Gemini family in cross-modal reasoning
                   and language understanding will enable a wide variety of use
                   cases. We discuss our approach toward post-training and
                   deploying Gemini models responsibly to users through services
                   including Gemini, Gemini Advanced, Google AI Studio, and
                   Cloud Vertex AI.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Github2024-io,
  title       = "Github Copilot",
  author      = "{Github}",
  institution = "Github",
  abstract    = "GitHub is where people build software. More than 100 million
                 people use GitHub to discover, fork, and contribute to over 420
                 million projects.",
  year        =  2024,
  language    = "en"
}

@MISC{Cursor2024-ow,
  title        = "Cursor",
  author       = "{Cursor}",
  booktitle    = "cursor.com",
  abstract     = "The AI Code Editor",
  year         =  2024,
  howpublished = "\url{https://www.cursor.com/}",
  note         = "Accessed: 2024-9-11",
  language     = "en"
}

@MISC{Anthropic2024-dl,
  title        = "What are Artifacts and how do {I} use them?",
  author       = "{Anthropic}",
  booktitle    = "anthropic.com",
  year         =  2024,
  howpublished = "\url{https://support.anthropic.com/en/articles/9487310-what-are-artifacts-and-how-do-i-use-them}",
  note         = "Accessed: 2024-9-11",
  language     = "en"
}

@INPROCEEDINGS{Fan2019-qq,
  title     = "Collabdraw: An environment for collaborative sketching with an
               artificial agent",
  author    = "Fan, Judith E and Dinculescu, Monica and Ha, David",
  booktitle = "Proceedings of the 2019 on Creativity and Cognition",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2019
}

@ARTICLE{Ray2023-fu,
  title     = "{ChatGPT}: A comprehensive review on background, applications,
               key challenges, bias, ethics, limitations and future scope",
  author    = "Ray, Partha Pratim",
  journal   = "Internet of Things and Cyber-Physical Systems",
  publisher = "Elsevier BV",
  volume    =  3,
  pages     = "121--154",
  abstract  = "In recent years, artificial intelligence (AI) and machine
               learning have been transforming the landscape of scientific
               research. Out of which, the chat…",
  month     =  jan,
  year      =  2023,
  language  = "en"
}

@MISC{OpenAI2022-wx,
  title        = "Introducing {ChatGPT}",
  author       = "{OpenAI}",
  booktitle    = "openai.com",
  abstract     = "We’ve trained a model called ChatGPT which interacts in a
                  conversational way. The dialogue format makes it possible for
                  ChatGPT to answer followup questions, admit its mistakes,
                  challenge incorrect premises, and reject inappropriate
                  requests.",
  month        =  nov,
  year         =  2022,
  howpublished = "\url{https://openai.com/index/chatgpt/}",
  note         = "Accessed: 2024-9-11",
  language     = "en"
}

@ARTICLE{Radford2018-fm,
  title    = "Improving Language Understanding by Generative Pre-Training",
  author   = "Radford, Alec and Narasimhan, Karthik",
  abstract = "The general task-agnostic model outperforms discriminatively
              trained models that use architectures speciﬁcally crafted for each
              task, improving upon the state of the art in 9 out of the 12 tasks
              studied. Natural language understanding comprises a wide range of
              diverse tasks such as textual entailment, question answering,
              semantic similarity assessment, and document classiﬁcation.
              Although large unlabeled text corpora are abundant, labeled data
              for learning these speciﬁc tasks is scarce, making it challenging
              for discriminatively trained models to perform adequately. We
              demonstrate that large gains on these tasks can be realized by
              generative pre-training of a language model on a diverse corpus of
              unlabeled text, followed by discriminative ﬁne-tuning on each
              speciﬁc task. In contrast to previous approaches, we make use of
              task-aware input transformations during ﬁne-tuning to achieve
              effective transfer while requiring minimal changes to the model
              architecture. We demonstrate the effectiveness of our approach on
              a wide range of benchmarks for natural language understanding. Our
              general task-agnostic model outperforms discriminatively trained
              models that use architectures speciﬁcally crafted for each task,
              signiﬁcantly improving upon the state of the art in 9 out of the
              12 tasks studied. For instance, we achieve absolute improvements
              of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on
              question answering (RACE), and 1.5\% on textual entailment
              (MultiNLI).",
  year     =  2018,
  language = "en"
}

@ARTICLE{Bahdanau2014-nh,
  title         = "Neural machine translation by jointly learning to align and
                   translate",
  author        = "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
  journal       = "arXiv [cs.CL]",
  abstract      = "Neural machine translation is a recently proposed approach to
                   machine translation. Unlike the traditional statistical
                   machine translation, the neural machine translation aims at
                   building a single neural network that can be jointly tuned to
                   maximize the translation performance. The models proposed
                   recently for neural machine translation often belong to a
                   family of encoder-decoders and consists of an encoder that
                   encodes a source sentence into a fixed-length vector from
                   which a decoder generates a translation. In this paper, we
                   conjecture that the use of a fixed-length vector is a
                   bottleneck in improving the performance of this basic
                   encoder-decoder architecture, and propose to extend this by
                   allowing a model to automatically (soft-)search for parts of
                   a source sentence that are relevant to predicting a target
                   word, without having to form these parts as a hard segment
                   explicitly. With this new approach, we achieve a translation
                   performance comparable to the existing state-of-the-art
                   phrase-based system on the task of English-to-French
                   translation. Furthermore, qualitative analysis reveals that
                   the (soft-)alignments found by the model agree well with our
                   intuition.",
  month         =  sep,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Sutskever2014-jq,
  title         = "Sequence to sequence learning with Neural Networks",
  author        = "Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V",
  journal       = "arXiv [cs.CL]",
  abstract      = "Deep Neural Networks (DNNs) are powerful models that have
                   achieved excellent performance on difficult learning tasks.
                   Although DNNs work well whenever large labeled training sets
                   are available, they cannot be used to map sequences to
                   sequences. In this paper, we present a general end-to-end
                   approach to sequence learning that makes minimal assumptions
                   on the sequence structure. Our method uses a multilayered
                   Long Short-Term Memory (LSTM) to map the input sequence to a
                   vector of a fixed dimensionality, and then another deep LSTM
                   to decode the target sequence from the vector. Our main
                   result is that on an English to French translation task from
                   the WMT'14 dataset, the translations produced by the LSTM
                   achieve a BLEU score of 34.8 on the entire test set, where
                   the LSTM's BLEU score was penalized on out-of-vocabulary
                   words. Additionally, the LSTM did not have difficulty on long
                   sentences. For comparison, a phrase-based SMT system achieves
                   a BLEU score of 33.3 on the same dataset. When we used the
                   LSTM to rerank the 1000 hypotheses produced by the
                   aforementioned SMT system, its BLEU score increases to 36.5,
                   which is close to the previous best result on this task. The
                   LSTM also learned sensible phrase and sentence
                   representations that are sensitive to word order and are
                   relatively invariant to the active and the passive voice.
                   Finally, we found that reversing the order of the words in
                   all source sentences (but not target sentences) improved the
                   LSTM's performance markedly, because doing so introduced many
                   short term dependencies between the source and the target
                   sentence which made the optimization problem easier.",
  month         =  sep,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Graves2013-yv,
  title         = "Generating sequences with recurrent neural networks",
  author        = "Graves, Alex",
  journal       = "arXiv [cs.NE]",
  abstract      = "This paper shows how Long Short-term Memory recurrent neural
                   networks can be used to generate complex sequences with
                   long-range structure, simply by predicting one data point at
                   a time. The approach is demonstrated for text (where the data
                   are discrete) and online handwriting (where the data are
                   real-valued). It is then extended to handwriting synthesis by
                   allowing the network to condition its predictions on a text
                   sequence. The resulting system is able to generate highly
                   realistic cursive handwriting in a wide variety of styles.",
  month         =  aug,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE"
}

@ARTICLE{Pascanu2012-tl,
  title         = "On the difficulty of training Recurrent Neural Networks",
  author        = "Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua",
  journal       = "arXiv [cs.LG]",
  abstract      = "There are two widely known issues with properly training
                   Recurrent Neural Networks, the vanishing and the exploding
                   gradient problems detailed in Bengio et al. (1994). In this
                   paper we attempt to improve the understanding of the
                   underlying issues by exploring these problems from an
                   analytical, a geometric and a dynamical systems perspective.
                   Our analysis is used to justify a simple yet effective
                   solution. We propose a gradient norm clipping strategy to
                   deal with exploding gradients and a soft constraint for the
                   vanishing gradients problem. We validate empirically our
                   hypothesis and proposed solutions in the experimental
                   section.",
  month         =  nov,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Brown1990-jd,
  title     = "A statistical approach to machine translation",
  author    = "Brown, Peter F and Cocke, John and Pietra, Stephen A Della and
               Pietra, Vincent J Della and Jelinek, Fredrick and Lafferty, John
               D and Mercer, Robert L and Roossin, Paul S",
  journal   = "Comput. Linguist.",
  publisher = "MIT Press",
  address   = "Cambridge, MA, USA",
  volume    =  16,
  number    =  2,
  pages     = "79--85",
  abstract  = "In this paper, we present a statistical approach to machine
               translation. We describe the application of our approach to
               translation from French to English and give preliminary results.",
  month     =  jun,
  year      =  1990
}

@ARTICLE{Lin2023-jd,
  title         = "Beyond prompts: Exploring the design space of
                   mixed-initiative co-creativity systems",
  author        = "Lin, Zhiyu and Ehsan, Upol and Agarwal, Rohan and Dani,
                   Samihan and Vashishth, Vidushi and Riedl, Mark",
  journal       = "arXiv [cs.AI]",
  abstract      = "Generative Artificial Intelligence systems have been
                   developed for image, code, story, and game generation with
                   the goal of facilitating human creativity. Recent work on
                   neural generative systems has emphasized one particular means
                   of interacting with AI systems: the user provides a
                   specification, usually in the form of prompts, and the AI
                   system generates the content. However, there are other
                   configurations of human and AI coordination, such as
                   co-creativity (CC) in which both human and AI systems can
                   contribute to content creation, and mixed-initiative (MI) in
                   which both human and AI systems can initiate content changes.
                   In this paper, we define a hypothetical human-AI
                   configuration design space consisting of different means for
                   humans and AI systems to communicate creative intent to each
                   other. We conduct a human participant study with 185
                   participants to understand how users want to interact with
                   differently configured MI-CC systems. We find out that MI-CC
                   systems with more extensive coverage of the design space are
                   rated higher or on par on a variety of creative and
                   goal-completion metrics, demonstrating that wider coverage of
                   the design space can improve user experience and achievement
                   when using the system; Preference varies greatly between
                   expertise groups, suggesting the development of adaptive,
                   personalized MI-CC systems; Participants identified new
                   design space dimensions including scrutability -- the ability
                   to poke and prod at models -- and explainability.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@INPROCEEDINGS{Shneiderman1997-tv,
  title     = "Direct manipulation for comprehensible, predictable and
               controllable user interfaces",
  author    = "Shneiderman, Ben",
  booktitle = "Proceedings of the 2nd international conference on Intelligent
               user interfaces - IUI '97",
  publisher = "ACM Press",
  address   = "New York, New York, USA",
  abstract  = "Direct manipulation user interfaces have proven their worth over
               two decades, but they are still in their youth. Dramatic
               opportunities exist to develop direct manipulation programming to
               create end-user programming tools, dynamic queries to perform
               information search in large databases, and information
               visualization to support network database browsing. Direct
               manipulation depends on visual representation of the objects and
               actions of interest, physical . actions or pointing instead of
               complex syntax, and rapid incremental reversible operations whose
               effect on the object of interest is immediately visible. This
               strategy can lead to user interfaces that are comprehensible,
               predictable and controllable. Direct manipulation interfaces are
               seen as more likely candidates to influence advanced user
               interfaces than adaptive, autonomous, intelligent agents. User
               control and responsibility are highly desirable. Note: This paper
               is adapted, with permission of the publisher, from the
               forthcoming book: Designing the User Znte~ace: Strategies for
               Effective Human-Computer Interaction (3rd Edition), Addison
               Wesley, Reading, MA (1997).",
  year      =  1997,
  language  = "en"
}

@INPROCEEDINGS{Masson2024-nt,
  title     = "{DirectGPT}: A direct manipulation interface to interact with
               large language models",
  author    = "Masson, Damien and Malacria, Sylvain and Casiez, Géry and Vogel,
               Daniel",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--16",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Lee2024-hd,
  title     = "A Design Space for Intelligent and Interactive Writing Assistants",
  author    = "Lee, Mina and Gero, Katy Ilonka and Chung, John Joon Young and
               Shum, Simon Buckingham and Raheja, Vipul and Shen, Hua and
               Venugopalan, Subhashini and Wambsganss, Thiemo and Zhou, David
               and Alghamdi, Emad A and August, Tal and Bhat, Avinash and
               Choksi, Madiha Zahrah and Dutta, Senjuti and Guo, Jin L C and
               Hoque, Md Naimul and Kim, Yewon and Knight, Simon and Neshaei,
               Seyed Parsa and Shibani, Antonette and Shrivastava, Disha and
               Shroff, Lila and Sergeyuk, Agnia and Stark, Jessi and Sterman,
               Sarah and Wang, Sitong and Bosselut, Antoine and Buschek, Daniel
               and Chang, Joseph Chee and Chen, Sherol and Kreminski, Max and
               Park, Joonsuk and Pea, Roy and Rho, Eugenia Ha Rim and Shen,
               Zejiang and Siangliulue, Pao",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  7,
  pages     = "1--35",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@INPROCEEDINGS{Ding2024-ta,
  title     = "Advancing {GUI} for generative {AI}: Charting the design space of
               human-{AI} interactions through task creativity and complexity",
  author    = "Ding, Zijian",
  booktitle = "Companion Proceedings of the 29th International Conference on
               Intelligent User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  mar,
  year      =  2024,
  language  = "en"
}

@MISC{Purohit2024-nx,
  title        = "Why {I} Avoided {AI—And} How {I} Finally Embraced It",
  author       = "Purohit, Rhea",
  abstract     = "Using a new technology can be hard. Here's what you can do
                  about it.",
  month        =  aug,
  year         =  2024,
  howpublished = "\url{https://every.to/learning-curve/why-i-avoided-ai-and-how-i-finally-embraced-it?utm\_source=www.theneurondaily.com\&utm\_medium=newsletter\&utm\_campaign=world-s-first-ai-mayor\&\_bhlid=a2b8f3444d7a70e5559413899dcf404d8e2b5e5a}",
  note         = "Accessed: 2024-8-23"
}

@INPROCEEDINGS{Clark2018-yf,
  title     = "Creative writing with a machine in the loop",
  author    = "Clark, Elizabeth and Ross, Anne Spencer and Tan, Chenhao and Ji,
               Yangfeng and Smith, Noah A",
  booktitle = "23rd International Conference on Intelligent User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  mar,
  year      =  2018
}

@INPROCEEDINGS{Amershi2019-wu,
  title     = "Guidelines for Human-{AI} Interaction",
  author    = "Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and
               Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh,
               Jina and Iqbal, Shamsi and Bennett, Paul N and Inkpen, Kori and
               Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Paper 3",
  pages     = "1--13",
  abstract  = "Advances in artificial intelligence (AI) frame opportunities and
               challenges for user interface design. Principles for human-AI
               interaction have been discussed in the human-computer interaction
               community for over two decades, but more study and innovation are
               needed in light of advances in AI and the growing uses of AI
               technologies in human-facing applications. We propose 18
               generally applicable design guidelines for human-AI interaction.
               These guidelines are validated through multiple rounds of
               evaluation including a user study with 49 design practitioners
               who tested the guidelines against 20 popular AI-infused products.
               The results verify the relevance of the guidelines over a
               spectrum of interaction scenarios and reveal gaps in our
               knowledge, highlighting opportunities for further research. Based
               on the evaluations, we believe the set of design guidelines can
               serve as a resource to practitioners working on the design of
               applications and features that harness AI technologies, and to
               researchers interested in the further development of human-AI
               interaction design principles.",
  series    = "CHI '19",
  month     =  may,
  year      =  2019,
  keywords  = "ai-infused systems, design guidelines, human-ai interaction"
}

@ARTICLE{Abbas2024-sf,
  title     = "Is it harmful or helpful? Examining the causes and consequences
               of generative {AI} usage among university students",
  author    = "Abbas, Muhammad and Jam, Farooq Ahmed and Khan, Tariq Iqbal",
  journal   = "International Journal of Educational Technology in Higher
               Education",
  publisher = "SpringerOpen",
  volume    =  21,
  number    =  1,
  pages     = "1--22",
  abstract  = "While the discussion on generative artificial intelligence, such
               as ChatGPT, is making waves in academia and the popular press,
               there is a need for more insight into the use of ChatGPT among
               students and the potential harmful or beneficial consequences
               associated with its usage. Using samples from two studies, the
               current research examined the causes and consequences of ChatGPT
               usage among university students. Study 1 developed and validated
               an eight-item scale to measure ChatGPT usage by conducting a
               survey among university students (N = 165). Study 2 used a
               three-wave time-lagged design to collect data from university
               students (N = 494) to further validate the scale and test the
               study’s hypotheses. Study 2 also examined the effects of academic
               workload, academic time pressure, sensitivity to rewards, and
               sensitivity to quality on ChatGPT usage. Study 2 further examined
               the effects of ChatGPT usage on students’ levels of
               procrastination, memory loss, and academic performance. Study 1
               provided evidence for the validity and reliability of the ChatGPT
               usage scale. Furthermore, study 2 revealed that when students
               faced higher academic workload and time pressure, they were more
               likely to use ChatGPT. In contrast, students who were sensitive
               to rewards were less likely to use ChatGPT. Not surprisingly, use
               of ChatGPT was likely to develop tendencies for procrastination
               and memory loss and dampen the students’ academic performance.
               Finally, academic workload, time pressure, and sensitivity to
               rewards had indirect effects on students’ outcomes through
               ChatGPT usage.",
  month     =  feb,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Heaven2023-wa,
  title    = "The inside story of how {ChatGPT} was built from the people who
              made it",
  author   = "Heaven, Will Douglas",
  journal  = "MIT Technology Review",
  abstract = "Exclusive conversations that take us behind the scenes of a
              cultural phenomenon.",
  month    =  mar,
  year     =  2023,
  language = "en"
}

@MISC{noauthor_undated-jm,
  title        = "{SerendipityLM}",
  abstract     = "Interactive evolutionary exploration of generative design
                  spaces with large language models",
  howpublished = "\url{https://samim.io/studio/work/serendipityLM/}",
  note         = "Accessed: 2024-8-5",
  language     = "en"
}

@MISC{Sacasas2024-fj,
  title        = "Re-sourcing the mind",
  author       = "Sacasas, L M",
  booktitle    = "The Convivial Society",
  abstract     = "The Convivial Society: Vol. 5, No.",
  month        =  aug,
  year         =  2024,
  howpublished = "\url{https://theconvivialsociety.substack.com/p/re-sourcing-the-mind?utm\_campaign=email-half-post\&r=1aer3\&utm\_source=substack\&utm\_medium=email}",
  note         = "Accessed: 2024-8-1",
  language     = "en"
}

@ARTICLE{Vinchon2023-gh,
  title     = "Artificial intelligence \& creativity: A manifesto for
               collaboration",
  author    = "Vinchon, Florent and Lubart, Todd and Bartolotta, Sabrina and
               Gironnay, Valentin and Botella, Marion and Bourgeois-Bougrine,
               Samira and Burkhardt, Jean-Marie and Bonnardel, Nathalie and
               Corazza, Giovanni Emanuele and Glăveanu, Vlad and Hanchett
               Hanson, Michael and Ivcevic, Zorana and Karwowski, Maciej and
               Kaufman, James C and Okada, Takeshi and Reiter-Palmon, Roni and
               Gaggioli, Andrea",
  journal   = "J. Creat. Behav.",
  publisher = "Wiley",
  volume    =  57,
  number    =  4,
  pages     = "472--484",
  abstract  = "ABSTRACTWith the advent of artificial intelligence (AI), the
               field of creativity faces new opportunities and challenges. This
               manifesto explores several scenarios of human–machine
               collaboration on creative tasks and proposes “fundamental laws of
               generative AI” to reinforce the responsible and ethical use of AI
               in the creativity field. Four scenarios are proposed and
               discussed: “Co‐Cre‐AI‐tion,” “Organic,” “Plagiarism 3.0,” and
               “Shut down,” each illustrating different possible futures based
               on the collaboration between humans and machines. In addition, we
               have incorporated an AI‐generated manifesto that also highlights
               important themes, ranging from accessibility and ethics to
               cultural sensitivity. The fundamental laws proposed aim to
               prevent AIs from generating harmful content and competing
               directly with humans. Creating labels and laws are also
               highlighted to ensure responsible use of AIs. The positive future
               of creativity and AI lies in a harmonious collaboration that can
               benefit everyone, potentially leading to a new level of creative
               productivity respecting ethical considerations and human values
               during the creative process.",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Yuan2022-kb,
  title     = "Wordcraft: Story Writing With Large Language Models",
  author    = "Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne",
  booktitle = "Proceedings of the 27th International Conference on Intelligent
               User Interfaces",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "841--852",
  abstract  = "The latest generation of large neural language models such as
               GPT-3 have achieved new levels of performance on benchmarks for
               language understanding and generation. These models have even
               demonstrated an ability to perform arbitrary tasks without
               explicit training. In this work, we sought to learn how people
               might use such models in the process of creative writing. We
               built Wordcraft, a text editor in which users collaborate with a
               generative language model to write a story. We evaluated
               Wordcraft with a user study in which participants wrote short
               stories with and without the tool. Our results show that large
               language models enable novel co-writing experiences. For example,
               the language model is able to engage in open-ended conversation
               about the story, respond to writers’ custom requests expressed in
               natural language (such as ”rewrite this text to be more
               Dickensian”), and generate suggestions that serve to unblock
               writers in the creative process. Based on these results, we
               discuss design implications for future human-AI co-writing
               systems.",
  series    = "IUI '22",
  month     =  mar,
  year      =  2022,
  keywords  = "NLP"
}

@ARTICLE{Ippolito2022-mf,
  title     = "Creative writing with an {AI}-powered writing assistant:
               Perspectives from professional writers",
  author    = "Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Burnam,
               Sehmon",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "Recent developments in natural language generation (NLG) using
               neural language models have brought us closer than ever to the
               goal of building AI-powered creative writing tools. However, most
               prior work on human-AI collaboration in the creative writing
               domain has evaluated new systems with amateur writers, typically
               in contrived user studies of limited scope. In this work, we
               commissioned 13 professional, published writers from a diverse
               set of creative writing backgrounds to craft stories using
               Wordcraft, a text editor with built-in AI-powered writing
               assistance tools. Using interviews and participant journals, we
               discuss the potential of NLG to have significant impact in the
               creative writing domain--especially with respect to
               brainstorming, generation of story details, world-building, and
               research assistance. Experienced writers, more so than amateurs,
               typically have well-developed systems and methodologies for
               writing, as well as distinctive voices and target audiences. Our
               work highlights the challenges in building for these writers; NLG
               technologies struggle to preserve style and authorial voice, and
               they lack deep understanding of story contents. In order for
               AI-powered writing assistants to realize their full potential, it
               is essential that they take into account the diverse goals and
               expertise of human writers.",
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Lehmann2022-kr,
  title     = "Suggestion lists vs. Continuous generation: Interaction design
               for writing with generative models on mobile devices affect text
               length, wording and perceived authorship",
  author    = "Lehmann, Florian and Markert, Niklas and Dang, Hai and Buschek,
               Daniel",
  booktitle = "Mensch und Computer 2022",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Two user interfaces for writing with AI on mobile devices, which
               manipulate levels of initiative and control are designed and
               compared, which add new empirical evidence on the impact of UI
               design decisions on user experience and output with co-creative
               systems. Neural language models have the potential to support
               human writing. However, questions remain on their integration and
               influence on writing and output. To address this, we designed and
               compared two user interfaces for writing with AI on mobile
               devices, which manipulate levels of initiative and control: 1)
               Writing with continuously generated text, the AI adds text
               word-by-word and user steers. 2) Writing with suggestions, the AI
               suggests phrases and user selects from a list. In a supervised
               online study (N=18), participants used these prototypes and a
               baseline without AI. We collected touch interactions, ratings on
               inspiration and authorship, and interview data. With AI
               suggestions, people wrote less actively, yet felt they were the
               author. Continuously generated text reduced this perceived
               authorship, yet increased editing behavior. In both designs, AI
               increased text length and was perceived to influence wording. Our
               findings add new empirical evidence on the impact of UI design
               decisions on user experience and output with co-creative systems.",
  month     =  sep,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Nguyen2024-pc,
  title     = "Human-{AI} collaboration patterns in {AI}-assisted academic
               writing",
  author    = "Nguyen, Andy and Hong, Yvonne and Dang, Belle and Huang, Xiaoshan",
  journal   = "Stud. High. Educ.",
  publisher = "Informa UK Limited",
  volume    =  49,
  number    =  5,
  pages     = "847--864",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@MISC{Demke_undated-aw,
  title        = "Overcoming algorithmic bias as a measure of computational
                  creativity",
  author       = "Demke, Jonathan and Ventura, Dan",
  howpublished = "\url{https://computationalcreativity.net/iccc24/papers/ICCC24\_paper\_42.pdf}",
  note         = "Accessed: 2024-7-9"
}

@ARTICLE{Copet2023-mh,
  title         = "Simple and Controllable Music Generation",
  author        = "Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and
                   Kant, David and Synnaeve, Gabriel and Adi, Yossi and
                   Défossez, Alexandre",
  journal       = "arXiv [cs.SD]",
  abstract      = "We tackle the task of conditional music generation. We
                   introduce MusicGen, a single Language Model (LM) that
                   operates over several streams of compressed discrete music
                   representation, i.e., tokens. Unlike prior work, MusicGen is
                   comprised of a single-stage transformer LM together with
                   efficient token interleaving patterns, which eliminates the
                   need for cascading several models, e.g., hierarchically or
                   upsampling. Following this approach, we demonstrate how
                   MusicGen can generate high-quality samples, both mono and
                   stereo, while being conditioned on textual description or
                   melodic features, allowing better controls over the generated
                   output. We conduct extensive empirical evaluation,
                   considering both automatic and human studies, showing the
                   proposed approach is superior to the evaluated baselines on a
                   standard text-to-music benchmark. Through ablation studies,
                   we shed light over the importance of each of the components
                   comprising MusicGen. Music samples, code, and models are
                   available at https://github.com/facebookresearch/audiocraft",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD"
}

@ARTICLE{Eicher2024-fv,
  title         = "Compensatory Biases Under Cognitive Load: Reducing Selection
                   Bias in Large Language Models",
  author        = "Eicher, J E and Irgolič, R F",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) like gpt-3.5-turbo and
                   claude-instant-1.2 have become instrumental in interpreting
                   and executing semantic-based tasks. Unfortunately, these
                   models' inherent biases, akin to human cognitive biases,
                   adversely affect their performance. Particularly affected is
                   object selection from lists; a fundamental operation in
                   digital navigation and decision-making. This research
                   critically examines these biases and quantifies the effects
                   on a representative list selection task. To explore these
                   biases, we conducted a series of controlled experiments,
                   manipulating temperature, list length, object identity,
                   object type, prompt complexity, and model. This enabled us to
                   isolate and measure the influence of the biases on selection
                   behavior. Our findings show that bias structure is strongly
                   dependent on the model, with object type modulating the
                   magnitude of the effect. With a strong primacy effect,
                   causing the first objects in a list to be disproportionately
                   represented in outputs. Furthermore the usage of guard rails,
                   a prompt engineering method of ensuring a response structure,
                   can increase bias and decrease instruction adherence when
                   combined with a selection task. The bias is ablated when the
                   guard rail step is separated from the list sampling step,
                   lowering the complexity of each individual task. The
                   implications of this research are two-fold, practically
                   providing a guide for designing unbiased LLM applications and
                   theoretically suggesting that LLMs experience a form of
                   cognitive load compensated for by increasing bias.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Polli2004-bf,
  title    = "Atmospherics/Weatherworks: A Multi-Channel Storm Sonification
              Project",
  author   = "Polli, Andrea",
  journal  = "Int Conf Audit Disp",
  abstract = "Atmospherics/Weather Works is an interdisciplinary project in the
              sonification of storms and other meteorological events generated
              directly from data produced by a highly detailed and physically
              accurate model of weather systems used for research and
              forecasting. This paper discusses the background, conception, and
              execution of a series of sonifications of a historical hurricane
              and winter snowstorm that resulted in several performances, stereo
              recordings, a public multi-channel spatialized sound installation,
              and an online interactive sound listening environment.",
  month    =  jul,
  year     =  2004
}

@INPROCEEDINGS{Quinn2001-wc,
  title     = "The Climate Symphony and Other Sonifications of Ice Core, Radar,
               {DNA}, Seismic and Solar Wind Data",
  author    = "Quinn, M",
  booktitle = "Proceedings of the 2001 International Conference on Auditory
               Display, Helsinki Finnland",
  year      =  2001
}

@ARTICLE{Talbot2011-jr,
  title     = "The Vivaldi compendium",
  author    = "Talbot, M",
  journal   = "Choice",
  publisher = "American Library Association",
  volume    =  49,
  number    =  03,
  pages     = "49--1209--49--1209",
  abstract  = "The Vivaldi Compendium will serve as the most reliable and
               up-to-date source of quick reference on the composer Antonio
               Vivaldi and his music. This takes the form of a dictionary
               listing persons, places, musical works and many other topics
               connected with Vivaldi; its alphabetically arranged entries are
               copiously cross-referenced so as to guide the reader towards
               related topics. The Vivaldi Compendium also provides a gateway to
               further reading. This is achieved via an extensive bibliography,
               to which reference is made in most of the dictionary entries.
               These two sections are complemented by an article-length
               biography of the composer and a carefully organized list of his
               works. Knowledge about Vivaldi and his music is still advancing
               at an incredible rate - many discoveries occurred while the book
               was in preparation - and every effort has been made to ensure
               that The Vivaldi Compendium represents the latest in Vivaldi
               research, drawing on the author's close involvement with Vivaldi
               and Venetian music over four decades. MICHAEL TALBOT is Emeritus
               Professor of Music at the University of Liverpool and a Fellow of
               the British Academy. He is known internationally for his studies
               of late-baroque Italian music, which include recent books on
               Vivaldi's chamber cantatas (2003) and the same composer's fugal
               writing (2007).",
  month     =  nov,
  year      =  2011,
  language  = "en"
}

@MISC{Novak_undated-yd,
  title        = "Former Pixar Animator Gives One Big Reason {AI} Video Won’t
                  Work in Hollywood",
  author       = "Novak, Matt",
  howpublished = "\url{https://gizmodo.com.au/2024/04/former-pixar-animator-gives-one-big-reason-ai-video-wont-work-in-hollywood/}",
  note         = "Accessed: 2024-4-30"
}

@INPROCEEDINGS{Kantosalo2021-mp,
  title       = "Role-based perceptions of computer participants in
                 human-computer co-creativity",
  author      = "Kantosalo, Anna and Jordanous, Anna",
  institution = "AISB",
  year        =  2021
}

@ARTICLE{Wingstrom_undated-ca,
  title     = "Redefining Creativity in the Era of {AI}? Perspectives of
               Computer Scientists and New Media Artists",
  author    = "Wingström, Roosa and Hautala, Johanna and Lundman, Riina",
  journal   = "Creat. Res. J.",
  publisher = "Routledge",
  pages     = "1--17",
  abstract  = "Artificial intelligence (AI) has breached creativity research.
               The advancements of creative AI systems dispute the common
               definitions of creativity that have traditionally focused on five
               elements: actor, process, outcome, domain, and space. Moreover,
               creative workers, such as scientists and artists, increasingly
               use AI in their creative processes, and the concept of
               co-creativity has emerged to describe blended human?AI
               creativity. These issues evoke the question of whether creativity
               requires redefinition in the era of AI. Currently, co-creativity
               is mostly studied within the framework of computer science in
               pre-organized laboratory settings. This study contributes from a
               human scientific perspective with 52 interviews of Finland-based
               computer scientists and new media artists who use AI in their
               work. The results suggest scientists and artists use similar
               elements to define creativity. However, the role of AI differs
               between the scientific and artistic creative processes.
               Scientists need AI to produce accurate and trustworthy outcomes,
               whereas artists use AI to explore and play. Unlike the
               scientists, some artists also considered their work with AI
               co-creative. We suggest that co-creativity can explain the
               contemporary creative processes in the era of AI and should be
               the focal point of future creativity research."
}

@ARTICLE{Goldstein2024-fu,
  title     = "Alignment of brain embeddings and artificial contextual
               embeddings in natural language points to common geometric
               patterns",
  author    = "Goldstein, Ariel and Grinstein-Dabush, Avigail and Schain,
               Mariano and Wang, Haocheng and Hong, Zhuoqiao and Aubrey, Bobbi
               and Schain, Mariano and Nastase, Samuel A and Zada, Zaid and Ham,
               Eric and Feder, Amir and Gazula, Harshvardhan and Buchnik, Eliav
               and Doyle, Werner and Devore, Sasha and Dugan, Patricia and
               Reichart, Roi and Friedman, Daniel and Brenner, Michael and
               Hassidim, Avinatan and Devinsky, Orrin and Flinker, Adeen and
               Hasson, Uri",
  journal   = "Nat. Commun.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  15,
  number    =  1,
  pages     = "1--12",
  abstract  = "AbstractContextual embeddings, derived from deep language models
               (DLMs), provide a continuous vectorial representation of
               language. This embedding space differs fundamentally from the
               symbolic representations posited by traditional
               psycholinguistics. We hypothesize that language areas in the
               human brain, similar to DLMs, rely on a continuous embedding
               space to represent language. To test this hypothesis, we densely
               record the neural activity patterns in the inferior frontal gyrus
               (IFG) of three participants using dense intracranial arrays while
               they listened to a 30-minute podcast. From these fine-grained
               spatiotemporal neural recordings, we derive a continuous
               vectorial representation for each word (i.e., a brain embedding)
               in each patient. Using stringent zero-shot mapping we demonstrate
               that brain embeddings in the IFG and the DLM contextual embedding
               space have common geometric patterns. The common geometric
               patterns allow us to predict the brain embedding in IFG of a
               given left-out word based solely on its geometrical relationship
               to other non-overlapping words in the podcast. Furthermore, we
               show that contextual embeddings capture the geometry of IFG
               embeddings better than static word embeddings. The continuous
               brain embedding space exposes a vector-based neural code for
               natural language processing in the human brain.",
  month     =  mar,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Heersmink2024-mk,
  title     = "Use of large language models might affect our cognitive skills",
  author    = "Heersmink, Richard",
  journal   = "Nat. Hum. Behav.",
  publisher = "Springer Science and Business Media LLC",
  pages     = "1--2",
  abstract  = "Large language models can generate sophisticated text or code
               with little input from a user, which has the potential to
               impoverish our own writing and thinking skills. We need to
               understand the effect of this technology on our cognition and to
               decide whether this is what we want.",
  month     =  mar,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Essel2024-qc,
  title     = "{ChatGPT} effects on cognitive skills of undergraduate students:
               Receiving instant responses from {AI}-based conversational large
               language models ({LLMs})",
  author    = "Essel, Harry Barton and Vlachopoulos, Dimitrios and Essuman,
               Albert Benjamin and Amankwa, John Opuni",
  journal   = "Computers and Education: Artificial Intelligence",
  publisher = "Elsevier BV",
  volume    =  6,
  number    =  100198,
  pages     =  100198,
  month     =  jun,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Talebirad2023-fe,
  title         = "Multi-Agent Collaboration: Harnessing the Power of
                   Intelligent {LLM} Agents",
  author        = "Talebirad, Yashar and Nadiri, Amirhossein",
  journal       = "arXiv [cs.AI]",
  abstract      = "In this paper, we present a novel framework for enhancing the
                   capabilities of large language models (LLMs) by leveraging
                   the power of multi-agent systems. Our framework introduces a
                   collaborative environment where multiple intelligent agent
                   components, each with distinctive attributes and roles, work
                   together to handle complex tasks more efficiently and
                   effectively. We demonstrate the practicality and versatility
                   of our framework through case studies in artificial general
                   intelligence (AGI), specifically focusing on the Auto-GPT and
                   BabyAGI models. We also examine the ``Gorilla'' model, which
                   integrates external APIs into the LLM. Our framework
                   addresses limitations and challenges such as looping issues,
                   security risks, scalability, system evaluation, and ethical
                   considerations. By modeling various domains such as courtroom
                   simulations and software development scenarios, we showcase
                   the potential applications and benefits of our proposed
                   multi-agent system. Our framework provides an avenue for
                   advancing the capabilities and performance of LLMs through
                   collaboration and knowledge exchange among intelligent
                   agents.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Zavadski2023-ud,
  title         = "{ControlNet}-{XS}: Designing an Efficient and Effective
                   Architecture for Controlling Text-to-Image Diffusion Models",
  author        = "Zavadski, Denis and Feiden, Johann-Friedrich and Rother,
                   Carsten",
  journal       = "arXiv [cs.CV]",
  abstract      = "The field of image synthesis has made tremendous strides
                   forward in the last years. Besides defining the desired
                   output image with text-prompts, an intuitive approach is to
                   additionally use spatial guidance in form of an image, such
                   as a depth map. For this, a recent and highly popular
                   approach is to use a controlling network, such as ControlNet,
                   in combination with a pre-trained image generation model,
                   such as Stable Diffusion. When evaluating the design of
                   existing controlling networks, we observe that they all
                   suffer from the same problem of a delay in information
                   flowing between the generation and controlling process. This,
                   in turn, means that the controlling network must have
                   generative capabilities. In this work we propose a new
                   controlling architecture, called ControlNet-XS, which does
                   not suffer from this problem, and hence can focus on the
                   given task of learning to control. In contrast to ControlNet,
                   our model needs only a fraction of parameters, and hence is
                   about twice as fast during inference and training time.
                   Furthermore, the generated images are of higher quality and
                   the control is of higher fidelity. All code and pre-trained
                   models will be made publicly available.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INCOLLECTION{Malafouris2008-xn,
  title     = "At the potter’s wheel : An argument for material agency",
  author    = "Malafouris, Lambros",
  booktitle = "Material Agency",
  publisher = "Springer US",
  address   = "Boston, MA",
  pages     = "19--36",
  year      =  2008
}

@ARTICLE{Masood2023-va,
  title    = "Deepfakes generation and detection: state-of-the-art, open
              challenges, countermeasures, and way forward",
  author   = "Masood, Momina and Nawaz, Mariam and Malik, Khalid Mahmood and
              Javed, Ali and Irtaza, Aun and Malik, Hafiz",
  journal  = "Applied Intelligence",
  volume   =  53,
  number   =  4,
  pages    = "3974--4026",
  abstract = "Easy access to audio-visual content on social media, combined with
              the availability of modern tools such as Tensorflow or Keras, and
              open-source trained models, along with economical computing
              infrastructure, and the rapid evolution of deep-learning (DL)
              methods have heralded a new and frightening trend. Particularly,
              the advent of easily available and ready to use Generative
              Adversarial Networks (GANs), have made it possible to generate
              deepfakes media partially or completely fabricated with the intent
              to deceive to disseminate disinformation and revenge porn, to
              perpetrate financial frauds and other hoaxes, and to disrupt
              government functioning. Existing surveys have mainly focused on
              the detection of deepfake images and videos; this paper provides a
              comprehensive review and detailed analysis of existing tools and
              machine learning (ML) based approaches for deepfake generation,
              and the methodologies used to detect such manipulations in both
              audio and video. For each category of deepfake, we discuss
              information related to manipulation approaches, current public
              datasets, and key standards for the evaluation of the performance
              of deepfake detection techniques, along with their results.
              Additionally, we also discuss open challenges and enumerate future
              directions to guide researchers on issues which need to be
              considered in order to improve the domains of both deepfake
              generation and detection. This work is expected to assist readers
              in understanding how deepfakes are created and detected, along
              with their current limitations and where future research may lead.",
  month    =  feb,
  year     =  2023
}

@ARTICLE{Albahar_undated-vl,
  title  = "{DEEPFAKES}: {THREATS} {AND} {COUNTERMEASURES} {SYSTEMATIC} {REVIEW}",
  author = "Albahar, Marwan and Almalki, Jameel"
}

@MISC{noauthor_undated-ti,
  title        = "Deep Fakes – Threats and Countermeasures",
  booktitle    = "Federal Office for Information Security",
  howpublished = "\url{https://www.bsi.bund.de/EN/Themen/Unternehmen-und-Organisationen/Informationen-und-Empfehlungen/Kuenstliche-Intelligenz/Deepfakes/deepfakes\_node.html}",
  note         = "Accessed: 2024-3-13",
  language     = "en"
}

@BOOK{Bohm2013-ol,
  title     = "On Dialogue",
  author    = "Bohm, David",
  publisher = "Routledge",
  abstract  = "Never before has there been a greater need for deeper listening
               and more open communication to cope with the complex problems
               facing our organizations, businesses and societies. Renowned
               scientist David Bohm believed there was a better way for humanity
               to discover meaning and to achieve harmony. He identified
               creative dialogue, a sharing of assumptions and understanding, as
               a means by which the individual, and society as a whole, can
               learn more about themselves and others, and achieve a renewed
               sense of purpose.",
  month     =  may,
  year      =  2013,
  language  = "en"
}

@MISC{Ted2012-vc,
  title     = "Impossible photography | Erik Johansson",
  author    = "{TED}",
  publisher = "Youtube",
  abstract  = "http://www.ted.com Erik Johansson creates realistic photos of
               impossible scenes -- capturing ideas, not moments. In this witty
               how-to, the Photoshop wizard d...",
  month     =  feb,
  year      =  2012,
  keywords  = "Erik Johansson; creativity; culture; photography; technology;
               TED; TEDTalk; TEDTalks; TED Talk; TED Talks"
}

@ARTICLE{noauthor_2014-ev,
  title    = "Post-photography: when artists go wild with cameras - in pictures",
  journal  = "The Guardian",
  abstract = "A disembodied leg stands alone on stage; a man unfolds a motorway.
              Welcome to the weird world of post-photography, an exciting new
              form of image manipulation that mixes digital and analogue methods
              to cook up the unexpected",
  month    =  oct,
  year     =  2014,
  language = "en"
}

@ARTICLE{Shun-liang_Chao2017-se,
  title     = "The Alchemy of Photography: “Grotesque Realism” and Hybrid Nature
               in Jerry Uelsmann's Photomontages",
  author    = "{Shun-liang Chao}",
  journal   = "Criticism",
  publisher = "Wayne State University Press",
  volume    =  59,
  number    =  2,
  pages     = "301--328",
  year      =  2017
}

@MISC{Offerman2024-lf,
  title        = "Creative pros see generative {AI} as part of their future",
  author       = "Offerman, Stefan",
  year         =  2024,
  howpublished = "\url{https://blog.adobe.com/en/publish/2023/03/21/research-creative-pros-see-generative-ai-as-part-of-their-future}",
  note         = "Accessed: 2024-3-13"
}

@ARTICLE{Davenport2022-sa,
  title    = "How Generative {AI} Is Changing Creative Work",
  author   = "Davenport, Thomas H and Mittal, Nitin",
  journal  = "Harvard Business Review",
  abstract = "Generative AI models for businesses threaten to upend the world of
              content creation, with substantial impacts on marketing, software,
              design, entertainment, and interpersonal communications. These
              models are able to produce text and images: blog posts, program
              code, poetry, and artwork. The software uses complex machine
              learning models to predict the next word based on previous word
              sequences, or the next image based on words describing previous
              images. Companies need to understand how these tools work, and how
              they can add value.",
  month    =  nov,
  year     =  2022
}

@ARTICLE{Rafner2023-jr,
  title    = "Creativity in the age of generative {AI}",
  author   = "Rafner, Janet and Beaty, Roger E and Kaufman, James C and Lubart,
              Todd and Sherson, Jacob",
  journal  = "Nat Hum Behav",
  volume   =  7,
  number   =  11,
  pages    = "1836--1838",
  month    =  nov,
  year     =  2023,
  language = "en"
}

@INPROCEEDINGS{Wang2023-ve,
  title     = "Primacy Effect of {ChatGPT}",
  author    = "Wang, Yiwei and Cai, Yujun and Chen, Muhao and Liang, Yuxuan and
               Hooi, Bryan",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Singapore",
  pages     = "108--115",
  abstract  = "Instruction-tuned large language models (LLMs), such as ChatGPT,
               have led to promising zero-shot performance in discriminative
               natural language understanding (NLU) tasks. This involves
               querying the LLM using a prompt containing the question, and the
               candidate labels to choose from. The question-answering
               capabilities of ChatGPT arise from its pre-training on large
               amounts of human-written text, as well as its subsequent
               fine-tuning on human preferences, which motivates us to ask: Does
               ChatGPT also inherit humans' cognitive biases? In this paper, we
               study the primacy effect of ChatGPT: the tendency of selecting
               the labels at earlier positions as the answer. We have two main
               findings: i) ChatGPT's decision is sensitive to the order of
               labels in the prompt; ii) ChatGPT has a clearly higher chance to
               select the labels at earlier positions as the answer. We hope
               that our experiments and analyses provide additional insights
               into building more reliable ChatGPT-based solutions. We release
               the source code at https://github.com/wangywUST/PrimacyEffectGPT.",
  month     =  dec,
  year      =  2023
}

@BOOK{Hermann2011-ng,
  title     = "The Sonification Handbook",
  editor    = "Hermann, Thomas and Hunt, Andy and Neuhoff, John G",
  publisher = "Logos Verlag Berlin",
  address   = "Berlin, Germany",
  month     =  dec,
  year      =  2011,
  language  = "en"
}

@INPROCEEDINGS{Rocchesso2008-qp,
  title     = "Sonic interaction design",
  author    = "Rocchesso, Davide and Serafin, Stefania and Behrendt, Frauke and
               Bernardini, Nicola and Bresin, Roberto and Eckel, Gerhard and
               Franinovic, Karmen and Hermann, Thomas and Pauletto, Sandra and
               Susini, Patrick and Visell, Yon",
  booktitle = "CHI '08 Extended Abstracts on Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2008
}

@INPROCEEDINGS{Pigrem2017-sa,
  title     = "Datascaping",
  author    = "Pigrem, Jon and Barthet, Mathieu",
  booktitle = "Proceedings of the 12th International Audio Mostly Conference on
               Augmented and Participatory Sound and Music Experiences",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2017
}

@PROCEEDINGS{noauthor_2017-wt,
  title     = "Proceedings of the {12th} international audio mostly conference
               on augmented and participatory sound and music experiences",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2017
}

@MISC{Vox2023-ab,
  title     = "An {AI} artist explains his workflow",
  author    = "{Vox}",
  publisher = "Youtube",
  abstract  = "How it works — and why it takes a surprisingly long time to make
               something good.Subscribe and turn on notifications 🔔 so you
               don't miss any videos: http://...",
  month     =  may,
  year      =  2023,
  keywords  = "AI art; Midjourney; Vox.com; ai; art process; explain; explainer;
               machine learning; process; vox; stable diffusion; stelfie;
               stelfie the time traveller; stelfie creator; ai selfie; ai art;
               digital art; sketching; sketch to finished drawing; illustrator;
               photoshop; realistic photoshop; muhammed ali; denoise; ai art
               tutorial; how to make ai art"
}

@ARTICLE{Zhang2023-by,
  title         = "Adding Conditional Control to Text-to-Image Diffusion Models",
  author        = "Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh",
  journal       = "arXiv [cs.CV]",
  abstract      = "We present ControlNet, a neural network architecture to add
                   spatial conditioning controls to large, pretrained
                   text-to-image diffusion models. ControlNet locks the
                   production-ready large diffusion models, and reuses their
                   deep and robust encoding layers pretrained with billions of
                   images as a strong backbone to learn a diverse set of
                   conditional controls. The neural architecture is connected
                   with ``zero convolutions'' (zero-initialized convolution
                   layers) that progressively grow the parameters from zero and
                   ensure that no harmful noise could affect the finetuning. We
                   test various conditioning controls, eg, edges, depth,
                   segmentation, human pose, etc, with Stable Diffusion, using
                   single or multiple conditions, with or without prompts. We
                   show that the training of ControlNets is robust with small
                   (1m) datasets. Extensive results show that ControlNet may
                   facilitate wider applications to control image diffusion
                   models.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@MISC{noauthor_undated-td,
  title        = "{AI} Photo Generator",
  booktitle    = "Photo AI",
  abstract     = "Generate photorealistic images of people with AI. Save money
                  and use AI to do a photo shoot from your laptop or phone
                  instead of hiring an expensive photographer ✏️ Upload your
                  selfies and create your own AI character 📸 Take 100\% AI
                  photos in any pose, place or action 🎞️ Create 100\% AI videos
                  from any AI photo you take ❤️ Run photo packs like AI Yearbook
                  and Old Money ✍️ Create AI-generated fashion designs with
                  Sketch2Image™",
  howpublished = "\url{https://photoai.com/}",
  note         = "Accessed: 2024-3-5",
  language     = "en"
}

@ARTICLE{Brooks2022-yh,
  title         = "{InstructPix2Pix}: Learning to Follow Image Editing
                   Instructions",
  author        = "Brooks, Tim and Holynski, Aleksander and Efros, Alexei A",
  journal       = "arXiv [cs.CV]",
  abstract      = "We propose a method for editing images from human
                   instructions: given an input image and a written instruction
                   that tells the model what to do, our model follows these
                   instructions to edit the image. To obtain training data for
                   this problem, we combine the knowledge of two large
                   pretrained models -- a language model (GPT-3) and a
                   text-to-image model (Stable Diffusion) -- to generate a large
                   dataset of image editing examples. Our conditional diffusion
                   model, InstructPix2Pix, is trained on our generated data, and
                   generalizes to real images and user-written instructions at
                   inference time. Since it performs edits in the forward pass
                   and does not require per example fine-tuning or inversion,
                   our model edits images quickly, in a matter of seconds. We
                   show compelling editing results for a diverse collection of
                   input images and written instructions.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@MISC{noauthor_undated-bz,
  title        = "Stelfie",
  booktitle    = "My Site",
  abstract     = "I'm Stelfie. the Time Traveller. In a parody of life and
                  history powered by AI, I time travel and I take stelfies.",
  howpublished = "\url{https://www.stelfiett.com/}",
  note         = "Accessed: 2024-3-4",
  language     = "en"
}

@MISC{noauthor_2024-tm,
  title        = "stephen mcmennamy (@combophoto) • Instagram photos and videos",
  booktitle    = "Instagram",
  year         =  2024,
  howpublished = "\url{https://www.instagram.com/combophoto/}",
  note         = "Accessed: 2024-3-4",
  language     = "en"
}

@MISC{noauthor_undated-kv,
  title        = "{ERIK} {JOHANSSON}",
  booktitle    = "ERIK JOHANSSON",
  abstract     = "Website of Swedish surreal photographer Erik Johansson.",
  howpublished = "\url{https://www.erikjo.com/}",
  note         = "Accessed: 2024-3-4",
  language     = "en"
}

@MISC{noauthor_undated-mg,
  title        = "{ERIK} {JOHANSSON}",
  booktitle    = "ERIK JOHANSSON",
  abstract     = "Website of Swedish surreal photographer Erik Johansson.",
  howpublished = "\url{https://www.erikjo.com/}",
  note         = "Accessed: 2024-3-4",
  language     = "en"
}

@MISC{noauthor_undated-sl,
  title        = "Jerry Uelsmann",
  abstract     = "Born in Detroit on June 11, 1934, Jerry Uelsmann received his
                  B.F.A. degree at the Rochester Institute of Technology in 1957
                  and his M.S. and M.F.A. at Indiana University in 1960. He
                  began teaching photography at the University of Florida in
                  Gainesville in 1960 (“my first job offer”). He became a
                  graduate research professor of art at the university in 1974,
                  and is now retired from teaching. He lives in Gainesville,
                  Florida.",
  howpublished = "\url{https://www.uelsmann.net/}",
  note         = "Accessed: 2024-3-4"
}

@MISC{Drummond2023-bh,
  title        = "What we learnt when making {AI} images for the 2023 Power
                  issue",
  author       = "Drummond, Matthew",
  booktitle    = "Australian Financial Review",
  abstract     = "Our team taught artificial intelligence to make portraits,
                  like this one of Margot Robbie. The results show how it’s
                  learning about 21st-century culture.",
  month        =  sep,
  year         =  2023,
  howpublished = "\url{https://www.afr.com/politics/federal/what-we-learnt-when-making-ai-images-for-the-2023-power-issue-20230830-p5e0jp}",
  note         = "Accessed: 2024-3-4",
  language     = "en"
}

@MISC{Drummond2023-av,
  title        = "A note on this year’s Power issue – and why we published {AI}
                  deepfakes",
  author       = "Drummond, Matthew",
  booktitle    = "Australian Financial Review",
  abstract     = "When our overt, covert and cultural power lists appear on
                  afr.com towards the end of the week, you’ll notice some rather
                  bizarre images of our power listers.",
  month        =  sep,
  year         =  2023,
  howpublished = "\url{https://www.afr.com/politics/federal/a-note-on-this-year-s-power-issue-and-why-we-published-ai-deepfakes-20230907-p5e2wu}",
  note         = "Accessed: 2024-3-4",
  language     = "en"
}

@BOOK{Kaiser_undated-gq,
  title  = "Creative Collaborations",
  author = "Kaiser, Marc Downie, Shelley Eshkar,"
}

@MISC{Nebelong2024-jz,
  title        = "The next stage of {AI} image gen is going to be all about
                  control. Human creativity is a beautiful thing, and the images
                  we all have in our heads is much better expressed through
                  motion, brush-strokes, song, tone of voice than through a
                  simple text prompt.``Make a tree with a…
                  pic.twitter.com/{2lRJAMhxf3}",
  author       = "Nebelong, Martin",
  booktitle    = "Twitter",
  month        =  feb,
  year         =  2024,
  howpublished = "\url{https://twitter.com/MartinNebelong/status/1761864757051630028}",
  note         = "Accessed: 2024-2-29",
  language     = "en"
}

@MISC{Dylan2023-ma,
  title        = "Stop using text-to-image.this blender + real-time latent
                  consistency workflow is way more fun, and shows how you can
                  use Generative {AI} collaboratively, instead of as a creative
                  slot machinetry it yourself here https://t.co/{QyiR5IDQI2}
                  pic.twitter.com/{pSsMiA33Rj}",
  author       = "{dylan}",
  booktitle    = "Twitter",
  month        =  nov,
  year         =  2023,
  howpublished = "\url{https://twitter.com/dylan\_ebert\_/status/1724885074313642424}",
  note         = "Accessed: 2024-2-29",
  language     = "en"
}

@MISC{noauthor_undated-bf,
  title        = "Real-Time Latent Consistency Model Image-to-Image {ControlNet}
                  - a Hugging Face Space by radames",
  abstract     = "Discover amazing ML apps made by the community",
  howpublished = "\url{https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model}",
  note         = "Accessed: 2024-2-29"
}

@MISC{Nebelong2023-rb,
  title        = "For generative {AI} to become an interesting art tool, we need
                  much more control over the output. The slot-machine-like
                  nature of pure text-to-image leaves too much to chance.Using
                  the ``Real-time Latent Consistency Model'' that {I}'m using in
                  the example here, is the first time I…
                  pic.twitter.com/{YH90MCHVNd}",
  author       = "Nebelong, Martin",
  booktitle    = "Twitter",
  month        =  nov,
  year         =  2023,
  howpublished = "\url{https://twitter.com/MartinNebelong/status/1724191921411608808}",
  note         = "Accessed: 2024-2-29",
  language     = "en"
}

@BOOK{Toromanoff2018-hc,
  title     = "Impossible Photography: Surreal Pictures that Challenge our
               Perception",
  author    = "Toromanoff, Agata",
  publisher = "Eken Press",
  year      =  2018
}

@MISC{noauthor_undated-ih,
  title        = "Practice-as-research-context-method-knowledge-by-estelle-barrett-barbara-bolt-z-lib.Org-.Pdf",
  booktitle    = "Are.na",
  abstract     = "Are.na is a platform for connecting ideas and building
                  knowledge",
  howpublished = "\url{https://www.are.na/block/9736221}",
  note         = "Accessed: 2024-2-14",
  language     = "en"
}

@ARTICLE{Zhou2024-jk,
  title         = "Self-discover: Large language models self-compose reasoning
                   structures",
  author        = "Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and
                   Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and Zhou, Denny
                   and Mishra, Swaroop and Zheng, Huaixiu Steven",
  journal       = "arXiv [cs.AI]",
  abstract      = "We introduce SELF-DISCOVER, a general framework for LLMs to
                   self-discover the task-intrinsic reasoning structures to
                   tackle complex reasoning problems that are challenging for
                   typical prompting methods. Core to the framework is a
                   self-discovery process where LLMs select multiple atomic
                   reasoning modules such as critical thinking and step-by-step
                   thinking, and compose them into an explicit reasoning
                   structure for LLMs to follow during decoding. SELF-DISCOVER
                   substantially improves GPT-4 and PaLM 2's performance on
                   challenging reasoning benchmarks such as BigBench-Hard,
                   grounded agent reasoning, and MATH, by as much as 32\%
                   compared to Chain of Thought (CoT). Furthermore,
                   SELF-DISCOVER outperforms inference-intensive methods such as
                   CoT-Self-Consistency by more than 20\%, while requiring
                   10-40x fewer inference compute. Finally, we show that the
                   self-discovered reasoning structures are universally
                   applicable across model families: from PaLM 2-L to GPT-4, and
                   from GPT-4 to Llama2, and share commonalities with human
                   reasoning patterns.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Ruiz2022-mb,
  title         = "{DreamBooth}: Fine Tuning Text-to-Image Diffusion Models for
                   Subject-Driven Generation",
  author        = "Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and
                   Pritch, Yael and Rubinstein, Michael and Aberman, Kfir",
  journal       = "arXiv [cs.CV]",
  abstract      = "Large text-to-image models achieved a remarkable leap in the
                   evolution of AI, enabling high-quality and diverse synthesis
                   of images from a given text prompt. However, these models
                   lack the ability to mimic the appearance of subjects in a
                   given reference set and synthesize novel renditions of them
                   in different contexts. In this work, we present a new
                   approach for ``personalization'' of text-to-image diffusion
                   models. Given as input just a few images of a subject, we
                   fine-tune a pretrained text-to-image model such that it
                   learns to bind a unique identifier with that specific
                   subject. Once the subject is embedded in the output domain of
                   the model, the unique identifier can be used to synthesize
                   novel photorealistic images of the subject contextualized in
                   different scenes. By leveraging the semantic prior embedded
                   in the model with a new autogenous class-specific prior
                   preservation loss, our technique enables synthesizing the
                   subject in diverse scenes, poses, views and lighting
                   conditions that do not appear in the reference images. We
                   apply our technique to several previously-unassailable tasks,
                   including subject recontextualization, text-guided view
                   synthesis, and artistic rendering, all while preserving the
                   subject's key features. We also provide a new dataset and
                   evaluation protocol for this new task of subject-driven
                   generation. Project page: https://dreambooth.github.io/",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Bown2024-yx,
  title  = "”Coffee So Good, You'll Want to Slap Your Barista”: Evaluating
            Co-Creative Interaction Through Dialogic Actions",
  author = "Bown, Oliver",
  year   =  2024
}

@ARTICLE{DellAcqua2023-og,
  title     = "Navigating the jagged technological frontier: Field experimental
               evidence of the effects of {AI} on knowledge worker productivity
               and quality",
  author    = "Dell'Acqua, Fabrizio and McFowland, Edward and Mollick, Ethan R
               and Lifshitz-Assaf, Hila and Kellogg, Katherine and Rajendran,
               Saran and Krayer, Lisa and Candelon, François and Lakhani, Karim
               R",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2023,
  language  = "en"
}

@ARTICLE{Gomez2023-bp,
  title         = "Designing {AI} Support for Human Involvement in {AI}-assisted
                   Decision Making: A Taxonomy of Human-{AI} Interactions from a
                   Systematic Review",
  author        = "Gomez, Catalina and Cho, Sue Min and Ke, Shichang and Huang,
                   Chien-Ming and Unberath, Mathias",
  journal       = "arXiv [cs.HC]",
  abstract      = "Efforts in levering Artificial Intelligence (AI) in decision
                   support systems have disproportionately focused on
                   technological advancements, often overlooking the alignment
                   between algorithmic outputs and human expectations. To
                   address this, explainable AI promotes AI development from a
                   more human-centered perspective. Determining what information
                   AI should provide to aid humans is vital, however, how the
                   information is presented, e. g., the sequence of
                   recommendations and the solicitation of interpretations, is
                   equally crucial. This motivates the need to more precisely
                   study Human-AI interaction as a pivotal component of AI-based
                   decision support. While several empirical studies have
                   evaluated Human-AI interactions in multiple application
                   domains in which interactions can take many forms, there is
                   not yet a common vocabulary to describe human-AI interaction
                   protocols. To address this gap, we describe the results of a
                   systematic review of the AI-assisted decision making
                   literature, analyzing 105 selected articles, which grounds
                   the introduction of a taxonomy of interaction patterns that
                   delineate various modes of human-AI interactivity. We find
                   that current interactions are dominated by simplistic
                   collaboration paradigms and report comparatively little
                   support for truly interactive functionality. Our taxonomy
                   serves as a valuable tool to understand how interactivity
                   with AI is currently supported in decision-making contexts
                   and foster deliberate choices of interaction designs.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Mozannar2023-qu,
  title         = "Effective Human-{AI} Teams via Learned Natural Language Rules
                   and Onboarding",
  author        = "Mozannar, Hussein and Lee, Jimin J and Wei, Dennis and
                   Sattigeri, Prasanna and Das, Subhro and Sontag, David",
  journal       = "arXiv [cs.LG]",
  abstract      = "People are relying on AI agents to assist them with various
                   tasks. The human must know when to rely on the agent,
                   collaborate with the agent, or ignore its suggestions. In
                   this work, we propose to learn rules grounded in data regions
                   and described in natural language that illustrate how the
                   human should collaborate with the AI. Our novel region
                   discovery algorithm finds local regions in the data as
                   neighborhoods in an embedding space that corrects the human
                   prior. Each region is then described using an iterative and
                   contrastive procedure where a large language model describes
                   the region. We then teach these rules to the human via an
                   onboarding stage. Through user studies on object detection
                   and question-answering tasks, we show that our method can
                   lead to more accurate human-AI teams. We also evaluate our
                   region discovery and description algorithms separately.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zheng2023-pz,
  title         = "Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena",
  author        = "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and
                   Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin,
                   Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P and
                   Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion",
  journal       = "arXiv [cs.CL]",
  abstract      = "Evaluating large language model (LLM) based chat assistants
                   is challenging due to their broad capabilities and the
                   inadequacy of existing benchmarks in measuring human
                   preferences. To address this, we explore using strong LLMs as
                   judges to evaluate these models on more open-ended questions.
                   We examine the usage and limitations of LLM-as-a-judge,
                   including position, verbosity, and self-enhancement biases,
                   as well as limited reasoning ability, and propose solutions
                   to mitigate some of them. We then verify the agreement
                   between LLM judges and human preferences by introducing two
                   benchmarks: MT-bench, a multi-turn question set; and Chatbot
                   Arena, a crowdsourced battle platform. Our results reveal
                   that strong LLM judges like GPT-4 can match both controlled
                   and crowdsourced human preferences well, achieving over 80\%
                   agreement, the same level of agreement between humans. Hence,
                   LLM-as-a-judge is a scalable and explainable way to
                   approximate human preferences, which are otherwise very
                   expensive to obtain. Additionally, we show our benchmark and
                   traditional benchmarks complement each other by evaluating
                   several variants of LLaMA and Vicuna. The MT-bench questions,
                   3K expert votes, and 30K conversations with human preferences
                   are publicly available at
                   https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Muller2012-hg,
  title     = "Participatory design: The third space in human–computer
               interaction",
  author    = "Muller, M J and Druin, A",
  journal   = "Human Computer Interaction Handbook",
  publisher = "taylorfrancis.com",
  abstract  = "This chapter discusses methods that go beyond merely adding
               users—methods to create new settings and experiences that can
               assist computer professionals to work in partnership …",
  year      =  2012
}

@BOOK{noauthor_2023-sv,
  title     = "Proceedings of the 2023 {CHI} Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  month     =  mar,
  year      =  2023
}

@INPROCEEDINGS{Ocampo2023-gu,
  title     = "Using {GPT}-3 to Achieve Semantically Relevant Data Sonificiation
               for an Art Installation",
  author    = "Ocampo, Rodolfo and Andres, Josh and Schmidt, Adrian and Pegram,
               Caroline and Shave, Justin and Hill, Charlton and Wright, Brendan
               and Bown, Oliver",
  booktitle = "Artificial Intelligence in Music, Sound, Art and Design",
  publisher = "Springer Nature Switzerland",
  pages     = "212--227",
  abstract  = "Large Language Models such as GPT-3 exhibit generative language
               capabilities with multiple potential applications in creative
               practice. In this paper, we present a method for data
               sonification that employs the GPT-3 model to create semantically
               relevant mappings between artificial intelligence-generated
               natural language descriptions of data, and human-generated
               descriptions of sounds. We implemented this method in a public
               art installation to generate a soundscape based on data from
               different systems. While common sonification approaches rely on
               arbitrary mappings between data values and sonic values, our
               approach explores the use of language models to achieve a mapping
               not via values but via meaning. We find our approach is a useful
               tool for musification practice and demonstrates a new application
               of generative language models in creative new media arts
               practice. We show how different prompts influence data to sound
               mappings, and highlight that matching the embeddings of texts of
               different lengths produces undesired behavior.",
  year      =  2023
}

@MISC{Steinberg2023-vm,
  title        = "Adobe Flies Towards Generative {AI}",
  author       = "Steinberg, Jon",
  booktitle    = "The Information",
  abstract     = "Hi, welcome to your Weekend.For about 11 months after OpenAI
                  released its text-to-image generator Dall-E 2 in April 2022,
                  many of us wondered when Adobe would come to market with a
                  rival product, which could be neatly integrated into design
                  programs like Photoshop, Illustrator and Premiere.The ...",
  month        =  jun,
  year         =  2023,
  howpublished = "\url{https://www.theinformation.com/articles/adobe-flies-towards-generative-ai?utm\_source=sg\&utm\_medium=email\&utm\_campaign=article\_email\&utm\_content=article-10747}",
  note         = "Accessed: 2023-6-27"
}

@INPROCEEDINGS{Steinfeld2006-qj,
  title     = "Common metrics for human-robot interaction",
  author    = "Steinfeld, Aaron and Fong, Terrence and Kaber, David and Lewis,
               Michael and Scholtz, Jean and Schultz, Alan and Goodrich, Michael",
  booktitle = "Proceedings of the 1st ACM SIGCHI/SIGART conference on
               Human-robot interaction",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "33--40",
  abstract  = "This paper describes an effort to identify common metrics for
               task-oriented human-robot interaction (HRI). We begin by
               discussing the need for a toolkit of HRI metrics. We then
               describe the framework of our work and identify important biasing
               factors that must be taken into consideration. Finally, we
               present suggested common metrics for standardization and a case
               study. Preparation of a larger, more detailed toolkit is in
               progress.",
  series    = "HRI '06",
  month     =  mar,
  year      =  2006,
  keywords  = "human-robot interaction, metrics, unmanned ground vehicles"
}

@ARTICLE{Hoffman2019-ag,
  title    = "Evaluating Fluency in {Human–Robot} Collaboration",
  author   = "Hoffman, Guy",
  journal  = "IEEE Transactions on Human-Machine Systems",
  volume   =  49,
  number   =  3,
  pages    = "209--218",
  abstract = "Collaborative fluency is the coordinated meshing of joint
              activities between members of a well-synchronized team. In recent
              years, researchers in human-robot collaboration have been
              developing robots to work alongside humans aiming not only at task
              efficiency, but also at human-robot fluency. As part of this
              effort, we have developed a number of metrics to evaluate the
              level of fluency in human-robot shared-location teamwork. While
              these metrics are being used in existing research, there has been
              no systematic discussion on how to measure fluency and how the
              commonly used metrics perform and compare. In this paper, we
              codify subjective and objective human-robot fluency metrics,
              provide an analytical model for four objective metrics, and assess
              their dynamics in a turn-taking framework. We also report on a
              user study linking objective and subjective fluency metrics and
              survey recent use of these metrics in the literature.",
  month    =  jun,
  year     =  2019,
  keywords = "Measurement;Robot kinematics;Robot sensing systems;Task
              analysis;Teamwork;Artificial intelligence;computational and
              artificial intelligence;cooperative systems;human-robot
              interaction;intelligent robots;intelligent systems;man-machine
              systems;systems;man;cybernetics;user interfaces"
}

@ARTICLE{Fugener2019-yz,
  title    = "Cognitive challenges in human-{AI} collaboration: Investigating
              the path towards productive delegation",
  author   = "Fügener, Andreas and Grahl, Jörn and Gupta, Alok and Ketter,
              Wolfgang",
  abstract = "We study how humans make decisions when they collaborate with an
              artificial intelligence (AI): each instance of a classification
              task could be classified by themselves or by the AI. Experimental
              results suggest that humans and AI who work together can
              outperform the superior AI when it works alone. However, this only
              occurred when the AI delegated work to humans, not when humans
              delegated work to the AI. The AI profited, even from working with
              low-performing subjects, but humans did not delegate well. This
              bad delegation performance cannot be explained with algorithm
              aversion. On the contrary, subjects tried to follow a provided
              delegation strategy diligently and appeared to appreciate the AI
              support. However, human results suffered due to a lack of
              metaknowledge. They were not able to assess their own capabilities
              correctly, which in turn leads to poor delegation decisions. In
              contrast to reluctance to use AI, lacking metaknowledgeis an
              unconscious trait. It limits fundamentally how well human decision
              makers can collaborate with AI and other algorithms when there is
              no explicit performance feedback. The results have implications
              for the future of work, the design of human-AI collaborative
              environments and education in the digital age.",
  month    =  apr,
  year     =  2019,
  keywords = "Future of Work, Artificial Intelligence, Augemented Decision
              Environments, Deep Learning, Human-AI Collaboration, Machine
              Learning, Intelligent Software Agents"
}

@ARTICLE{Freedman2020-pi,
  title         = "Helpfulness as a Key Metric of Human-Robot Collaboration",
  author        = "Freedman, Richard G and Levine, Steven J and Williams, Brian
                   C and Zilberstein, Shlomo",
  journal       = "arXiv [cs.AI]",
  abstract      = "As robotic teammates become more common in society, people
                   will assess the robots' roles in their interactions along
                   many dimensions. One such dimension is effectiveness: people
                   will ask whether their robotic partners are trustworthy and
                   effective collaborators. This begs a crucial question: how
                   can we quantitatively measure the helpfulness of a robotic
                   partner for a given task at hand? This paper seeks to answer
                   this question with regards to the interactive robot's
                   decision making. We describe a clear, concise, and
                   task-oriented metric applicable to many different planning
                   and execution paradigms. The proposed helpfulness metric is
                   fundamental to assessing the benefit that a partner has on a
                   team for a given task. In this paper, we define helpfulness,
                   illustrate it on concrete examples from a variety of domains,
                   discuss its properties and ramifications for planning
                   interactions with humans, and present preliminary results.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Sankaran2022-ti,
  title     = "A Modeling Approach for Measuring the Performance of a Human-{AI}
               Collaborative Process",
  author    = "Sankaran, Ganesh and Palomino, Marco A and Knahl, Martin and
               Siestrup, Guido",
  journal   = "NATO Adv. Sci. Inst. Ser. E Appl. Sci.",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  12,
  number    =  22,
  pages     =  11642,
  abstract  = "Despite the unabated growth of algorithmic decision-making in
               organizations, there is a growing consensus that numerous
               situations will continue to require humans in the loop. However,
               the blending of a formal machine and bounded human rationality
               also amplifies the risk of what is known as local rationality.
               Therefore, it is crucial, especially in a data-abundant
               environment that characterizes algorithmic decision-making, to
               devise means to assess performance holistically. In this paper,
               we propose a simulation-based model to address the current lack
               of research on quantifying algorithmic interventions in a broader
               organizational context. Our approach allows the combining of
               causal modeling and data science algorithms to represent decision
               settings involving a mix of machine and human rationality to
               measure performance. As a testbed, we consider the case of a
               fictitious company trying to improve its forecasting process with
               the help of a machine learning approach. The example demonstrates
               that a myopic assessment obscures problems that only a broader
               framing reveals. It highlights the value of a systems view since
               the effects of the interplay between human and algorithmic
               decisions can be largely unintuitive. Such a simulation-based
               approach can be an effective tool in efforts to delineate roles
               for humans and algorithms in hybrid contexts.",
  month     =  nov,
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Howell-Munson2022-pt,
  title     = "Towards Brain Metrics for Improving Multi-Agent Adaptive
               Human-Robot Collaboration: A Preliminary Study",
  author    = "Howell-Munson, Alicia and Doherty, Emily and Gavriel, Peter and
               Nicolas, Claire and Norton, Adam and Neamtu, Rodica and Yanco,
               Holly and Wu, Yi-Ning and Solovey, Erin T",
  booktitle = "2022 Symposium on Human-Computer Interaction for Work",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Article 11",
  pages     = "1--10",
  abstract  = "When humans work closely together, they can pick up subtle cues
               from their team members and adapt their behavior appropriately.
               Humans working closely with robots may also give off cues, but
               the robots cannot detect these signals and therefore cannot
               change behavior. In this paper, we focus on heterogeneous
               multi-human and robot teams. Such scenarios exist frequently in
               search and rescue operations as well as space missions, where
               robots perform tasks that are unsafe or even impossible for
               humans. At the same time, human team members collaborate to make
               important decisions that influence and direct the robots’ work.
               These decisions often have to be made quickly with high levels of
               uncertainty, with simultaneous physical and mental demands on the
               human. In this project, we aim to explore the following
               questions: Can brain data provide insights that could improve
               team performance? Could we use these signals to detect when
               someone is experiencing excessive workload? Could we detect an
               impact on team performance caused by the robot?",
  series    = "CHIWORK 2022",
  month     =  jun,
  year      =  2022,
  keywords  = "teamwork, collaboration, human-robot interaction, fNIRS"
}

@INPROCEEDINGS{Jacobsen2020-sn,
  title     = "Perceived and Measured Task Effectiveness in Human-{AI}
               Collaboration",
  author    = "Jacobsen, Rune Møberg and Bysted, Lukas Bjørn Leer and Johansen,
               Patrick Skov and Papachristos, Eleftherios and Skov, Mikael B",
  booktitle = "Extended Abstracts of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--9",
  abstract  = "Human-AI Collaboration is emerging all around with the increasing
               utilisation of AI. Few prior studies have investigated the
               perceived effectiveness of users solving tasks with AI. To expand
               on these, we conducted a within-subjects repeated measures study
               involving 35 participants sorting household waste according to
               recyclability both with and without the help of an AI system. Our
               results show that people both sorted more effectively and
               perceived themselves more effective. Furthermore, we document a
               trend where people sorting without suggestions perceived
               themselves more effective than they were, while the opposite was
               true for people when sorting receiving suggestions. Based on our
               results we propose open questions for future research on
               perceived effectiveness when collaborating with AI systems.",
  series    = "CHI EA '20",
  month     =  apr,
  year      =  2020,
  keywords  = "lab study, artificial intelligence, effectiveness, human-ai
               collaboration"
}

@ARTICLE{Davis2017-yz,
  title     = "Quantifying Collaboration with a Co-Creative Drawing Agent",
  author    = "Davis, N and Hsiao, C and Singh, K Y and Lin, B and Magerko, B",
  journal   = "ACM Trans. Interact. Intell. Syst.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    =  4,
  pages     = "1--25",
  abstract  = "This article describes a new technique for quantifying creative
               collaboration and applies it to the user study evaluation of a
               co-creative drawing agent. We present a cognitive framework
               called creative sense-making that provides a new method to
               visualize and quantify the interaction dynamics of creative
               collaboration, for example, the rhythm of interaction, style of
               turn taking, and the manner in which participants are mutually
               making sense of a situation. The creative sense-making framework
               includes a qualitative coding technique, interaction coding
               software, an analysis method, and the cognitive theory behind
               these applications. This framework and analysis method are
               applied to empirical studies of the Drawing Apprentice
               collaborative sketching system to compare human collaboration
               with a co-creative AI agent vs. a Wizard of Oz setup. The
               analysis demonstrates how the proposed technique can be used to
               analyze interaction data using continuous functions (e.g.,
               integrations and moving averages) to measure and evaluate how
               collaborations unfold through time.",
  month     =  dec,
  year      =  2017,
  keywords  = "Creativity, drawing, interaction dynamics, creative agents,
               collaboration, evaluation methods, qualitative coding"
}

@BOOK{Paulus2003-iy,
  title     = "Group Creativity: Innovation through Collaboration",
  author    = "Paulus, Paul B and Nijstad, Bernard A",
  publisher = "Oxford University Press",
  abstract  = "Creativity often leads to the development of original ideas that
               are useful or influential, and maintaining creativity is crucial
               for the continued development of organizations in particular and
               society in general. Most research and writing has focused on
               individual creativity. Yet, in recent years there has been an
               increasing acknowledgment of the importance of the social and
               contextual factors in creativity. Even with the information
               explosion and the growing necessity for specialization, the
               development of innovations still requires group interaction at
               various stages in the creative process. Most organizations
               increasingly rely on the work of creative teams where each
               individual is an expert in a particular area. This volume
               summarizes the exciting new research developments on the
               processes involved in group creativity and innovation, and
               explores the relationship between group processes, group context,
               and creativity. It draws from a broad range of research
               perspectives, including those investigating cognition, groups,
               creativity, information systems, and organizational psychology.
               These different perspectives have been brought together in one
               volume in order to focus attention on this developing literature
               and its implications for theory and application. The chapters in
               this volume are organized into two sections. The first focuses on
               how group decision making is affected by factors such as
               cognitive fixation and flexibility, group diversity, minority
               dissent, group decision-making, brainstorming, and group support
               systems. Special attention is devoted to the various processes
               and conditions that can inhibit or facilitate group creativity.
               The second section explores how various contextual and
               environmental factors affect the creative processes of groups.
               The chapters explore issues of group autonomy, group
               socialization, mentoring, team innovation, knowledge transfer,
               and creativity at the level of cultures and societies. The
               research presented in this section makes it clear that a full
               understanding of group creativity cannot be accomplished without
               adequate attention to the group environment. It will be a useful
               source of information for scholars, practitioners, and students
               wishing to understand and facilitate group creativity.",
  month     =  sep,
  year      =  2003,
  language  = "en"
}

@BOOK{Keith_Sawyer2014-et,
  title     = "Group Creativity: Music, Theater, Collaboration",
  author    = "Keith Sawyer, R",
  publisher = "Psychology Press",
  abstract  = "Group Creativity explores the unique form of creativity that
               emerges from collaborating groups. Dr. Sawyer draws on his
               studies of jazz ensembles and improvisational theater groups to
               develop a model of creative group processes. He applies this
               model of group creativity to a wide range of collaborating
               groups, including group learning in classrooms and innovative
               teams in organizations. In group creativity, a group comes
               together to collaboratively create in real time. The creative
               inspiration emerges from the interaction and communication among
               the members, and makes the result more than the sum of its parts.
               The dynamic, moment-to-moment communication among jazz musicians
               and improvising actors is the primary topic of the book. Sawyer
               explores performers' close listening and sensitivity, the
               submerging of the ego to the group mind, and the ways that
               performers work together to create something better than and
               different from what one solitary individual could create alone.
               These explorations provide insight into all forms of group
               creativity and collaboration.",
  month     =  apr,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Lubart2001-vl,
  title     = "Models of the Creative Process: Past, Present and Future",
  author    = "Lubart, Todd I",
  journal   = "Creat. Res. J.",
  publisher = "Routledge",
  volume    =  13,
  number    = "3-4",
  pages     = "295--308",
  abstract  = "The creative process, one of the key topics discussed in
               Guilford's (1950) address to the American Psychological
               Association and his subsequent work, refers to the sequence of
               thoughts and actions that leads to novel, adaptive productions.
               This article examines conceptions of the creative process that
               have been advocated during the past century. In particular,
               stage-based models of the creative process are discussed and the
               evolution of these models is traced. Empirical research suggests
               that the basic 4-stage model of the creative process may need to
               be revised or replaced. Several key questions about the creative
               process are raised, such as how the creative process differs from
               the noncreative process and how process-related differences may
               lead to different levels of creative performance. New directions
               for future research are identified.",
  month     =  oct,
  year      =  2001
}

@ARTICLE{Beghetto2007-bg,
  title    = "Toward a broader conception of creativity: A case for ``mini-c''
              creativity",
  author   = "Beghetto, Ronald A and Kaufman, James C",
  journal  = "Psychology of Aesthetics, Creativity, and the Arts",
  volume   =  1,
  number   =  2,
  pages    = "73--79",
  abstract = "In this article the authors argue that a new category of
              creativity, called ``mini-c'' creativity, is needed to advance
              creativity theory and research. Mini-c creativity differs from
              little-c (everyday) or Big-C (eminent) creativity as it refers to
              the creative processes involved in the construction of personal
              knowledge and understanding. The authors discuss how the category
              of mini-c creativity addresses gaps in current conceptions of
              creativity, offers researchers a new and important unit of
              analysis, and helps to better frame the domain question in
              creativity research. Implications for creativity research are also
              discussed. (PsycINFO Database Record (c) 2017 APA, all rights
              reserved)",
  month    =  may,
  year     =  2007
}

@MISC{The_Museum_of_Modern_Art2023-tp,
  title     = "{AI} Art: How artists are using and confronting machine learning
               | {HOW} {TO} {SEE} {LIKE} A {MACHINE}",
  author    = "{The Museum of Modern Art}",
  publisher = "Youtube",
  abstract  = "For the latest episode of our How to See series, we spoke with
               three artists—Kate Crawford, Trevor Paglen, and Refik Anadol—who
               engage with the ways that AI ...",
  month     =  mar,
  year      =  2023,
  keywords  = "moma; museum of modern art; new york; art; artist; museum;
               contemporary"
}

@MISC{noauthor_undated-ew,
  title        = "{OpenAI} {API}",
  abstract     = "An API for accessing new AI models developed by OpenAI",
  howpublished = "\url{https://platform.openai.com/docs/guides/embeddings/limitations-risks}",
  note         = "Accessed: 2023-4-13",
  language     = "en"
}

@ARTICLE{May2019-jg,
  title         = "On Measuring Social Biases in Sentence Encoders",
  author        = "May, Chandler and Wang, Alex and Bordia, Shikha and Bowman,
                   Samuel R and Rudinger, Rachel",
  journal       = "arXiv [cs.CL]",
  abstract      = "The Word Embedding Association Test shows that GloVe and
                   word2vec word embeddings exhibit human-like implicit biases
                   based on gender, race, and other social constructs (Caliskan
                   et al., 2017). Meanwhile, research on learning reusable text
                   representations has begun to explore sentence-level texts,
                   with some sentence encoders seeing enthusiastic adoption.
                   Accordingly, we extend the Word Embedding Association Test to
                   measure bias in sentence encoders. We then test several
                   sentence encoders, including state-of-the-art methods such as
                   ELMo and BERT, for the social biases studied in prior work
                   and two important biases that are difficult or impossible to
                   test at the word level. We observe mixed results including
                   suspicious patterns of sensitivity that suggest the test's
                   assumptions may not hold in general. We conclude by proposing
                   directions for future work on measuring bias in sentence
                   encoders.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{LaPorte_undated-js,
  title        = "Listen to Wikipedoa",
  author       = "LaPorte, Stephen and Hashemi, Mahmoud",
  abstract     = "Listen to recent changes on Wikipedia",
  howpublished = "\url{http://listen.hatnote.com/}",
  note         = "Accessed: 2023-4-13"
}

@MISC{Nakajima_undated-er,
  title       = "babyagi",
  author      = "Nakajima, Yohei",
  institution = "Github",
  abstract    = "Contribute to yoheinakajima/babyagi development by creating an
                 account on GitHub.",
  language    = "en"
}

@ARTICLE{Walker2010-fy,
  title     = "Experiencing flow: Is doing it together better than doing it
               alone?",
  author    = "Walker, Charles J",
  journal   = "J. Posit. Psychol.",
  publisher = "Routledge",
  volume    =  5,
  number    =  1,
  pages     = "3--11",
  abstract  = "A survey study and two experiments were done to test the
               hypothesis that social flow is more enjoyable than solitary flow.
               In the survey study it was found that recalled social flow
               experiences were rated more enjoyable than solitary flow
               experiences. In the first experiment when challenge and skill
               were the same across social and solitary conditions, social flow
               was reported to be more enjoyable than solitary flow. In the
               second experiment when the level of social interdependence was
               manipulated it was found that participants in highly
               interdependent teams reported more joy in flow than individuals
               performing less interdependently. In both experiments, people
               playing simple paddleball games reported and expressed more joy
               performing with others than alone. Taken together, the three
               investigations support the conclusion that doing it together is
               better than doing it alone. Solitary flow, while quite enjoyable,
               is not as enjoyable as social flow.",
  month     =  jan,
  year      =  2010
}

@ARTICLE{Eloundou2023-et,
  title         = "{GPTs} are {GPTs}: An Early Look at the Labor Market Impact
                   Potential of Large Language Models",
  author        = "Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock,
                   Daniel",
  journal       = "arXiv [econ.GN]",
  abstract      = "We investigate the potential implications of Generative
                   Pre-trained Transformer (GPT) models and related technologies
                   on the U.S. labor market. Using a new rubric, we assess
                   occupations based on their correspondence with GPT
                   capabilities, incorporating both human expertise and
                   classifications from GPT-4. Our findings indicate that
                   approximately 80\% of the U.S. workforce could have at least
                   10\% of their work tasks affected by the introduction of
                   GPTs, while around 19\% of workers may see at least 50\% of
                   their tasks impacted. The influence spans all wage levels,
                   with higher-income jobs potentially facing greater exposure.
                   Notably, the impact is not limited to industries with higher
                   recent productivity growth. We conclude that Generative
                   Pre-trained Transformers exhibit characteristics of
                   general-purpose technologies (GPTs), suggesting that as these
                   models could have notable economic, social, and policy
                   implications.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "econ.GN"
}

@MISC{Ianigro2022-wg,
  title     = "Plecto: Investigating the musical affordances of Continuous Time
               Recurrent Neural Networks",
  author    = "Ianigro, Steffan",
  publisher = "UNSW Sydney",
  abstract  = "``Plecto: Investigating the musical affordances of Continuous
               Time Recurrent Neural Networks'' is a practice-based research
               project that investigates how continuous time recurrent neural
               networks (CTRNNs) can be applied to the problem of achieving
               gestural control in improvised electronic music. One of the
               challenges of improvising using computers is manipulating
               different compositional layers during a performance while
               maintaining granular and expressive control. Artists turn to
               concepts such as artificial life to solve this problem and pursue
               software agents with complex, responsive and organic qualities
               that lead to the perception of lifelikeness. Guided by this
               theme, I propose a design for a low frequency oscillator (LFO),
               called Plecto, for use within existing composition workflows that
               harnesses the idiosyncratic behaviours of CTRNNs as a gestural
               agent within improvised electronic music performances. CTRNNs
               have been used in studies of biological modelling such as animal
               locomotion, and also of minimally cognitive behaviours such as
               basic object perception. Their ability to produce lifelike
               abstract forms makes them well suited as a source of gestural
               control. Oliver Bown and Sebastian Lexer have applied CTRNNs to
               musical event generation, using evolutionary algorithms (EA) to
               search for different CTRNN behaviours. I have extended this
               approach, using a novelty search (NS) variant for the open-ended
               discovery of CTRNN configurations, each exhibiting novel
               behaviours that can be applied to different musical problems.
               Through a series of computational studies, I have explored the
               lifelike qualities of CTRNNs best suited for gestural control and
               a novelty search algorithm design for their discovery. An
               iterative design process was also undertaken, establishing clear
               design principles adopted to build a usable representation of the
               CTRNN algorithm within an LFO device built for the Ableton Live
               environment. Evaluation of the tool was conducted through a user
               survey and practice-based case studies that incorporate the
               device into my own improvised electronic music workflow as a
               gestural agent. The primary outcomes of this research are a suite
               of software that can be adopted by the broader community of
               practitioners and a series of compositions reflecting the impacts
               of the CTRNN algorithm on my creative process.",
  year      =  2022
}

@INCOLLECTION{Candy2019-vg,
  title     = "Reflective creative practice",
  author    = "Candy, Linda",
  booktitle = "The Creative Reflective Practitioner",
  publisher = "Routledge",
  pages     = "44--101",
  month     =  nov,
  year      =  2019
}

@ARTICLE{Shneiderman2006-di,
  title     = "Creativity support tools: Report from a {U}.s. national science
               foundation sponsored workshop",
  author    = "Shneiderman, Ben and Fischer, Gerhard and Czerwinski, Mary and
               Resnick, Mitch and Myers, Brad and Candy, Linda and Edmonds,
               Ernest and Eisenberg, Mike and Giaccardi, Elisa and Hewett, Tom
               and Jennings, Pamela and Kules, Bill and Nakakoji, Kumiyo and
               Nunamaker, Jay and Pausch, Randy and Selker, Ted and Sylvan,
               Elisabeth and Terry, Michael",
  journal   = "Int. J. Hum. Comput. Interact.",
  publisher = "Informa UK Limited",
  volume    =  20,
  number    =  2,
  pages     = "61--77",
  month     =  may,
  year      =  2006,
  language  = "en"
}

@INPROCEEDINGS{Candy2020-ym,
  title     = "Creating with the Digital: Tool, Medium, Mediator, Partner",
  author    = "Candy, Linda",
  booktitle = "Interactivity, Game Creation, Design, Learning, and Innovation",
  publisher = "Springer International Publishing",
  pages     = "13--28",
  abstract  = "This chapter is about the different kinds of relationships that
               creative practitioners have with digital technologies in the
               making of artworks. Four types of creative process are described
               in which the role of the digital is differentiated as tool,
               medium, mediator and partner. In many cases, the digital
               technology performs more than one role: practitioners are using
               ready-made tools for making interactive works and at the same
               time writing algorithms to create digital partners with whom they
               perform. In this kind of creative practice, the technology is
               often the material of the creative works as well as the means by
               which they are made. It can enable a wide range of aesthetic
               qualities as well as facilitate different kinds of experience for
               both creators and audiences. This is a journey that many artists
               are taking in the 21st century contemporary digital arts world.
               The discussion is illustrated by the works of creative
               practitioners for whom digital technology is integral to the way
               they work.",
  year      =  2020
}

@ARTICLE{Clark2008-ui,
  title    = "Supersizing the Mind: Embodiment, Action, and Cognitive Extension",
  author   = "Clark, Andy",
  abstract = "Abstract. Studies of mind, thought, and reason have tended to
              marginalize the role of bodily form, real-world action, and
              environmental backdrop. In recent year",
  month    =  dec,
  year     =  2008
}

@ARTICLE{Edmonds2006-or,
  title     = "On creative engagement",
  author    = "Edmonds, Ernest and Muller, Lizzie and Connell, Matthew",
  journal   = "Visual Communication",
  publisher = "SAGE Publications",
  volume    =  5,
  number    =  3,
  pages     = "307--322",
  abstract  = "This article is concerned with the design of interactive art
               systems intended for display in public locations. It reviews
               approaches to interactive art systems and discusses the issue of
               creative engagement with them by the active audience. An approach
               to elaborating a model of creative engagement is described and
               exploratory work on its refinement is reported.",
  month     =  oct,
  year      =  2006
}

@ARTICLE{Bilda2008-hx,
  title    = "Designing for creative engagement",
  author   = "Bilda, Zafer and Edmonds, Ernest and Candy, Linda",
  journal  = "Design Studies",
  volume   =  29,
  number   =  6,
  pages    = "525--540",
  abstract = "This paper addresses the problem of understanding creative
              engagement with interactive systems. A model of engagement is
              proposed which represents modalities and phases of interactive
              experiences. The model was derived from empirical studies of
              audience interaction with art systems. The aim is to provide a
              means of facilitating communication between participants in the
              interaction design process. The intention is to help improve
              collaboration between participants through examining,
              understanding and agreeing on the set of concepts and modalities
              on interactive experience. The ongoing research involves refining
              and developing the model into a more general-purpose instrument.",
  month    =  nov,
  year     =  2008,
  keywords = "interaction design; user behaviour; evaluation; engagement;
              modelling"
}

@ARTICLE{Karwowski2018-ot,
  title    = "Measuring creative self-efficacy and creative personal identity",
  author   = "Karwowski, Maciej and Lebuda, Izabela and Wiśniewska, Ewa",
  journal  = "The International Journal of Creativity \& Problem Solving",
  volume   =  28,
  number   =  1,
  pages    = "45--57",
  abstract = "This paper presents the Short Scale of Creative Self (SSCS)—an
              instrument to measure trait-like creative self-efficacy and
              creative personal identity: characteristics of growing importance
              in creativity literature. Study 1 (N = 1,582) confirmed the
              assumed factor structure of the SSCS as well as high internal
              consistency of both sub-scales. Study 2 (N = 186) demonstrated
              high reliability of measurement over time, and Study 3 (N = 80)
              showed robust correlations with the scale previously used for
              measuring creative self-efficacy. Study 4 (N = 385) revealed
              positive links of creative self-efficacy and creative personal
              identity with self-reported originality and Test for Creative
              Thinking (TCT-DP), and weaker, yet expected links with
              intelligence, as well as positive relations with self-esteem,
              emotional intelligence and intrinsic motivation. Study 5 (N = 115)
              showed statistically significant relations of the new scales with
              divergent thinking. We discuss the promises and shortcomings of
              applying the SSCS in the school settings. (PsycINFO Database
              Record (c) 2019 APA, all rights reserved)",
  month    =  apr,
  year     =  2018
}

@MISC{Leeds2021-hc,
  title        = "A neural net wrote my book. Kidding. Sort of",
  author       = "Leeds, Leanne",
  booktitle    = "Leanne Leeds",
  abstract     = "The following section includes tips, tricks, and my experience
                  using Sudowrite, an app created using GPT-3 to assist writers
                  in writing fiction. This blog assumes you know what this is
                  and have some familiarity with it. If not, the last section
                  links to some info. Sudowrite Tips and Tricks My first days
                  with Sudowrite More on AI-Assisted Writing Sudowrite Tips and
                  Tricks The following are some tips and tricks I discovered
                  while using Sudowrite. Because there is no right or wrong way
                  to use AI in your writing, none of these posts should be
                  interpreted as ``the way things should be done.'' It's just a
                  suggestion to get you started. Dates of posting are provided,
                  with the oldest being first. (Sudowrite changes and progresses
                  fairly frequently, so keep that in mind.) My first days with
                  Sudowrite When I was interested in giving this a whirl, I
                  found little on it regarding authors, so I thought I'd add my
                  experience here. The following blog posts chronicle my
                  experience with Sudowrite if/when I have some insight. It's
                  not necessarily going to be ongoing. Dates are provided,
                  oldest is first. Sudowrite/\#AI Tools \& Articles Tools I use:
                  Sudowrite Quillbot LitRPG Adventures - a must-have for fantasy
                  writers. Articles and Info New Yorker: The Computers Are
                  Getting Better at Writing The Creative Penn (Youtube): The
                  AI-Augmented Author. Writing With GPT-3 With Paul Bellow The
                  Creative Penn (Youtube): Writing Fiction With AI. Sudowrite
                  With Amit Gupta The Verge: The Great Fiction of AI: The
                  strange world of high-speed semi-automated genre fiction - a
                  long form article that yours truly appears in. Sudowrite an AI
                  Writing tool for authors: Features and Demonstration on the
                  Business of Writing Podcast featuring author Elizabeth Ann
                  West.",
  month        =  jun,
  year         =  2021,
  howpublished = "\url{https://leanneleeds.com/sudowrite/}",
  note         = "Accessed: 2022-12-26",
  language     = "en"
}

@MISC{Dzieza2022-qj,
  title        = "How Kindle novelists are using {ChatGPT}",
  author       = "Dzieza, Josh",
  abstract     = "Authors of Kindle genre fiction have been experimenting with
                  AI tools. One novelist discusses how writers are thinking
                  about ChatGPT.",
  month        =  dec,
  year         =  2022,
  howpublished = "\url{https://www.theverge.com/23520625/chatgpt-openai-amazon-kindle-novel}",
  note         = "Accessed: 2022-12-26"
}

@MISC{noauthor_undated-ae,
  title       = "Mubert-Text-to-Music: A simple notebook demonstrating
                 prompt-based music generation via Mubert {API}",
  institution = "Github",
  abstract    = "A simple notebook demonstrating prompt-based music generation
                 via Mubert API - MubertAI/Mubert-Text-to-Music: A simple
                 notebook demonstrating prompt-based music generation via Mubert
                 API",
  language    = "en"
}

@ARTICLE{Ramesh2021-xb,
  title         = "Zero-Shot Text-to-Image Generation",
  author        = "Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray,
                   Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and
                   Sutskever, Ilya",
  journal       = "arXiv [cs.CV]",
  abstract      = "Text-to-image generation has traditionally focused on finding
                   better modeling assumptions for training on a fixed dataset.
                   These assumptions might involve complex architectures,
                   auxiliary losses, or side information such as object part
                   labels or segmentation masks supplied during training. We
                   describe a simple approach for this task based on a
                   transformer that autoregressively models the text and image
                   tokens as a single stream of data. With sufficient data and
                   scale, our approach is competitive with previous
                   domain-specific models when evaluated in a zero-shot fashion.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Wei2022-bm,
  title         = "Chain of Thought Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  journal       = "arXiv [cs.CL]",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought prompting,
                   where a few chain of thought demonstrations are provided as
                   exemplars in prompting. Experiments on three large language
                   models show that chain of thought prompting improves
                   performance on a range of arithmetic, commonsense, and
                   symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math word
                   problems, surpassing even finetuned GPT-3 with a verifier.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kalonaris2022-ug,
  title         = "Tokyo Kion-On: Query-Based Generative Sonification of
                   Atmospheric Data",
  author        = "Kalonaris, Stefano",
  journal       = "arXiv [cs.SD]",
  abstract      = "Amid growing environmental concerns, interactive displays of
                   data constitute an important tool for exploring and
                   understanding the impact of climate change on the planet's
                   ecosystemic integrity. This paper presents Tokyo kion-on, a
                   query-based sonification model of Tokyo's air temperature
                   from 1876 to 2021. The system uses a recurrent neural network
                   architecture known as LSTM with attention trained on a small
                   dataset of Japanese melodies and conditioned upon said
                   atmospheric data. After describing the model's
                   implementation, a brief comparative illustration of the
                   musical results is presented, along with a discussion on how
                   the exposed hyper-parameters can promote active and
                   non-linear exploration of the data.",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD"
}

@INPROCEEDINGS{Polli2004-nu,
  title       = "Atmospherics/weather works: A multi-channel storm sonification
                 project",
  author      = "Polli, Andrea",
  institution = "Georgia Institute of Technology",
  year        =  2004
}

@ARTICLE{Quinn2001-qb,
  title     = "Research set to music: The climate symphony and other
               sonifications of ice core, radar, {DNA}, seismic and solar wind
               data",
  author    = "Quinn, M",
  publisher = "smartech.gatech.edu",
  abstract  = "… the Office of Polar Programs and was warmly received. This
               paper describes the Climate Symphony portion of the … The script
               for the Climate Symphony performance was developed in …",
  year      =  2001
}

@ARTICLE{Floridi2020-qc,
  title     = "{GPT}-3: Its nature, scope, limits, and consequences",
  author    = "Floridi, Luciano and Chiriatti, Massimo",
  journal   = "Minds Mach.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  30,
  number    =  4,
  pages     = "681--694",
  abstract  = "AbstractIn this commentary, we discuss the nature of reversible
               and irreversible questions, that is, questions that may enable
               one to identify the nature of the source of their answers. We
               then introduce GPT-3, a third-generation, autoregressive language
               model that uses deep learning to produce human-like texts, and
               use the previous distinction to analyse it. We expand the
               analysis to present three tests based on mathematical, semantic
               (that is, the Turing Test), and ethical questions and show that
               GPT-3 is not designed to pass any of them. This is a reminder
               that GPT-3 does not do what it is not supposed to do, and that
               any interpretation of GPT-3 as the beginning of the emergence of
               a general form of artificial intelligence is merely uninformed
               science fiction. We conclude by outlining some of the significant
               consequences of the industrialisation of automatic and cheap
               production of good, semantic artefacts.",
  month     =  dec,
  year      =  2020,
  language  = "en"
}

@MISC{noauthor_undated-aw,
  title        = "An Image is Worth One Word: Personalizing Text-to-Image
                  Generation using Textual Inversion",
  abstract     = "Textual Inversions for personalized Text-to-Image generation",
  howpublished = "\url{https://textual-inversion.github.io/}",
  note         = "Accessed: 2022-8-30"
}

@MISC{Github2022-xd,
  title       = "Your {AI} pair programmer",
  author      = "{Github}",
  institution = "Github",
  abstract    = "GitHub Copilot works alongside you directly in your editor,
                 suggesting whole lines or entire functions for you.",
  year        =  2022,
  language    = "en"
}

@ARTICLE{Henriksen2018-wa,
  title   = "A Cybernetic Perspective on Design and Creativity: a Conversation
             with Dr. Paul Pangaro",
  author  = "Henriksen, Danah and Mishra, Punya and Warr, Melissa and {The
             Deep-Play Research Group}",
  journal = "TechTrends",
  volume  =  62,
  number  =  1,
  pages   = "6--10",
  month   =  jan,
  year    =  2018
}

@ARTICLE{Anantrasirichai2022-ps,
  title    = "Artificial intelligence in the creative industries: a review",
  author   = "Anantrasirichai, Nantheera and Bull, David",
  journal  = "Artificial Intelligence Review",
  volume   =  55,
  number   =  1,
  pages    = "589--656",
  abstract = "This paper reviews the current state of the art in artificial
              intelligence (AI) technologies and applications in the context of
              the creative industries. A brief background of AI, and
              specifically machine learning (ML) algorithms, is provided
              including convolutional neural networks (CNNs), generative
              adversarial networks (GANs), recurrent neural networks (RNNs) and
              deep Reinforcement Learning (DRL). We categorize creative
              applications into five groups, related to how AI technologies are
              used: (i) content creation, (ii) information analysis, (iii)
              content enhancement and post production workflows, (iv)
              information extraction and enhancement, and (v) data compression.
              We critically examine the successes and limitations of this
              rapidly advancing technology in each of these areas. We further
              differentiate between the use of AI as a creative tool and its
              potential as a creator in its own right. We foresee that, in the
              near future, ML-based AI will be adopted widely as a tool or
              collaborative assistant for creativity. In contrast, we observe
              that the successes of ML in domains with fewer constraints, where
              AI is the ‘creator’, remain modest. The potential of AI (or its
              developers) to win awards for its original creations in
              competition with human creatives is also limited, based on
              contemporary technologies. We therefore conclude that, in the
              context of creative industries, maximum benefit from AI will be
              derived where its focus is human-centric—where it is designed to
              augment, rather than replace, human creativity.",
  month    =  jan,
  year     =  2022
}

@INPROCEEDINGS{Hwang2022-hp,
  title     = "Too late to be creative? {AI}-empowered tools in creative
               processes",
  author    = "Hwang, Angel Hsing-Chi",
  booktitle = "CHI Conference on Human Factors in Computing Systems Extended
               Abstracts",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The present case study examines the product landscape of current
               AI-empowered co-creative tools. Specifically, I review literature
               in both creativity and HCI research and investigate how these
               tools support different stages in humans’ creative processes and
               how common challenges in human-AI interaction (HAII) are
               addressed. I find these AI-driven tools mostly support the
               generation and execution of ideas and are less involved in the
               early stages of co-creation. Moreover, HAII challenges identified
               in other fields receive little attention in the creative domain.
               Based on a synthetic analysis, I elaborate on how future tools
               can leverage the ”non-human” quality of AI to achieve innovation
               through a more human-centered, collaborative journey.",
  month     =  apr,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Hutchings2020-bv,
  title     = "Design considerations for real-time collaboration with creative
               artificial intelligence",
  author    = "Hutchings, P and Gifford, T and Yee-King, M and Llano, M T and
               {others}",
  journal   = "Organised",
  publisher = "cambridge.org",
  abstract  = "Machines incorporating techniques from artificial intelligence
               and machine learning can work with human users on a
               moment-to-moment, real-time basis to generate creative outcomes,
               …",
  year      =  2020
}

@ARTICLE{Pease_undated-jx,
  title   = "On the Machine Condition and its Creative Expression",
  author  = "{Pease} and {Guckelsberger} and {McCormack} and {Llano}",
  journal = "Int. Commun. Chin. Cult."
}

@ARTICLE{Stankeviciene2011-gi,
  title     = "Creative ecologies: developing and managing new concepts of
               creative economy",
  author    = "Stankevičienė, Jelena and Levickaitė, Rasa and Braškutė, Monika
               and Noreikaitė, Elinga",
  journal   = "Business, Management and Economics Engineering",
  publisher = "journals.vilniustech.lt",
  volume    =  9,
  number    =  2,
  pages     = "277--294",
  abstract  = "The idea of creativity is becoming more and more relevant and is
               ob­served in various fields, such as contemporary economics,
               technology and science. This article is based on the creative
               ecology theory which has emerged from the creative economy theory
               developed by economist John Howkins. According to him, it is
               fundamental to understand the current crisis in the natural
               environment and economy, and the balance of creativity and
               control required in our response. The article is based on three
               research questions: 1) what are the fundamental prin­ciples of
               creativity and the process of sustainable creation; 2) how can
               one de­velop high quality ideas and turn them into reality; 3) is
               it possible for the reckless consuming society to share
               sustainable creative products and how could this be achieved.
               Creative economy is a rapidly growing sector of world market.
               Howkins (2010) uses the creative ecologies theory to analyse
               human creativity and abilities to create. Creative ecology is
               presented as “a niche where diverse individuals ex­press
               themselves in a systemic and adaptive way, using ideas to produce
               new ideas; and where others support this endeavour even if they
               don’t understand it”. Four as­pects (diversity, change, learning,
               adaptation) of ecological thinking are presented as directly
               related to creativity and innovations, thus extremely important
               to any contemporary organisation seeking leadership in the
               creative economy. Looking into the new concept of creativity,
               authors of the article came to the conclusion that a sustainable
               relationship between creativity and science is a necessary tool
               for change, development and management of new concepts of
               creative economy. The article is based on the project Creative
               Ecologies: Creating, Developing and Sharing Sustainable Ideas
               presented by the authors in the Euroweek 2011 confer­ence
               Water4World. The project received two awards – the 1st prize in
               the project section and The Best Project of the Euroweek 2011.",
  month     =  nov,
  year      =  2011,
  keywords  = "creative ecologies; creative economy; new concepts",
  language  = "en"
}

@ARTICLE{McNamara2012-dx,
  title     = "Six rules for practice-led research",
  author    = "McNamara, Andrew",
  journal   = "TEXT: Journal of Writing and Writing Programs",
  publisher = "The Australian Association of Writing Programs",
  volume    =  2012,
  number    = "S14",
  pages     = "1--15",
  abstract  = "Recent experience of practice-led postgraduate supervision has
               prompted me to conclude that the practice-led research method, as
               it is currently construed, produces good outcomes, especially in
               permitting practitioners in the creative arts, design and media
               into the research framework, but at the same time it also
               generates certain recurring difficulties. What are these
               difficulties? Practice-led candidates tend to rely on a narrow
               range of formulations with the result that they assume: (i) the
               innovative nature of practice-led research; (ii) that its novelty
               is based in opposition to other research methods; (iii) that
               practice is intrinsically research, often leading to tautological
               formulations; (iv) the hyper-self-reflexive nature of
               practice-led research. This set of guidelines was composed in
               order to circumvent the shortcomings that result from these
               recurring formulations. My belief is that, if these shortcomings
               are avoided, there is nothing to prevent practice-led from
               further developing as a research inquiry and thus achieving
               rewarding and successful research outcomes. Originally composed
               for the purposes of postgraduate supervision, these six rules are
               presented here in the context of a wider analysis of the
               emergence of practice-led research and its current conditions of
               possibility as a research method.",
  year      =  2012
}

@ARTICLE{Schon_undated-jd,
  title   = "The Reflective Practitioner: How Professionals Think in Action.
             Arena",
  author  = "{Schön}",
  journal = "Farnham: Ashgate Publishing"
}

@INPROCEEDINGS{Xu2022-je,
  title     = "A systematic evaluation of large language models of code",
  author    = "Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn,
               Vincent Josua",
  booktitle = "Proceedings of the 6th ACM SIGPLAN International Symposium on
               Machine Programming",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--10",
  abstract  = "Large language models (LMs) of code have recently shown
               tremendous promise in completing code and synthesizing code from
               natural language descriptions. However, the current
               state-of-the-art code LMs (e.g., Codex) are not publicly
               available, leaving many questions about their model and data
               design decisions. We aim to fill in some of these blanks through
               a systematic evaluation of the largest existing models: Codex,
               GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various
               programming languages. Although Codex itself is not open-source,
               we find that existing opensource models do achieve close results
               in some programming languages, although targeted mainly for
               natural language modeling. We further identify an important
               missing piece in the form of a large open-source model trained
               exclusively on a multi-lingual corpus of code. We release a new
               model, PolyCoder, with 2.7B parameters based on the GPT-2
               architecture, that was trained on 249GB of code across 12
               programming languages on a single machine. In the C programming
               language, PolyCoder outperforms all models including Codex. Our
               trained models are open-source and publicly available at
               https://github.com/VHellendoorn/Code-LMs, which enables future
               research and application in this area. We have an online appendix
               at https://arxiv.org/abs/2202.13169.",
  series    = "MAPS 2022",
  month     =  jun,
  year      =  2022,
  keywords  = "evaluation, code language model, code generation, pretraining,
               open-source"
}

@MISC{Hellendoorn_undated-yw,
  title       = "Code-{LMs}: Guide to using pre-trained large language models of
                 source code",
  author      = "Hellendoorn, Vincent",
  institution = "Github",
  abstract    = "Guide to using pre-trained large language models of source code
                 - VHellendoorn/Code-LMs: Guide to using pre-trained large
                 language models of source code",
  language    = "en"
}

@ARTICLE{Thoppilan2022-jf,
  title         = "{LaMDA}: Language Models for Dialog Applications",
  author        = "Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and
                   Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze
                   and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu
                   and Li, Yaguang and Lee, Hongrae and Zheng, Huaixiu Steven
                   and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping
                   and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and
                   Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts,
                   Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and
                   Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and
                   Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and
                   Meier-Hellstern, Kathleen and Morris, Meredith Ringel and
                   Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and
                   Soraker, Johnny and Zevenbergen, Ben and Prabhakaran,
                   Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson,
                   Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee,
                   Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena
                   and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and
                   Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and
                   Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and
                   Chi, Ed and Le, Quoc",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present LaMDA: Language Models for Dialog Applications.
                   LaMDA is a family of Transformer-based neural language models
                   specialized for dialog, which have up to 137B parameters and
                   are pre-trained on 1.56T words of public dialog data and web
                   text. While model scaling alone can improve quality, it shows
                   less improvements on safety and factual grounding. We
                   demonstrate that fine-tuning with annotated data and enabling
                   the model to consult external knowledge sources can lead to
                   significant improvements towards the two key challenges of
                   safety and factual grounding. The first challenge, safety,
                   involves ensuring that the model's responses are consistent
                   with a set of human values, such as preventing harmful
                   suggestions and unfair bias. We quantify safety using a
                   metric based on an illustrative set of human values, and we
                   find that filtering candidate responses using a LaMDA
                   classifier fine-tuned with a small amount of
                   crowdworker-annotated data offers a promising approach to
                   improving model safety. The second challenge, factual
                   grounding, involves enabling the model to consult external
                   knowledge sources, such as an information retrieval system, a
                   language translator, and a calculator. We quantify factuality
                   using a groundedness metric, and we find that our approach
                   enables the model to generate responses grounded in known
                   sources, rather than responses that merely sound plausible.
                   Finally, we explore the use of LaMDA in the domains of
                   education and content recommendations, and analyze their
                   helpfulness and role consistency.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Fallman2008-ch,
  title     = "The Interaction Design Research Triangle of Design Practice,
               Design Studies, and Design Exploration",
  author    = "Fallman, Daniel",
  journal   = "Design Issues",
  publisher = "The MIT Press",
  volume    =  24,
  number    =  3,
  pages     = "4--18",
  year      =  2008
}

@ARTICLE{Schon1987-fy,
  title     = "The reflective practitioner: How professionals think in action",
  author    = "Schon, Donald A",
  journal   = "Adm. Sci. Q.",
  publisher = "JSTOR",
  volume    =  32,
  number    =  4,
  pages     =  614,
  month     =  dec,
  year      =  1987
}

@ARTICLE{Langlotz2019-hh,
  title    = "Will Artificial Intelligence Replace Radiologists?",
  author   = "Langlotz, Curtis P",
  journal  = "Radiol Artif Intell",
  volume   =  1,
  number   =  3,
  pages    = "e190058",
  month    =  may,
  year     =  2019,
  language = "en"
}

@MISC{noauthor_2021-qk,
  title        = "{HCI} and {ML}: Putting People First",
  booktitle    = "Magenta",
  abstract     = "The goal of the Magenta project is not just to build powerful
                  generative models, but to use those models to empower people
                  to realize their creative goals. I...",
  month        =  dec,
  year         =  2021,
  howpublished = "\url{https://magenta.tensorflow.org/people-first-hci-ml-collaborations}",
  note         = "Accessed: 2022-7-12",
  language     = "en"
}

@ARTICLE{Linardatos2020-uq,
  title    = "Explainable {AI}: A Review of Machine Learning Interpretability
              Methods",
  author   = "Linardatos, Pantelis and Papastefanopoulos, Vasilis and
              Kotsiantis, Sotiris",
  journal  = "Entropy",
  volume   =  23,
  number   =  1,
  abstract = "Recent advances in artificial intelligence (AI) have led to its
              widespread industrial adoption, with machine learning systems
              demonstrating superhuman performance in a significant number of
              tasks. However, this surge in performance, has often been achieved
              through increased model complexity, turning such systems into
              ``black box'' approaches and causing uncertainty regarding the way
              they operate and, ultimately, the way that they come to decisions.
              This ambiguity has made it problematic for machine learning
              systems to be adopted in sensitive yet critical domains, where
              their value could be immense, such as healthcare. As a result,
              scientific interest in the field of Explainable Artificial
              Intelligence (XAI), a field that is concerned with the development
              of new methods that explain and interpret machine learning models,
              has been tremendously reignited over recent years. This study
              focuses on machine learning interpretability methods; more
              specifically, a literature review and taxonomy of these methods
              are presented, as well as links to their programming
              implementations, in the hope that this survey would serve as a
              reference point for both theorists and practitioners.",
  month    =  dec,
  year     =  2020,
  keywords = "black-box; explainability; fairness; interpretability; machine
              learning; sensitivity; xai",
  language = "en"
}

@ARTICLE{Yudkowsky2016-wf,
  title     = "The {AI} alignment problem: why it is hard, and where to start",
  author    = "{Yudkowsky}",
  journal   = "Symbolic Systems Distinguished Speaker",
  publisher = "intelligence.org",
  abstract  = "… This is where we are on most of the AI alignment problems, like
               if I ask you, “How do you build a friendly AI ?” What stops you
               is not that you don’t have enough computing power. What …",
  year      =  2016
}

@ARTICLE{Csikszentmihalyi1992-ah,
  title    = "The measurement of flow in everyday life: toward a theory of
              emergent motivation",
  author   = "Csikszentmihalyi, M and Rathunde, K",
  journal  = "Nebr. Symp. Motiv.",
  volume   =  40,
  pages    = "57--97",
  year     =  1992,
  language = "en"
}

@ARTICLE{Kingma2013-di,
  title         = "Auto-Encoding Variational Bayes",
  author        = "Kingma, Diederik P and Welling, Max",
  journal       = "arXiv [stat.ML]",
  abstract      = "How can we perform efficient inference and learning in
                   directed probabilistic models, in the presence of continuous
                   latent variables with intractable posterior distributions,
                   and large datasets? We introduce a stochastic variational
                   inference and learning algorithm that scales to large
                   datasets and, under some mild differentiability conditions,
                   even works in the intractable case. Our contributions is
                   two-fold. First, we show that a reparameterization of the
                   variational lower bound yields a lower bound estimator that
                   can be straightforwardly optimized using standard stochastic
                   gradient methods. Second, we show that for i.i.d. datasets
                   with continuous latent variables per datapoint, posterior
                   inference can be made especially efficient by fitting an
                   approximate inference model (also called a recognition model)
                   to the intractable posterior using the proposed lower bound
                   estimator. Theoretical advantages are reflected in
                   experimental results.",
  month         =  dec,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML"
}

@INPROCEEDINGS{Candy2002-ra,
  title     = "Modeling co-creativity in art and technology",
  author    = "Candy, Linda and Edmonds, Ernest",
  booktitle = "Proceedings of the 4th conference on Creativity \& cognition",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "134--141",
  abstract  = "Collaborative projects in art and technology provide an
               opportunity to investigate how co-creativity takes place. This
               paper describes some of the characteristics of collaborative work
               that were identified from empirical evidence captured during the
               COSTART project [4]. We examine the way the information was
               analyzed and the results of that exercise. An approach to
               modeling co-creativity based on case study data is described and
               three example models proposed. This work enabled us to consider
               the implications of the different models for supporting
               creativity and their relationship to success factors. We conclude
               that the provision of 'support' for co-creativity in art and
               technology needs to include ongoing collaborative relationships
               that are fostered by organizations dedicated to the co-evolution
               of both art and new technology.",
  series    = "C\&C '02",
  month     =  oct,
  year      =  2002,
  keywords  = "creativity, digital technology, art, modeling, collaboration"
}

@INPROCEEDINGS{Muller2022-lh,
  title     = "{GenAICHI}: Generative {AI} and {HCI}",
  author    = "Muller, Michael and Chilton, Lydia B and Kantosalo, Anna and
               Martin, Charles Patrick and Walsh, Greg",
  booktitle = "CHI Conference on Human Factors in Computing Systems Extended
               Abstracts",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This workshop applies human centered themes to a new and powerful
               technology, generative artificial intelligence (AI). Unlike AI
               systems that produce decisions or descriptions, generative AI
               systems can produce new and creative content that can include
               images, texts, music, video, and other forms of design. The
               results are often similar to results produced by humans. However,
               it is not yet clear how humans make sense of generative AI
               algorithms or their outcomes. It is also not yet clear how humans
               can control and more generally, interact with, these powerful
               capabilities. Finally, it is not clear what kinds of
               collaboration patterns will emerge when creative humans and
               creative technologies work together. It is time to convene the
               interdisciplinary research domain of generative AI and HCI.
               Participation in this invitational workshop is open to seasoned
               scholars and early career researchers. We solicit descriptions of
               completed projects, works-in-progress, and provocations. Together
               we will develop theories and practices in this intriguing new
               domain.",
  month     =  apr,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Xu2021-ap,
  title    = "Human-{AI} interaction: An emerging interdisciplinary domain for
              enabling human-centered {AI}",
  author   = "Xu, Wei and Ge, Liezhong and Gao, Zaifeng",
  journal  = "ArXiv",
  abstract = "This paper analyzes the new challenges faced by AI systems and
              further elaborates the “Human-Centered AI” (HCAI) approach, and
              systematically proposes an emerging interdisciplinary domain of
              ``Human-AI Interaction'' (HAII), and defines the objective,
              methodology, and scope. The new characteristics of AI technology
              have brought new challenges to the research and development of AI
              systems. AI technology has benefited humans, but if improperly
              developed, it will harm humans. At present, there is no systematic
              interdisciplinary approach to effectively deal with these new
              challenges. This paper analyzes the new challenges faced by AI
              systems and further elaborates the “Human-Centered AI” (HCAI)
              approach we proposed in 2019. In order to enable the
              implementation of the HCAI approach, we systematically propose an
              emerging interdisciplinary domain of ``Human-AI Interaction''
              (HAII), and define the objective, methodology, and scope. Based on
              literature review and analyses, this paper summarizes the main
              areas of the HAII research and application as well as puts forward
              the future research agenda for HAII. Finally, the paper provides
              strategic recommendations for future implementation of the HCAI
              approach and HAII work.",
  year     =  2021,
  language = "en"
}

@INPROCEEDINGS{Newn2020-mv,
  title     = "Nonverbal communication in human-{AI} interaction: Opportunities
               and challenges",
  author    = "Newn, Joshua and Singh, Ronal and Allison, Fraser and Madumal,
               Prashan and Velloso, Eduardo and Vetere, Frank",
  booktitle = "Human Computer Interaction and Emerging Technologies: Adjunct
               Proceedings from the INTERACT 2019 Workshops",
  publisher = "Cardiff University Press",
  abstract  = "This work investigated whether an artificial agent, given the
               ability to observe human gaze, can make inferences on intentions,
               and how aspects of these inferences can be communicated to a
               human collaborator. In recent years, we have explored the use of
               gaze—an important nonverbal communication signal and cue in
               everyday human-human interaction—for use with AI systems.
               Specifically, our work investigated whether an artificial agent,
               given the ability to observe human gaze, can make inferences on
               intentions, and how aspects of these inferences can be
               communicated to a human collaborator. We leveraged a range of
               humancomputer interaction techniques to inform the design of a
               gaze-enabled artificial agent that can predict and communicate
               predictions. In this paper, we include a snapshot of how AI and
               HCI can be brought together to inform the design of an
               explainable interface for an artificial agent. To conclude, we
               outline the challenges we faced when designing AI systems that
               incorporate nonverbal communication stemming from our work.",
  month     =  may,
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Rezwana2022-ui,
  title     = "Understanding user perceptions, collaborative experience and user
               engagement in different human-{AI} interaction designs for
               co-creative systems",
  author    = "Rezwana, Jeba and Maher, Mary Lou",
  booktitle = "Creativity and Cognition",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "The study involves user interaction with two prototypes of a
               co-creative system that contributes sketches as design
               inspirations during a design task and shows improved
               collaborative experience and user engagement with the system
               incorporating AI-to-human communication. Human-AI co-creativity
               involves humans and AI collaborating on a shared creative product
               as partners. In a creative collaboration, communication is an
               essential component among collaborators. In many existing
               co-creative systems, users can communicate with the AI, usually
               using buttons or sliders. Typically, the AI in co-creative
               systems cannot communicate back to humans, limiting their
               potential to be perceived as partners rather than just a tool.
               This paper presents a study with 38 participants to explore the
               impact of two interaction designs, with and without AI-to-human
               communication, on user engagement, collaborative experience and
               user perception of a co-creative AI. The study involves user
               interaction with two prototypes of a co-creative system that
               contributes sketches as design inspirations during a design task.
               The results show improved collaborative experience and user
               engagement with the system incorporating AI-to-human
               communication. Users perceive co-creative AI as more reliable,
               personal, and intelligent when the AI communicates to users. The
               findings can be used to design effective co-creative systems, and
               the insights can be transferred to other fields involving
               human-AI interaction and collaboration.",
  month     =  jun,
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Zheng2022-qw,
  title     = "{UX} research on conversational human-{AI} interaction: A
               literature review of the {ACM} digital library",
  author    = "Zheng, Qingxiao and Tang, Yiliu and Liu, Yiren and Liu, Weizi and
               Huang, Yun",
  booktitle = "CHI Conference on Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Early conversational agents (CAs) focused on dyadic human-AI
               interaction between humans and the CAs, followed by the
               increasing popularity of polyadic human-AI interaction, in which
               CAs are designed to mediate human-human interactions. CAs for
               polyadic interactions are unique because they encompass hybrid
               social interactions, i.e., human-CA, human-to-human, and
               human-to-group behaviors. However, research on polyadic CAs is
               scattered across different fields, making it challenging to
               identify, compare, and accumulate existing knowledge. To promote
               the future design of CA systems, we conducted a literature review
               of ACM publications and identified a set of works that conducted
               UX (user experience) research. We qualitatively synthesized the
               effects of polyadic CAs into four aspects of human-human
               interactions, i.e., communication, engagement, connection, and
               relationship maintenance. Through a mixed-method analysis of the
               selected polyadic and dyadic CA studies, we developed a suite of
               evaluation measurements on the effects. Our findings show that
               designing with social boundaries, such as privacy, disclosure,
               and identification, is crucial for ethical polyadic CAs. Future
               research should also advance usability testing methods and
               trust-building guidelines for conversational AI.",
  month     =  apr,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Kantosalo2020-zf,
  title    = "Five {C}'s for Human-Computer Co-Creativity - An Update on
              Classical Creativity Perspectives",
  author   = "Kantosalo, Anna and Takala, T",
  journal  = "ICCC",
  abstract = "A domain independent framework for discussing human–computer
              co-creativity is presented that allows the attribution of
              creativity not only to individual creators but to a collective of
              creators, recognising the importance of meta-level communication
              to the creative collaboration, and the variety of creative
              contributions that emerge during a co-Creative process. This paper
              presents a domain independent framework for discussing
              human–computer co-creativity. It expands on Rhodes’ (1961) four
              perspectives on creativity and their later adaptations to
              socio-cultural views of creativity and computational creativity.
              The new framework allows the attribution of creativity not only to
              individual creators but to a collective of creators, recognising
              the importance of meta-level communication to the creative
              collaboration, and the variety of creative contributions that
              emerge during a co-creative process. It also elaborates on the
              different communities and contexts surrounding co-creative
              collaboration and thus facilitates the analysis, evaluation and
              study of human– computer co-creativity by allowing researchers to
              describe and situate their work in the field.",
  year     =  2020,
  language = "en"
}

@ARTICLE{Bown2020-zn,
  title    = "A Speculative Exploration of the Role of Dialogue in
              Human-{ComputerCo}-creation",
  author   = "Bown, O and Grace, Kazjon and Bray, Liam and Ventura, Dan",
  journal  = "ICCC",
  abstract = "The paper motivates the pursuit of dialogic interaction and
              provides some explanation of why it has been defined through its
              impacts rather than its mechanism of action. In this paper we
              consider the notion of dialogic creative artificial intelligence
              (DCAI) systems, where co-creativity between a human user and a
              computational system is supported through dialogic interaction. By
              dialogue we mean both traditional language-based communication for
              explicit critique and persuasion (dialogue about creative
              artefacts) as well as a broader potentially non-linguistic notion
              of dialogue that emerges through the exchange of suggestions and
              changes (dialogue through creative artefacts). To capture this, we
              define DCAI as occurring when both system and user are able to
              influence each others’ creative objectives. The paper motivates
              our pursuit of dialogic interaction and provides some explanation
              of why we have defined it through its impacts rather than its
              mechanism of action. We provide two analyses to support our
              argument: an exploration of a commercial creativity support tool
              that has an extensive vocabulary for describing artefacts
              abstractly but does not meet our definition of dialogic
              interaction, and a case study of a creative interaction between
              two human professionals that exhibits dialogic interaction
              throughout. For the latter we consider how studies of human-human
              co-creation can offer non-obvious design concepts that might be
              applied to co-creative DCAI",
  year     =  2020,
  language = "en"
}

@ARTICLE{Rezwana2022-gg,
  title     = "Designing creative {AI} partners with {COFI}: A framework for
               modeling interaction in human-{AI} co-creative systems",
  author    = "Rezwana, Jeba and Maher, Mary Lou",
  journal   = "ACM Trans. Comput. Hum. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  abstract  = "Human-AI co-creativity involves both humans and AI collaborating
               on a shared creative product as partners. In a creative
               collaboration, interaction dynamics, such as turn-taking,
               contribution type, and communication, are the driving forces of
               the co-creative process. Therefore the interaction model is a
               critical and essential component for effective co-creative
               systems. There is relatively little research about interaction
               design in the co-creativity field, which is reflected in a lack
               of focus on interaction design in many existing co-creative
               systems. The primary focus of co-creativity research has been on
               the abilities of the AI. This paper focuses on the importance of
               interaction design in co-creative systems with the development of
               the Co-Creative Framework for Interaction design (COFI) that
               describes the broad scope of possibilities for interaction design
               in co-creative systems. Researchers can use COFI for modeling
               interaction in co-creative systems by exploring alternatives in
               this design space of interaction. COFI can also be beneficial
               while investigating and interpreting the interaction design of
               existing co-creative systems. We coded a dataset of existing 92
               co-creative systems using COFI and analyzed the data to show how
               COFI provides a basis to categorize the interaction models of
               existing co-creative systems. We identify opportunities to shift
               the focus of interaction models in co-creativity to enable more
               communication between the user and AI leading to human-AI
               partnerships.",
  month     =  feb,
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Inkpen2020-ce,
  title     = "Does my {AI} help or hurt? Exploring human-{AI} complementarity",
  author    = "Inkpen, Kori",
  booktitle = "Proceedings of the 28th ACM Conference on User Modeling,
               Adaptation and Personalization",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This talk will highlight important directions for Human-AI
               research, and explore the interplay between humans and AI
               systems. In a world where the use of AI is growing and evolving,
               where will we be in 5 years? 10 years? 20 years? What role will
               AI play in our society, and how will humans and AI interact?
               While there will undoubtedly be scenarios where AI systems will
               be able to outperform humans, there will also continue to be
               instances where humans will be a critical part of the process. As
               researchers explore improvements to AI systems, we also need to
               explore the interplay between humans and AI, and continue to
               evolve our understanding of how humans and AI systems can work
               together, effectively harnessing the benefits of both systems
               [3]. Designing effective interaction between the human and the AI
               systems is critical for future use of Human-AI systems [1].
               Merely building an AI system that blindly sends recommendations
               to users has been shown in some cases to decrease human
               performance [2]. Different models can also have differential
               impact on user's trust of the model, adherence to the
               recommendation, and can impact bias in decision making tasks.
               This talk will highlight important directions for Human-AI
               research.",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Zhang2021-ej,
  title     = "{COSMIC}: A conversational interface for human-{AI} music
               co-creation",
  author    = "Zhang, Yixiao and Xia, Gus and Levy, Mark and Dixon, Simon",
  booktitle = "NIME 2021",
  publisher = "PubPub",
  abstract  = "COSMIC is a chatbot with a two-fold design philosophy: to
               understand human creative intent and to help humans in their
               creation. In this paper, we propose COSMIC, a COnverSational
               Interface for Human-AI MusIc Co-Creation. It is a chatbot with a
               two-fold design philosophy: to understand human creative intent
               and to help humans in their creation. The core Natural Language
               Processing (NLP) module is responsible for three functions: 1)
               understanding human needs in chat, 2) cross-modal interaction
               between natural language understanding and music generation
               models, and 3) mixing and coordinating multiple algorithms to
               complete the composition.",
  year      =  2021,
  language  = "en"
}

@ARTICLE{Friedman2021-xg,
  title    = "Image Co-Creation by Non-Programmers and Generative Adversarial
              Networks",
  author   = "Friedman, D and Pollak, D",
  journal  = "IUI Workshops",
  abstract = "A new course intended for non-programmer MA students in
              human-computer interaction, aimed at training them in authoring
              content using generative models, finds ways to obtain their
              creative needs by mostly exploring the dataset level (as opposed
              to the model architecture). Generative models such as generative
              adversarial networks are now being studied extensively.
              Eventually, however, many of them are intended for non-programmers
              to work with, e.g. designers, artists, or other content creators.
              What happens when such individuals are confronted with using GANs?
              We present a case study – a new course intended for non-programmer
              MA students in human-computer interaction, aimed at training them
              in authoring content using generative models. As their final
              assignment, the students were asked to train generative
              adversarial networks in order to generate images from a predefined
              category of their choice. The students either used a graphical
              user interface (GUI)-based software or modified preexisting python
              code using simplified Google Colab notebooks. We present several
              lessons learned from this course. First, we analyze the joint
              human-AI creation process and recognize points where students
              could intervene, with anecdotal examples of how they creatively
              explored these opportunities. Interestingly, while the majority of
              algorithmic research is focused on how to make models more
              controllable (e.g., via conditioning or latent space
              disentanglement), the students found ways to obtain their creative
              needs by mostly exploring the dataset level (as opposed to the
              model architecture). Additionally, we present the results of a
              short survey, comparing the two modes of work (GUI vs code).",
  year     =  2021,
  language = "en"
}

@ARTICLE{Buschek2021-ks,
  title    = "Nine Potential Pitfalls when Designing Human-{AI} Co-Creative
              Systems",
  author   = "Buschek, Daniel and Mecke, Lukas and Lehmann, Florian and Dang,
              Hai",
  journal  = "IUI Workshops",
  abstract = "This position paper examines potential pitfalls on the way towards
              achieving human-AI co-creation with generative models in a way
              that is beneficial to the users’ interests and collects a set of
              nine potential pitfalls, based on the literature and experiences
              as researchers working at the intersection of HCI and AI. This
              position paper examines potential pitfalls on the way towards
              achieving human-AI co-creation with generative models in a way
              that is beneficial to the users’ interests. In particular, we
              collected a set of nine potential pitfalls, based on the
              literature and our own experiences as researchers working at the
              intersection of HCI and AI. We illustrate each pitfall with
              examples and suggest ideas for addressing it. Reflecting on all
              pitfalls, we discuss and conclude with implications for future
              research directions. With this collection, we hope to contribute
              to a critical and constructive discussion on the roles of humans
              and AI in co-creative interactions, with an eye on related
              assumptions and potential side-effects for creative practices and
              beyond.",
  year     =  2021,
  language = "en"
}

@ARTICLE{Liapis2016-zt,
  title    = "Boosting computational creativity with human interaction in
              mixed-initiative co-creation tasks",
  author   = "Liapis, Antonios and Yannakakis, Georgios N",
  abstract = "This paper attempts to connect mixed-initiative design with
              established theories of computational creativity, and adapt the
              latter to accommodate a human initiative impacting computationally
              creative processes and outcomes. Research in computational
              creativity often focuses on autonomously creative systems, which
              incorporate creative processes and result in creative outcomes.
              However, the integration of artificially intelligent processes in
              human-computer interaction tools necessitates that we identify how
              computational creativity can be shaped and ultimately enhanced by
              human intervention. This paper attempts to connect
              mixed-initiative design with established theories of computational
              creativity, and adapt the latter to accommodate a human initiative
              impacting computationally creative processes and outcomes. Several
              case studies of mixed-initiative tools for design and play are
              used to corroborate the arguments",
  year     =  2016,
  language = "en"
}

@ARTICLE{Winston2017-nb,
  title    = "Turn-Taking with Improvisational Co-Creative Agents",
  author   = "Winston, Lauren and Magerko, Brian",
  journal  = "AIIDE",
  abstract = "Turn-taking is the ability for agents to lead or follow in social
              interactions. Turn-taking between humans and intelligent agents
              has been studied in human-robot interaction but has not been
              applied to improvisational, dance-based interactions. User
              understanding and experience of turn-taking in an improvisational,
              dance-based system known as LuminAI was investigated in a
              preliminary study of 11 participants. The results showed a trend
              towards users understanding the difference between turn-taking and
              non-turn-taking versions of LuminAI but reduced user experience in
              the turn-taking version.",
  year     =  2017,
  language = "en"
}

@ARTICLE{Kantosalo2019-pz,
  title    = "Human-Computer Co-Creativity : Designing, Evaluating and Modelling
              Computational Collaborators for Poetry Writing",
  author   = "Kantosalo, Anna",
  abstract = "Human-computer co-creativity examines creative collaboration
              between humans and artificially intelligent computational agents.
              Human-computer co-creativity researchers assume that instead of
              using computational systems to merely automate creative tasks,
              computational creativity methods can be leveraged to design
              computational collaborators capable of sharing creative
              responsibility with a human collaborator. This has potential for
              extending both human and computational creative capability. This
              thesis focuses on the case of one human and one computational
              collaborator. More specifically this thesis studies how children
              collaborate with a computational collaborator called the Poetry
              Machine in the linguistically creative task of writing poems. This
              thesis investigates three topics related to human-computer
              co-creativity: The design of human-computer co-creative systems,
              their evaluation and the modelling of human-computer co-creative
              processes. These topics are approached from two perspectives: an
              interaction design perspective and a computational creativity
              perspective. The interaction design perspective provides practical
              methods for the design and evaluation of interactive systems as
              well as methodological frameworks for analysing design practices
              in the field. The computational creativity perspective then again
              provides a theoretical view to the evaluation and modelling of
              human-computer cocreativity. The thesis itself consists of five
              papers.",
  year     =  2019,
  language = "en"
}

@ARTICLE{Wright2020-nt,
  title    = "A Comparative Analysis of Industry Human-{AI} Interaction
              Guidelines",
  author   = "Wright, Austin P and Wang, Zijie J and Park, Haekyu and Guo, G and
              Sperrle, F and El-Assady, Mennatallah and Endert, A and Keim, D
              and Chau, Duen Horng",
  journal  = "ArXiv",
  abstract = "This work has surveyed all of the design guidelines from each of
              these major companies and developed a single, unified structure of
              guidelines, giving developers a centralized reference of AI design
              guidelines. With the recent release of AI interaction guidelines
              from Apple, Google, and Microsoft, there is clearly interest in
              understanding the best practices in human-AI interaction. However,
              industry standards are not determined by a single company, but
              rather by the synthesis of knowledge from the whole community. We
              have surveyed all of the design guidelines from each of these
              major companies and developed a single, unified structure of
              guidelines, giving developers a centralized reference. We have
              then used this framework to compare each of the surveyed companies
              to find differences in areas of emphasis. Finally, we encourage
              people to contribute additional guidelines from other companies,
              academia, or individuals, to provide an open and extensible
              reference of AI design guidelines at this https URL.",
  year     =  2020,
  language = "en"
}

@INPROCEEDINGS{Hodhod2016-mx,
  title     = "Closing the cognitive gap between humans and interactive
               narrative agents using shared mental models",
  author    = "Hodhod, Rania and Magerko, Brian",
  booktitle = "Proceedings of the 21st International Conference on Intelligent
               User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This paper proposes a new formal approach for negotiating shared
               mental models between humans and computational improvisational
               agents (improv agents) based on our sociocognitive studies of
               human improvisers. Negotiation of shared mental models serves as
               a core mechanism for improv agents to co-create stories with each
               other and with human interactors. The model aims to narrow the
               gap between human and machine intelligence by providing AI agents
               that, in the presence of incomplete knowledge about an improv
               scene, can use procedural representations not only to understand
               human parties but also to negotiate their mental models with
               them. The described approach allows flexible modeling of
               ambiguous, non-Boolean knowledge through the use of fuzzy logic
               and situation calculus that allows reasoning under uncertainty in
               a dynamic improvisational setting.",
  month     =  mar,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Llano2022-ti,
  title     = "Explainable Computational Creativity",
  author    = "Llano, Maria Teresa and d'Inverno, Mark and Yee-King, Matthew and
               McCormack, Jon and Ilsar, Alon and Pease, Alison and Colton,
               Simon",
  journal   = "ICCC",
  publisher = "arXiv",
  abstract  = "Human collaboration with systems within the Computational
               Creativity (CC) field is often restricted to shallow
               interactions, where the creative processes, of systems and humans
               alike, are carried out in isolation, without any (or little)
               intervention from the user, and without any discussion about how
               the unfolding decisions are taking place. Fruitful co-creation
               requires a sustained ongoing interaction that can include
               discussions of ideas, comparisons to previous/other works,
               incremental improvements and revisions, etc. For these
               interactions, communication is an intrinsic factor. This means
               giving a voice to CC systems and enabling two-way communication
               channels between them and their users so that they can: explain
               their processes and decisions, support their ideas so that these
               are given serious consideration by their creative collaborators,
               and learn from these discussions to further improve their
               creative processes. For this, we propose a set of design
               principles for CC systems that aim at supporting greater
               co-creation and collaboration with their human collaborators.",
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Eiband2021-vj,
  title     = "How to support users in understanding intelligent systems?
               Structuring the discussion",
  author    = "Eiband, Malin and Buschek, Daniel and Hussmann, Heinrich",
  booktitle = "26th International Conference on Intelligent User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This work reviews the literature in HCI through the lens of
               implied user questions to synthesise a conceptual framework
               integrating user mindsets, user involvement, and knowledge
               outcomes to reveal, differentiate and classify current notions in
               prior work. The opaque nature of many intelligent systems
               violates established usability principles and thus presents a
               challenge for human-computer interaction. Research in the field
               therefore highlights the need for transparency, scrutability,
               intelligibility, interpretability and explainability, among
               others. While all of these terms carry a vision of supporting
               users in understanding intelligent systems, the underlying
               notions and assumptions about users and their interaction with
               the system often remain unclear. We review the literature in HCI
               through the lens of implied user questions to synthesise a
               conceptual framework integrating user mindsets, user involvement,
               and knowledge outcomes to reveal, differentiate and classify
               current notions in prior work. This framework aims to resolve
               conceptual ambiguity in the field and enables researchers to
               clarify their assumptions and become aware of those made in prior
               work. We thus hope to advance and structure the dialogue in the
               HCI research community on supporting users in understanding
               intelligent systems.",
  month     =  apr,
  year      =  2021,
  language  = "en"
}

@INPROCEEDINGS{Wang2021-uy,
  title     = "Towards mutual theory of mind in human-{AI} interaction: How
               language reflects what students perceive about a virtual teaching
               assistant",
  author    = "Wang, Qiaosi and Saha, Koustuv and Gregori, Eric and Joyner,
               David and Goel, Ashok",
  booktitle = "Proceedings of the 2021 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "It is found that students’ perception of Jill Watson’s
               anthropomorphism and intelligence changed significantly over
               time, andRegression analyses reveal that linguistic verbosity,
               readability, sentiment, diversity, and adaptability reflect
               student perception of JW. Building conversational agents that can
               conduct natural and prolonged conversations has been a major
               technical and design challenge, especially for community-facing
               conversational agents. We posit Mutual Theory of Mind as a
               theoretical framework to design for natural long-term human-AI
               interactions. From this perspective, we explore a community’s
               perception of a question-answering conversational agent through
               self-reported surveys and computational linguistic approach in
               the context of online education. We first examine long-term
               temporal changes in students’ perception of Jill Watson (JW), a
               virtual teaching assistant deployed in an online class discussion
               forum. We then explore the feasibility of inferring students’
               perceptions of JW through linguistic features extracted from
               student-JW dialogues. We find that students’ perception of JW’s
               anthropomorphism and intelligence changed significantly over
               time. Regression analyses reveal that linguistic verbosity,
               readability, sentiment, diversity, and adaptability reflect
               student perception of JW. We discuss implications for building
               adaptive community-facing conversational agents as long-term
               companions and designing towards Mutual Theory of Mind in
               human-AI interaction.",
  month     =  may,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Guzdial2019-tz,
  title    = "An Interaction Framework for Studying Co-Creative {AI}",
  author   = "Guzdial, Matthew J and Riedl, Mark O",
  journal  = "ArXiv",
  abstract = "A general framework for turn-based interaction between human users
              and AI agents designed to support human creativity, called
              co-creative systems is proposed and can be used to better
              understand the space of possible designs of co-Creative systems
              and reveal future research directions. Machine learning has been
              applied to a number of creative, design-oriented tasks. However,
              it remains unclear how to best empower human users with these
              machine learning approaches, particularly those users without
              technical expertise. In this paper we propose a general framework
              for turn-based interaction between human users and AI agents
              designed to support human creativity, called {co-creative
              systems}. The framework can be used to better understand the space
              of possible designs of co-creative systems and reveal future
              research directions. We demonstrate how to apply this framework in
              conjunction with a pair of recent human subject studies, comparing
              between the four human-AI systems employed in these studies and
              generating hypotheses towards future studies.",
  year     =  2019,
  language = "en"
}

@ARTICLE{Coenen2021-ym,
  title    = "Wordcraft: a Human-{AI} Collaborative Editor for Story Writing",
  author   = "Coenen, Andy and Davis, Luke and Ippolito, Daphne and Reif, Emily
              and Yuan, Ann",
  journal  = "ArXiv",
  abstract = "Wordcraft, an AI-assisted editor for story writing in which a
              writer and a dialog system collaborate to write a story is
              proposed, which provides a sandbox for writers to probe the
              boundaries of transformer-based language models and paves the way
              for future human-in-the-loop training pipelines and novel
              evaluation methods. As neural language models grow in
              effectiveness, they are increasingly being applied in real-world
              settings. However these applications tend to be limited in the
              modes of interaction they support. In this extended abstract, we
              propose Wordcraft, an AI-assisted editor for story writing in
              which a writer and a dialog system collaborate to write a story.
              Our novel interface uses few-shot learning and the natural
              affordances of conversation to support a variety of interactions.
              Our editor provides a sandbox for writers to probe the boundaries
              of transformer-based language models and paves the way for future
              human-in-the-loop training pipelines and novel evaluation methods.",
  year     =  2021,
  language = "en"
}

@ARTICLE{Karimi2019-io,
  title    = "Deep Learning in a Computational Model for Conceptual Shifts in a
              Co-Creative Design System",
  author   = "Karimi, Pegah and Maher, M and Davis, N and Grace, Kazjon",
  journal  = "ICCC",
  abstract = "The paper presents the results of a user study showing that
              increasing novelty in the AI contribution is associated with
              higher creative outcomes, whereas low novelty leads to less
              creative outcomes. This paper presents a computational model for
              conceptual shifts, based on a novelty metric applied to a vector
              representation generated through deep learning. This model is
              integrated into a co-creative design system, which enables a
              partnership between an AI agent and a human designer interacting
              through a sketching canvas. The AI agent responds to the human
              designer's sketch with a new sketch that is a conceptual shift:
              intentionally varying the visual and conceptual similarity with
              increasingly more novelty. The paper presents the results of a
              user study showing that increasing novelty in the AI contribution
              is associated with higher creative outcomes, whereas low novelty
              leads to less creative outcomes.",
  year     =  2019,
  language = "en"
}

@ARTICLE{Khadpe2020-jf,
  title     = "Conceptual metaphors impact perceptions of human-{AI}
               collaboration",
  author    = "Khadpe, Pranav and Krishna, Ranjay and Fei-Fei, Li and Hancock,
               Jeffrey T and Bernstein, Michael S",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  4,
  number    = "CSCW2",
  pages     = "1--26",
  abstract  = "It is suggested that projecting competence may help attract new
               users, but those users may discard the agent unless it can
               quickly correct with a lower competence metaphor, and that users
               are drawn to systems that project higher competence and warmth.
               With the emergence of conversational artificial intelligence (AI)
               agents, it is important to understand the mechanisms that
               influence users' experiences of these agents. In this paper, we
               study one of the most common tools in the designer's toolkit:
               conceptual metaphors. Metaphors can present an agent as akin to a
               wry teenager, a toddler, or an experienced butler. How might a
               choice of metaphor influence our experience of the AI agent?
               Sampling a set of metaphors along the dimensions of warmth and
               competence---defined by psychological theories as the primary
               axes of variation for human social perception---we perform a
               study $(N=260)$ where we manipulate the metaphor, but not the
               behavior, of a Wizard-of-Oz conversational agent. Following the
               experience, participants are surveyed about their intention to
               use the agent, their desire to cooperate with the agent, and the
               agent's usability. Contrary to the current tendency of designers
               to use high competence metaphors to describe AI products, we find
               that metaphors that signal low competence lead to better
               evaluations of the agent than metaphors that signal high
               competence. This effect persists despite both high and low
               competence agents featuring identical, human-level performance
               and the wizards being blind to condition. A second study confirms
               that intention to adopt decreases rapidly as competence projected
               by the metaphor increases. In a third study, we assess effects of
               metaphor choices on potential users' desire to try out the system
               and find that users are drawn to systems that project higher
               competence and warmth. These results suggest that projecting
               competence may help attract new users, but those users may
               discard the agent unless it can quickly correct with a lower
               competence metaphor. We close with a retrospective analysis that
               finds similar patterns between metaphors and user attitudes
               towards past conversational agents such as Xiaoice, Replika,
               Woebot, Mitsuku, and Tay.",
  month     =  oct,
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Long2019-lw,
  title     = "Designing co-creative {AI} for public spaces",
  author    = "Long, Duri and Jacob, Mikhail and Magerko, Brian",
  booktitle = "Proceedings of the 2019 on Creativity and Cognition",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Artificial intelligence (AI) is becoming increasingly pervasive
               in our everyday lives. There are consequently many common
               misconceptions about what AI is, what it is capable of, and how
               it works. Compounding the issue, opportunities to learn about AI
               are often limited to audiences who already have access to and
               knowledge about technology. Increasing access to AI in public
               spaces has the potential to broaden public AI literacy, and
               experiences involving co-creative (i.e. collaboratively creative)
               AI are particularly well-suited for engaging a broad range of
               participants. This paper explores how to design co-creative AI
               for public interaction spaces, drawing both on existing
               literature and our own experiences designing co-creative AI for
               public venues. It presents a set of design principles that can
               aid others in the development of co-creative AI for public spaces
               as well as guide future research agendas.",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@INPROCEEDINGS{Weisz2021-iu,
  title     = "Perfection not required? Human-{AI} partnerships in code
               translation",
  author    = "Weisz, Justin D and Muller, Michael and Houde, Stephanie and
               Richards, John and Ross, Steven I and Martinez, Fernando and
               Agarwal, Mayank and Talamadupula, Kartik",
  booktitle = "26th International Conference on Intelligent User Interfaces",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This study highlights how UI features such as confidence
               highlighting and alternate translations help software engineers
               work with and better understand generative NMT models. Generative
               models have become adept at producing artifacts such as images,
               videos, and prose at human-like levels of proficiency. New
               generative techniques, such as unsupervised neural machine
               translation (NMT), have recently been applied to the task of
               generating source code, translating it from one programming
               language to another. The artifacts produced in this way may
               contain imperfections, such as compilation or logical errors. We
               examine the extent to which software engineers would tolerate
               such imperfections and explore ways to aid the detection and
               correction of those errors. Using a design scenario approach, we
               interviewed 11 software engineers to understand their reactions
               to the use of an NMT model in the context of application
               modernization, focusing on the task of translating source code
               from one language to another. Our three-stage scenario sparked
               discussions about the utility and desirability of working with an
               imperfect AI system, how acceptance of that system’s outputs
               would be established, and future opportunities for generative AI
               in application modernization. Our study highlights how UI
               features such as confidence highlighting and alternate
               translations help software engineers work with and better
               understand generative NMT models.",
  month     =  apr,
  year      =  2021,
  language  = "en"
}

@INPROCEEDINGS{Huang2020-fh,
  title     = "{AI} song contest: Human-{AI} co-creation in songwriting",
  author    = "Huang, Cheng-Zhi Anna and Koops, Hendrik Vincent and Newton-Rex,
               Ed and Dinculescu, Monica and Cai, Carrie",
  publisher = "Zenodo",
  abstract  = "Machine learning is challenging the way we make music. Although
               research in deep generative models has dramatically improved the
               capability and fluency of music models, recent work has shown
               that it can be challenging for humans to partner with this new
               class of algorithms. In this paper, we present findings on what
               13 musician/developer teams, a total of 61 users, needed when
               co-creating a song with AI, the challenges they faced, and how
               they leveraged and repurposed existing characteristics of AI to
               overcome some of these challenges. Many teams adopted modular
               approaches, such as independently running multiple smaller models
               that align with the musical building blocks of a song, before
               re-combining their results. As ML models are not easily
               steerable, teams also generated massive numbers of samples and
               curated them post-hoc, or used a range of strategies to direct
               the generation or algorithmically ranked the samples. Ultimately,
               teams not only had to manage the ``flare and focus'' aspects of
               the creative process, but also juggle that with a parallel
               process of exploring and curating multiple ML models and outputs.
               These findings reflect a need to design machine learning-powered
               music interfaces that are more decomposable, steerable,
               interpretable, and adaptive, which in return will enable artists
               to more effectively explore how AI can extend their personal
               expression.",
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Liang2019-gy,
  title     = "Implicit Communication of Actionable Information in Human-{AI}
               teams",
  author    = "Liang, Claire and Proft, Julia and Andersen, Erik and Knepper,
               Ross A",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "In a user study with 904 completed games and 246 completed
               surveys, human players randomly paired with an implicature AI are
               71\% more likely to think their partner is human than players
               paired with a non-implicatureAI. Humans expect their
               collaborators to look beyond the explicit interpretation of their
               words. Implicature is a common form of implicit communication
               that arises in natural language discourse when an utterance
               leverages context to imply information beyond what the words
               literally convey. Whereas computational methods have been
               proposed for interpreting and using different forms of
               implicature, its role in human and artificial agent collaboration
               has not yet been explored in a concrete domain. The results of
               this paper provide insights to how artificial agents should be
               structured to facilitate natural and efficient communication of
               actionable information with humans. We investigated implicature
               by implementing two strategies for playing Hanabi, a cooperative
               card game that relies heavily on communication of actionable
               implicit information to achieve a shared goal. In a user study
               with 904 completed games and 246 completed surveys, human players
               randomly paired with an implicature AI are 71\% more likely to
               think their partner is human than players paired with a
               non-implicature AI. These teams demonstrated game performance
               similar to other state of the art approaches.",
  month     =  may,
  year      =  2019,
  language  = "en"
}

@INPROCEEDINGS{McCormack2019-yh,
  title     = "In a silent way",
  author    = "McCormack, Jon and Gifford, Toby and Hutchings, Patrick and Llano
               Rodriguez, Maria Teresa and Yee-King, Matthew and d'Inverno, Mark",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "A collaborative improvising AI drummer is developed that
               communicates its confidence through an emoticon-based
               visualisation that shows a positive correlation between
               extra-musical communication of machine internal state and human
               musical engagement. Collaboration is built on trust, and
               establishing trust with a creative Artificial Intelligence is
               difficult when the decision process or internal state driving its
               behaviour isn't exposed. When human musicians improvise together,
               a number of extra-musical cues are used to augment musical
               communication and expose mental or emotional states which affect
               musical decisions and the effectiveness of the collaboration. We
               developed a collaborative improvising AI drummer that
               communicates its confidence through an emoticon-based
               visualisation. The AI was trained on musical performance data, as
               well as real-time skin conductance, of musicians improvising with
               professional drummers, exposing both musical and extra-musical
               cues to inform its generative process. Uni- and bi-directional
               extra-musical communication with real and false values were
               tested by experienced improvising musicians. Each condition was
               evaluated using the FSS-2 questionnaire, as a proxy for musical
               engagement. The results show a positive correlation between
               extra-musical communication of machine internal state and human
               musical engagement.",
  month     =  may,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Kantosalo2016-de,
  title    = "Modes for Creative Human-Computer Collaboration: Alternating and
              Task-Divided Co-Creativity",
  author   = "Kantosalo, Anna and Toivonen, Hannu (tt)",
  journal  = "ICCC",
  abstract = "Different styles of human- computer co-creation from a more
              computational perspective are examined, presenting new concepts
              for analysis of computational agents in human-computer cocreation.
              The analysis of human-computer co-creative systems in current
              literature is focused on a human perspective, highlighting the
              benefits of co-creative systems for human users. This study paper
              examines different styles of human-computer co-creation from a
              more computational perspective, presenting new concepts for
              analysis of computational agents in human-computer cocreation. Our
              perspective is based on Wiggins’ formalization of creativity as a
              search. We formalize for co-creative scenarios involving an
              alternating, iterative approach to co-creation, which we call
              alternating cocreativity and briefly discuss its non-alternating
              counterpart, task-divided co-creativity. With focus on alternating
              co-creativity, we analyze the co-creative process and discuss new
              modes and roles for the creative agents within it. Finally, we
              illustrate our theoretical findings in the context of current
              co-creative systems and discuss their relation to the roles and
              expectations presented in",
  year     =  2016,
  language = "en"
}

@INPROCEEDINGS{Deterding2017-wh,
  title     = "Mixed-Initiative Creative Interfaces",
  author    = "Deterding, Sebastian and Hook, Jonathan and Fiebrink, Rebecca and
               Gillies, Marco and Gow, Jeremy and Akten, Memo and Smith, Gillian
               and Liapis, Antonios and Compton, Kate",
  booktitle = "Proceedings of the 2017 CHI Conference Extended Abstracts on
               Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Enabled by artificial intelligence techniques, we are witnessing
               the rise of a new paradigm of computational creativity support:
               mixed-initiative creative interfaces put human and computer in a
               tight interactive loop where each suggests, produces, evaluates,
               modifies, and selects creative outputs in response to the other.
               This paradigm could broaden and amplify creative capacity for
               all, but has so far remained mostly confined to artificial
               intelligence for game content generation, and faces many unsolved
               interaction design challenges. This workshop therefore convenes
               CHI and game researchers to advance mixed-initiative approaches
               to creativity support.",
  month     =  may,
  year      =  2017,
  language  = "en"
}

@INPROCEEDINGS{Louie2020-aq,
  title     = "Novice-{AI} music co-creation via {AI}-steering tools for deep
               generative models",
  author    = "Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry,
               Michael and Cai, Carrie J",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "AI-steering tools were developed that increased users' trust,
               control, comprehension, and sense of collaboration with the AI,
               but also contributed to a greater sense of self-efficacy and
               ownership of the composition relative to the AI. While generative
               deep neural networks (DNNs) have demonstrated their capacity for
               creating novel musical compositions, less attention has been paid
               to the challenges and potential of co-creating with these musical
               AIs, especially for novices. In a needfinding study with a widely
               used, interactive musical AI, we found that the AI can overwhelm
               users with the amount of musical content it generates, and
               frustrate them with its non-deterministic output. To better match
               co-creation needs, we developed AI-steering tools, consisting of
               Voice Lanes that restrict content generation to particular
               voices; Example-Based Sliders to control the similarity of
               generated content to an existing example; Semantic Sliders to
               nudge music generation in high-level directions (happy/sad,
               conventional/surprising); and Multiple Alternatives of generated
               content to audition and choose from. In a summative study (N=21),
               we discovered the tools not only increased users' trust, control,
               comprehension, and sense of collaboration with the AI, but also
               contributed to a greater sense of self-efficacy and ownership of
               the composition relative to the AI.",
  month     =  apr,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Liapis2016-bv,
  title    = "Can Computers Foster Human Users’ Creativity? Theory and Praxis of
              Mixed-Initiative Co-Creativity",
  author   = "Liapis, Antonios and Yannakakis, Georgios N and Alexopoulos,
              Constantine and Lopes, P",
  abstract = "This article discusses the impact of artificially intelligent
              computers to the process of design, play and educational
              activities. A computational process which has the necessary
              intelligence and creativity to take a proactive role in such
              activities can not only support human creativity but also foster
              it and prompt lateral thinking. The argument is made both from the
              perspective of human creativity, where the computational input is
              treated as an external stimulus which triggers re-framing of
              humans’ routines and mental associations, but also from the
              perspective of computational creativity where human input and
              initiative constrains the search space of the algorithm, enabling
              it to focus on specific possible solutions to a problem rather
              than globally search for the optimal. The article reviews four
              mixed-initiative tools (for design and educational play) based on
              how they contribute to human-machine co-creativity. These
              paradigms serve different purposes, afford different human
              interaction methods and incorporate different computationally
              creative processes. Assessing how co-creativity is facilitated on
              a per-paradigm basis strengthens the theoretical argument and
              provides an initial seed for future work in the burgeoning domain
              of mixed-initiative interaction.",
  year     =  2016,
  language = "en"
}

@INPROCEEDINGS{Dellermann2019-ze,
  title     = "The future of human-{AI} collaboration: A taxonomy of design
               knowledge for hybrid intelligence systems",
  author    = "Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and
               Weber, Thorsten and Weigel, Sascha and Ebel, Philipp",
  booktitle = "Proceedings of the 52nd Hawaii International Conference on System
               Sciences",
  publisher = "Hawaii International Conference on System Sciences",
  abstract  = "Recent technological advances, especially in the field of machine
               learning, provide astonishing progress on the road towards
               artificial general intelligence. However, tasks in current
               real-world business applications cannot yet be solved by machines
               alone. We, therefore, identify the need for developing
               socio-technological ensembles of humans and machines. Such
               systems possess the ability to accomplish complex goals by
               combining human and artificial intelligence to collectively
               achieve superior results and continuously improve by learning
               from each other. Thus, the need for structured design knowledge
               for those systems arises. Following a taxonomy development
               method, this article provides three main contributions: First, we
               present a structured overview of interdisciplinary research on
               the role of humans in the machine learning pipeline. Second, we
               envision hybrid intelligence systems and conceptualize the
               relevant dimensions for system design for the first time.
               Finally, we offer useful guidance for system developers during
               the implementation of such applications.",
  year      =  2019,
  language  = "en"
}

@INPROCEEDINGS{Oh2018-mu,
  title     = "{I} lead, you help but only with enough details",
  author    = "Oh, Changhoon and Song, Jungwoo and Choi, Jinhan and Kim,
               Seonghyeon and Lee, Sungwoo and Suh, Bongwon",
  booktitle = "Proceedings of the 2018 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "A prototype, DuetDraw, an AI interface that allows users and the
               AI agent to draw pictures collaboratively and implications for
               user interfaces where users can collaborate with AI in creative
               works are discussed. Recent advances in artificial intelligence
               (AI) have increased the opportunities for users to interact with
               the technology. Now, users can even collaborate with AI in
               creative activities such as art. To understand the user
               experience in this new user--AI collaboration, we designed a
               prototype, DuetDraw, an AI interface that allows users and the AI
               agent to draw pictures collaboratively. We conducted a user study
               employing both quantitative and qualitative methods. Thirty
               participants performed a series of drawing tasks with the
               think-aloud method, followed by post-hoc surveys and interviews.
               Our findings are as follows: (1) Users were significantly more
               content with DuetDraw when the tool gave detailed instructions.
               (2) While users always wanted to lead the task, they also wanted
               the AI to explain its intentions but only when the users wanted
               it to do so. (3) Although users rated the AI relatively low in
               predictability, controllability, and comprehensibility, they
               enjoyed their interactions with it during the task. Based on
               these findings, we discuss implications for user interfaces where
               users can collaborate with AI in creative works.",
  month     =  apr,
  year      =  2018,
  language  = "en"
}

@INPROCEEDINGS{Zhu2018-zd,
  title     = "Explainable {AI} for designers: A human-centered perspective on
               mixed-initiative co-creation",
  author    = "Zhu, Jichen and Liapis, Antonios and Risi, Sebastian and Bidarra,
               Rafael and Youngblood, G Michael",
  booktitle = "2018 IEEE Conference on Computational Intelligence and Games
               (CIG)",
  publisher = "IEEE",
  abstract  = "This vision paper proposes a new research area of eXplainable AI
               for Designers (XAID), specifically for game designers, and
               illustrates the initial XAID framework through three use cases,
               which require an understanding both of the innate properties of
               the AI techniques and users’ needs. Growing interest in
               eXplainable Artificial Intelligence (XAI) aims to make AI and
               machine learning more understandable to human users. However,
               most existing work focuses on new algorithms, and not on
               usability, practical interpretability and efficacy on real users.
               In this vision paper, we propose a new research area of
               eXplainable AI for Designers (XAID), specifically for game
               designers. By focusing on a specific user group, their needs and
               tasks, we propose a human-centered approach for facilitating game
               designers to co-create with AI/ML techniques through XAID. We
               illustrate our initial XAID framework through three use cases,
               which require an understanding both of the innate properties of
               the AI techniques and users’ needs, and we identify key open
               challenges.",
  month     =  aug,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Bansal2019-mp,
  title    = "Beyond Accuracy: The Role of Mental Models in Human-{AI} Team
              Performance",
  author   = "Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Lasecki,
              Walter S and Weld, Daniel S and Horvitz, E",
  journal  = "AAAI 2019",
  abstract = "This work highlights two key properties of an AI’s error boundary,
              parsimony and stochasticity, and a property of the task,
              dimensionality, and shows experimentally how these properties
              affect humans’ mental models of AI capabilities and the resulting
              team performance. Decisions made by human-AI teams (e.g.,
              AI-advised humans) are increasingly common in high-stakes domains
              such as healthcare, criminal justice, and finance. Achieving high
              team performance depends on more than just the accuracy of the AI
              system: Since the human and the AI may have different expertise,
              the highest team performance is often reached when they both know
              how and when to complement one another. We focus on a factor that
              is crucial to supporting such complementary: the human’s mental
              model of the AI capabilities, specifically the AI system’s error
              boundary (i.e. knowing “When does the AI err?”). Awareness of this
              lets the human decide when to accept or override the AI’s
              recommendation. We highlight two key properties of an AI’s error
              boundary, parsimony and stochasticity, and a property of the task,
              dimensionality. We show experimentally how these properties affect
              humans’ mental models of AI capabilities and the resulting team
              performance. We connect our evaluations to related work and
              propose goals, beyond accuracy, that merit consideration during
              model selection and optimization to improve overall human-AI team
              performance.",
  year     =  2019,
  language = "en"
}

@INPROCEEDINGS{Amershi2019-vy,
  title     = "Guidelines for Human-{AI} Interaction",
  author    = "Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and
               Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh,
               Jina and Iqbal, Shamsi and Bennett, Paul N and Inkpen, Kori and
               Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "This work proposes 18 generally applicable design guidelines for
               human-AI interaction that can serve as a resource to
               practitioners working on the design of applications and features
               that harness AI technologies, and to researchers interested in
               the further development of human- AI interaction design
               principles. Advances in artificial intelligence (AI) frame
               opportunities and challenges for user interface design.
               Principles for human-AI interaction have been discussed in the
               human-computer interaction community for over two decades, but
               more study and innovation are needed in light of advances in AI
               and the growing uses of AI technologies in human-facing
               applications. We propose 18 generally applicable design
               guidelines for human-AI interaction. These guidelines are
               validated through multiple rounds of evaluation including a user
               study with 49 design practitioners who tested the guidelines
               against 20 popular AI-infused products. The results verify the
               relevance of the guidelines over a spectrum of interaction
               scenarios and reveal gaps in our knowledge, highlighting
               opportunities for further research. Based on the evaluations, we
               believe the set of design guidelines can serve as a resource to
               practitioners working on the design of applications and features
               that harness AI technologies, and to researchers interested in
               the further development of human-AI interaction design
               principles.",
  month     =  may,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Amershi2014-ro,
  title     = "Power to the people: The role of humans in interactive machine
               learning",
  author    = "Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and
               Kulesza, Todd",
  journal   = "AI Mag.",
  publisher = "Wiley",
  volume    =  35,
  number    =  4,
  pages     = "105--120",
  abstract  = "Intelligent systems that learn interactively from their end-users
               are quickly becoming widespread. Until recently, this progress
               has been fueled mostly by advances in machine learning; however,
               more and more researchers are realizing the importance of
               studying users of these systems. In this article we promote this
               approach and demonstrate how it can result in better user
               experiences and more effective learning systems. We present a
               number of case studies that characterize the impact of
               interactivity, demonstrate ways in which some existing systems
               fail to account for the user, and explore new ways for learning
               systems to interact with their users. We argue that the design
               process for interactive machine learning systems should involve
               users at all stages: explorations that reveal human interaction
               patterns and inspire novel interaction methods, as well as
               refinement stages to tune details of the interface and choose
               among alternatives. After giving a glimpse of the progress that
               has been made so far, we discuss the challenges that we face in
               moving the field forward.",
  month     =  dec,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Anantrasirichai2022-vy,
  title     = "Artificial intelligence in the creative industries: a review",
  author    = "Anantrasirichai, Nantheera and Bull, David",
  journal   = "Artif. Intell. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  55,
  number    =  1,
  pages     = "589--656",
  abstract  = "AbstractThis paper reviews the current state of the art in
               artificial intelligence (AI) technologies and applications in the
               context of the creative industries. A brief background of AI, and
               specifically machine learning (ML) algorithms, is provided
               including convolutional neural networks (CNNs), generative
               adversarial networks (GANs), recurrent neural networks (RNNs) and
               deep Reinforcement Learning (DRL). We categorize creative
               applications into five groups, related to how AI technologies are
               used: (i) content creation, (ii) information analysis, (iii)
               content enhancement and post production workflows, (iv)
               information extraction and enhancement, and (v) data compression.
               We critically examine the successes and limitations of this
               rapidly advancing technology in each of these areas. We further
               differentiate between the use of AI as a creative tool and its
               potential as a creator in its own right. We foresee that, in the
               near future, ML-based AI will be adopted widely as a tool or
               collaborative assistant for creativity. In contrast, we observe
               that the successes of ML in domains with fewer constraints, where
               AI is the ‘creator’, remain modest. The potential of AI (or its
               developers) to win awards for its original creations in
               competition with human creatives is also limited, based on
               contemporary technologies. We therefore conclude that, in the
               context of creative industries, maximum benefit from AI will be
               derived where its focus is human-centric—where it is designed to
               augment, rather than replace, human creativity.",
  month     =  jan,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Kaur2019-dp,
  title    = "Building Shared Mental Models between Humans and {AI} for
              Effective Collaboration",
  author   = "Kaur, Harmanpreet",
  abstract = "This paper proposes methods for building shared mental models
              between humans and AI to enable human-AI collaboration at a level
              where both can be equal partners working on a shared task.
              Intelligent systems have become increasingly common in settings
              ranging from performing everyday tasks more easily to
              decision-making for complex domains (e.g., healthcare, autonomous
              driving, criminal justice). Given this rising ubiquity of
              artificial intelligence (AI), both researchers and industry
              practitioners are exploring ways to better integrate AI agents in
              tasks that people do at home or work. However, these systems are
              currently limited because of gaps in the understanding between
              humans and their AI counterparts. In this paper, we propose
              methods for building shared mental models between humans and AI to
              enable human-AI collaboration at a level where both can be equal
              partners working on a shared task. We ground our approach in
              existing literature from CSCW and UX design.",
  year     =  2019,
  language = "en"
}

@MISC{Design_Theory2022-rw,
  title     = "Will Artificial Intelligence End Human Creativity?",
  author    = "{Design Theory}",
  publisher = "Youtube",
  abstract  = "You and your business can try Onshape for free at
               https://Onshape.pro/DesignTheory . With recent advancements in
               Artificial Intelligence design tools, we are...",
  month     =  jun,
  year      =  2022
}

@ARTICLE{Makela2018-mj,
  title     = "Documentation as a practice-led research tool for reflection on
               experiential knowledge",
  author    = "Mäkelä, Maarit and Nimkulrat, Nithikul",
  journal   = "FORMakademisk",
  publisher = "OsloMet - Oslo Metropolitan University",
  volume    =  11,
  number    =  2,
  abstract  = "Practice-led research has been under debate for three decades.
               One of its major issues concerns how the researcher who is also
               the practitioner documents and reflects on her creative process
               in relation to a research topic. This article reviews and
               discusses documentation and reflection in practice-led research
               through three cases of doctoral dissertations that were completed
               at Aalto University in Finland. Through the cases the article
               examines the role the documentation and reflection of creative
               processes and products in these studies. In conclusion,
               documentation in the practice-led research context functions as
               conscious reflection on and in action. Any means of
               documentation, for example diary writing, photographing, or
               sketching, can serve as a mode of reflection.",
  month     =  may,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Csikszentmihalyi_undated-ct,
  title   = "Flow and the psychology of discovery and invention",
  author  = "{Csikszentmihalyi}",
  journal = "HarperPerennial, New York"
}

@INCOLLECTION{Zimmerman2014-qa,
  title     = "Research Through Design in {HCI}",
  author    = "Zimmerman, John and Forlizzi, Jodi",
  editor    = "Olson, Judith S and Kellogg, Wendy A",
  booktitle = "Ways of Knowing in HCI",
  publisher = "Springer New York",
  address   = "New York, NY",
  pages     = "167--189",
  abstract  = "In Research through Design (RtD), researchers generate new
               knowledge by understanding the current state and then suggesting
               an improved future state in the form of a design. It involves
               deep reflection in iteratively understanding the people, problem,
               and context around a situation that researchers feel they can
               improve.",
  year      =  2014
}

@ARTICLE{Candy2006-qh,
  title   = "Practice based research: A guide",
  author  = "Candy, Linda",
  journal = "CCS report",
  volume  =  1,
  number  =  2,
  pages   = "1--19",
  year    =  2006
}

@BOOK{Karwowski2017-jp,
  title     = "The Creative Self: Effect of Beliefs, Self-Efficacy, Mindset, and
               Identity",
  author    = "Karwowski, Maciej and Kaufman, James C",
  publisher = "Academic Press",
  abstract  = "The Creative Self reviews and summarizes key theories, studies,
               and new ideas about the role and significance self-beliefs play
               in one’s creativity. It untangles the interrelated constructs of
               creative self-efficacy, creative metacognition, creative
               identity, and creative self-concept. It explores how and when
               creative self-beliefs are formed as well as how creative
               self-beliefs can be strengthened. Part I discusses how creativity
               plays a part in one’s self-identity and its relationship with
               free will and efficacy. Part II discusses creativity present in
               day-to-day life across the lifespan. Part III highlights the
               intersection of the creative self with other variables such as
               mindset, domains, the brain, and individual differences. Part IV
               explores methodology and culture in relation to creativity. Part
               V, discusses additional constructs or theories that offer promise
               for future research on creativity. Explores how beliefs about
               one’s creativity are part of one’s identity Investigates the
               development of self-beliefs about creativity Identifies external
               and personality factors influencing self-beliefs about creativity
               Incorporates worldwide research with cross-disciplinary
               contributors",
  month     =  feb,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Jansson1991-wy,
  title    = "Design fixation",
  author   = "Jansson, David G and Smith, Steven M",
  journal  = "Design Studies",
  volume   =  12,
  number   =  1,
  pages    = "3--11",
  abstract = "This paper reports on a series of experiments which were conducted
              to test the hypothesis that design fixation, defined as a blind
              adherence to a set of ideas or concepts limiting the output of
              conceptual design, is a measurable barrier in the conceptual
              design process. The results of the experiments clearly demonstrate
              the existence of design fixation. The paper related issues such as
              the nature of the phenomenon, some experimental issues which arise
              in such investigations, and directions for future research.",
  month    =  jan,
  year     =  1991,
  keywords = "conceptual design; engineering design; creativity"
}

@ARTICLE{Axelrod1981-ux,
  title    = "The evolution of cooperation",
  author   = "Axelrod, R and Hamilton, W D",
  journal  = "Science",
  volume   =  211,
  number   =  4489,
  pages    = "1390--1396",
  abstract = "Cooperation in organisms, whether bacteria or primates, has been a
              difficulty for evolutionary theory since Darwin. On the assumption
              that interactions between pairs of individuals occur on a
              probabilistic basis, a model is developed based on the concept of
              an evolutionarily stable strategy in the context of the Prisoner's
              Dilemma game. Deductions from the model, and the results of a
              computer tournament show how cooperation based on reciprocity can
              get started in an asocial world, can thrive while interacting with
              a wide range of other strategies, and can resist invasion once
              fully established. Potential applications include specific aspects
              of territoriality, mating, and disease.",
  month    =  mar,
  year     =  1981,
  language = "en"
}

@ARTICLE{Bengler2012-jf,
  title     = "Interaction Principles for Cooperative Human-Machine Systems",
  author    = "Bengler, Klaus and Zimmermann, Markus and Bortot, Dino and
               Kienle, Martin and Damböck, Daniel",
  journal   = "it - Information Technology",
  publisher = "Oldenbourg Verlag",
  volume    =  54,
  number    =  4,
  pages     = "157--164",
  abstract  = "Human-machine systems with shared authority can be observed in
               different domains of assistance systems. This article creates a
               taxonomy of the most important aspects of human-machine
               cooperation in five layers: intention, modes of cooperation,
               allocation, interfaces and contact. This is investigated with
               help of driver assistance and Human-Robot Interaction.
               Furthermore, a perspective for possibilities of cross-domain
               generalization is given.",
  month     =  aug,
  year      =  2012
}

@INPROCEEDINGS{Kruger2017-xa,
  title     = "From Tools Towards Cooperative Assistants",
  author    = "Krüger, Matti and Wiebel, Christiane B and Wersing, Heiko",
  booktitle = "Proceedings of the 5th International Conference on Human Agent
               Interaction",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "287--294",
  abstract  = "Endowing assistant systems with more autonomy establishes the
               transition from a human-controlled tool towards a self-directed
               agent capable of own decisions and goals. In this concept paper
               we suggest to perform the design of such an assistant agent
               according to principles of cooperativity. We first review
               definitions of cooperation between animals, humans and machines
               and then discuss advantages of cooperation also for a
               human-machine interaction system. We concentrate on the important
               roles of adaptivity and responsibility within the interaction. We
               argue that main benefits of a cooperative design are alleviation
               of typical automation issues like controllability, complacency,
               trust, and greater flexibility of the combined human-machine
               system in tasks with high variability.",
  series    = "HAI '17",
  month     =  oct,
  year      =  2017,
  keywords  = "adaptivity, autonomy, assistant systems, cooperation,
               human-machine interaction"
}

@BOOK{Preece2015-ts,
  title     = "Interaction Design - Beyond Human-computer Interaction {4E}",
  author    = "Preece, Jennifer and Sharp, Helen and Rogers, Yvonne",
  publisher = "Wiley",
  edition   =  4,
  month     =  feb,
  year      =  2015,
  language  = "en"
}

@INPROCEEDINGS{Hewett2005-ff,
  title     = "Creativity support tool evaluation methods and metrics",
  author    = "{Hewett} and {Czerwinski} and {Terry} and {others}",
  booktitle = "NSF Workshop Report Creativity Support Tools",
  year      =  2005
}

@ARTICLE{Resnick2005-fs,
  title     = "Design Principles for Tools to Support Creative Thinking",
  author    = "Resnick, Mitchel and Myers, Brad and Nakakoji, Kumiyo and
               Shneiderman, Ben and Eisenberg, Mike",
  publisher = "unknown",
  volume    =  20,
  number    =  2,
  abstract  = "PDF | On Jan 1, 2005, Mitchel Resnick and others published Design
               Principles for Tools to Support Creative Thinking | Find, read
               and cite all the research you need on ResearchGate",
  month     =  jan,
  year      =  2005
}

@ARTICLE{Csikszentmihalyi1990-hu,
  title     = "Flow: the psychology of optimal experience",
  author    = "Csikszentmihalyi, M",
  journal   = "Choice",
  publisher = "American Library Association",
  volume    =  28,
  number    =  01,
  pages     = "28--0597--28--0597",
  abstract  = "More than anything else, men and women seek happiness. Aristotle",
  month     =  sep,
  year      =  1990
}

@ARTICLE{Cherry2014-ty,
  title     = "Quantifying the Creativity Support of Digital Tools through the
               Creativity Support Index",
  author    = "Cherry, Erin and Latulipe, Celine",
  journal   = "ACM Trans. Comput.-Hum. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  21,
  number    =  4,
  pages     = "1--25",
  abstract  = "Creativity support tools help people engage creatively with the
               world, but measuring how well a tool supports creativity is
               challenging since creativity is ill-defined. To this end, we
               developed the Creativity Support Index (CSI), which is a
               psychometric survey designed for evaluating the ability of a
               creativity support tool to assist a user engaged in creative
               work. The CSI measures six dimensions of creativity support:
               Exploration, Expressiveness, Immersion, Enjoyment, Results Worth
               Effort, and Collaboration. The CSI allows researchers to
               understand not just how well a tool supports creative work
               overall, but what aspects of creativity support may need
               attention. In this article, we present the CSI, along with
               scenarios for how it can be deployed in a variety of HCI research
               settings and how the CSI scores can help target design
               improvements. We also present the iterative, rigorous development
               and validation process used to create the CSI.",
  month     =  jun,
  year      =  2014,
  keywords  = "psychometrics, evaluation, Creativity support tools, surveys"
}

@INPROCEEDINGS{Gaver2012-pd,
  title     = "What should we expect from research through design?",
  author    = "Gaver, William",
  booktitle = "Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "937--946",
  abstract  = "In this essay, I explore several facets of research through
               design in order to contribute to discussions about how the
               approach should develop. The essay has three parts. In the first,
               I review two influential theories from the Philosophy of Science
               to help reflect on the nature of design theory, concluding that
               research through design is likely to produce theories that are
               provisional, contingent, and aspirational. In the second part, I
               discuss three possible interpretations for the diversity of
               approaches to research through design, and suggest that this
               variation need not be seen as a sign of inadequate standards or a
               lack of cumulative progress in the field, but may be natural for
               a generative endeavour. In the final section, I suggest that,
               rather than aiming to develop increasingly comprehensive theories
               of design, practice based research might better view theory as
               annotation of realised design examples, and particularly
               portfolios of related pieces. Overall, I suggest that the design
               research community should be wary of impulses towards convergence
               and standardisation, and instead take pride in its aptitude for
               exploring and speculating, particularising and diversifying, and
               - especially - its ability to manifest the results in the form of
               new, conceptually rich artefacts.",
  series    = "CHI '12",
  month     =  may,
  year      =  2012,
  keywords  = "annotation, theory, research through design, portfolios,
               philosophy of science"
}

@INPROCEEDINGS{Zimmerman2007-zv,
  title     = "Research through design as a method for interaction design
               research in {HCI}",
  author    = "Zimmerman, John and Forlizzi, Jodi and Evenson, Shelley",
  booktitle = "Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "493--502",
  abstract  = "For years the HCI community has struggled to integrate design in
               research and practice. While design has gained a strong foothold
               in practice, it has had much less impact on the HCI research
               community. In this paper we propose a new model for interaction
               design research within HCI. Following a research through design
               approach, designers produce novel integrations of HCI research in
               an attempt to make the right thing: a product that transforms the
               world from its current state to a preferred state. This model
               allows interaction designers to make research contributions based
               on their strength in addressing under-constrained problems. To
               formalize this model, we provide a set of four lenses for
               evaluating the research contribution and a set of three examples
               to illustrate the benefits of this type of research.",
  series    = "CHI '07",
  month     =  apr,
  year      =  2007,
  keywords  = "wicked problems, HCI research, design, design method, interaction
               design research, interaction design, design theory, research
               through design"
}

@ARTICLE{Norman2010-sh,
  title     = "Natural user interfaces are not natural",
  author    = "Norman, Donald A",
  journal   = "Interactions",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  17,
  number    =  3,
  pages     = "6--10",
  abstract  = "``I believe we will look back on 2010 as the year we expanded
               beyond the mouse and keyboard and started incorporating more
               natural forms of interaction such as touch, speech,...",
  month     =  may,
  year      =  2010,
  language  = "en"
}

@BOOK{Boden2003-hk,
  title     = "The creative mind",
  author    = "Boden, Margaret A",
  publisher = "Routledge",
  address   = "London, England",
  edition   =  2,
  abstract  = "How is it possible to think new thoughts? What is creativity and
               can science explain it? And just how did Coleridge dream up the
               creatures of The Ancient Mariner? When The Creative Mind: Myths
               and Mechanisms was first published, Margaret A. Boden's bold and
               provocative exploration of creativity broke new ground. Boden
               uses examples such as jazz improvisation, chess, story writing,
               physics, and the music of Mozart, together with computing models
               from the field of artificial intelligence to uncove",
  month     =  sep,
  year      =  2003,
  language  = "en"
}

@BOOK{Malafouris2013-by,
  title     = "How things shape the mind",
  author    = "Malafouris, Lambros",
  publisher = "The MIT Press",
  abstract  = "An account of the different ways in which things have become
               cognitive extensions of the human body, from prehistory to the
               present.An increasingly influential",
  month     =  jul,
  year      =  2013,
  language  = "en"
}

@BOOK{Moggridge2007-sp,
  title     = "Designing interactions",
  author    = "Moggridge, Bill",
  publisher = "MIT Press",
  address   = "Cambridge, Mass.",
  year      =  2007,
  keywords  = "Human-computer interaction"
}

@MISC{Harvard_Advanced_Leadership_Initiative2021-fx,
  title     = "Human-{AI} interaction: From artificial intelligence to human
               intelligence augmentation",
  author    = "{Harvard Advanced Leadership Initiative}",
  publisher = "Youtube",
  abstract  = "Elena Glassman, Harvard SEAS, discusses how automation and
               artificial intelligence produces data that still needs to be
               interpreted by humans.",
  month     =  jun,
  year      =  2021
}

@INPROCEEDINGS{Wang2020-cw,
  title     = "From Human-Human Collaboration to Human-{AI} Collaboration:
               Designing {AI} Systems That Can Work Together with People",
  author    = "Wang, Dakuo and Churchill, Elizabeth and Maes, Pattie and Fan,
               Xiangmin and Shneiderman, Ben and Shi, Yuanchun and Wang,
               Qianying",
  booktitle = "Extended Abstracts of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--6",
  abstract  = "Artificial Intelligent (AI) and Machine Learning (ML) algorithms
               are coming out of research labs into the real-world applications,
               and recent research has focused a lot on Human-AI Interaction
               (HAI) and Explainable AI (XAI). However, Interaction is not the
               same as Collaboration. Collaboration involves mutual goal
               understanding, preemptive task co-management and shared progress
               tracking. Most of human activities today are done
               collaboratively, thus, to integrate AI into the
               already-complicated human workflow, it is critical to bring the
               Computer-Supported Cooperative Work (CSCW) perspective into the
               root of the algorithmic research and plan for a Human-AI
               Collaboration future of work. In this panel we ask: Can this
               future for trusted human-AI collaboration be realized? If so,
               what will it take? This panel will bring together HCI experts who
               work on human collaboration and AI applications in various
               application contexts, from industry and academia and from both
               the U.S. and China. Panelists will engage the audience through
               discussion of their shared and diverging visions, and through
               suggestions for opportunities and challenges for the future of
               human-AI collaboration.",
  series    = "CHI EA '20",
  month     =  apr,
  year      =  2020,
  keywords  = "ai-powered healthcare, ai partner, human-ai collaboration,
               explainable ai, group collaboration, trusted ai,
               computer-supported corporative work"
}

@ARTICLE{Braun2012-te,
  title     = "Thematic analysis",
  author    = "Braun, Virginia and Clarke, Victoria",
  publisher = "American Psychological Association",
  abstract  = "… then demonstrates how to do thematic analysis, using a … We
               conclude by discussing how to conduct thematic analysis … Our
               worked example of thematic analysis uses data from four …",
  year      =  2012
}

@ARTICLE{Authors_undated-fk,
  title     = "Why or Why Not: Barriers of Adopting Generative {AI} in
               Human-{AI} Co-Creativity",
  author    = "Author(s), Anonymous",
  journal   = "Woodstock '18: ACM Symposium on Neural Gaze Detection, June
               03â•ﬁ05, 2018, Woodstock, NY",
  publisher = "Association for Computing Machinery",
  volume    =  1,
  number    =  1
}

@ARTICLE{Ramesh2022-un,
  title     = "Hierarchical text-conditional image generation with clip latents",
  author    = "Ramesh, A and Dhariwal, P and Nichol, A and Chu, C and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Contrastive models like CLIP have been shown to learn robust
               representations of images that capture both semantics and style.
               To leverage these representations for image …",
  year      =  2022
}

@MISC{Apple_Inc_undated-pi,
  title        = "Human Interface Guidelines",
  author       = "{Apple Inc}",
  abstract     = "Get in-depth information and UI resources for designing great
                  apps that integrate seamlessly with Apple platforms.",
  howpublished = "\url{https://developer.apple.com/design/human-interface-guidelines/}",
  note         = "Accessed: 2022-4-30"
}

@MISC{Fein2022-ct,
  title        = "The Language Interface",
  author       = "Fein, Daniel",
  booktitle    = "Medium",
  abstract     = "He ultimately came up with “language interface,” at least for
                  now. This is more than just a one-off thought by Altman,
                  though. In recent weeks, we have seen many of the most
                  respected people in the…",
  month        =  apr,
  year         =  2022,
  howpublished = "\url{https://medium.com/@drfein/the-language-interface-c8096393383f}",
  note         = "Accessed: 2022-4-30",
  language     = "en"
}

@ARTICLE{Ouyang2022-af,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex
                   and Schulman, John and Hilton, Jacob and Kelton, Fraser and
                   Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                   Ryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this paper,
                   we show an avenue for aligning language models with user
                   intent on a wide range of tasks by fine-tuning with human
                   feedback. Starting with a set of labeler-written prompts and
                   prompts submitted through the OpenAI API, we collect a
                   dataset of labeler demonstrations of the desired model
                   behavior, which we use to fine-tune GPT-3 using supervised
                   learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised
                   model using reinforcement learning from human feedback. We
                   call the resulting models InstructGPT. In human evaluations
                   on our prompt distribution, outputs from the 1.3B parameter
                   InstructGPT model are preferred to outputs from the 175B
                   GPT-3, despite having 100x fewer parameters. Moreover,
                   InstructGPT models show improvements in truthfulness and
                   reductions in toxic output generation while having minimal
                   performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show
                   that fine-tuning with human feedback is a promising direction
                   for aligning language models with human intent.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kaplan2020-vz,
  title         = "Scaling Laws for Neural Language Models",
  author        = "Kaplan, Jared and McCandlish, Sam and Henighan, Tom and
                   Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray,
                   Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario",
  journal       = "arXiv [cs.LG]",
  abstract      = "We study empirical scaling laws for language model
                   performance on the cross-entropy loss. The loss scales as a
                   power-law with model size, dataset size, and the amount of
                   compute used for training, with some trends spanning more
                   than seven orders of magnitude. Other architectural details
                   such as network width or depth have minimal effects within a
                   wide range. Simple equations govern the dependence of
                   overfitting on model/dataset size and the dependence of
                   training speed on model size. These relationships allow us to
                   determine the optimal allocation of a fixed compute budget.
                   Larger models are significantly more sample-efficient, such
                   that optimally compute-efficient training involves training
                   very large models on a relatively modest amount of data and
                   stopping significantly before convergence.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Vaswani2017-pb,
  title     = "Attention is All you Need",
  author    = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
               Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and
               Polosukhin, Illia",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "proceedings.neurips.cc",
  volume    =  30,
  abstract  = "… the number of attention heads and the attention key and value
               dimensions, keeping the amount of computation constant, as
               described in Section 3.2.2. While single-head attention is 0.9 …",
  year      =  2017
}

@BOOK{Goodfellow2016-su,
  title     = "Deep Learning",
  author    = "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron",
  publisher = "MIT Press",
  abstract  = "An introduction to a broad range of topics in deep learning,
               covering mathematical and conceptual background, deep learning
               techniques used in industry, and research perspectives.“Written
               by three experts in the field, Deep Learning is the only
               comprehensive book on the subject.”—Elon Musk, cochair of OpenAI;
               cofounder and CEO of Tesla and SpaceXDeep learning is a form of
               machine learning that enables computers to learn from experience
               and understand the world in terms of a hierarchy of concepts.
               Because the computer gathers knowledge from experience, there is
               no need for a human computer operator to formally specify all the
               knowledge that the computer needs. The hierarchy of concepts
               allows the computer to learn complicated concepts by building
               them out of simpler ones; a graph of these hierarchies would be
               many layers deep. This book introduces a broad range of topics in
               deep learning. The text offers mathematical and conceptual
               background, covering relevant concepts in linear algebra,
               probability theory and information theory, numerical computation,
               and machine learning. It describes deep learning techniques used
               by practitioners in industry, including deep feedforward
               networks, regularization, optimization algorithms, convolutional
               networks, sequence modeling, and practical methodology; and it
               surveys such applications as natural language processing, speech
               recognition, computer vision, online recommendation systems,
               bioinformatics, and videogames. Finally, the book offers research
               perspectives, covering such theoretical topics as linear factor
               models, autoencoders, representation learning, structured
               probabilistic models, Monte Carlo methods, the partition
               function, approximate inference, and deep generative models. Deep
               Learning can be used by undergraduate or graduate students
               planning careers in either industry or research, and by software
               engineers who want to begin using deep learning in their products
               or platforms. A website offers supplementary material for both
               readers and instructors.",
  month     =  nov,
  year      =  2016,
  language  = "en"
}

@INPROCEEDINGS{Engel2019-nj,
  title  = "{GANSynth}: Adversarial Neural Audio Synthesis",
  author = "Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and
            Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam",
  year   =  2019
}

@ARTICLE{Bodily2018-en,
  title    = "Explainability: {An} {Aesthetic} for {Aesthetics} in
              {Computational} {Creative} {Systems}",
  author   = "Bodily, Paul M and Ventura, Dan",
  pages    =  8,
  abstract = "Of continued interest in the ﬁeld of Computational Creativity (CC)
              is the question of what characteristics are required for
              autonomous creativity. Many characteristics have been proposed
              including the possession of an autonomous aesthetic. Paramount to
              the idea of an autonomous aesthetic is the need for a
              meta-aesthetic: an aesthetic which guides the system in selecting
              its own aesthetic. We review how aesthetics have (and have not)
              been used in CC systems to date, including examples of autonomous
              aesthetics. We formalize the idea of a meta-aesthetic in an
              extension of Wiggins’ 2006 framework for describing computational
              systems generally. We propose explainability as an effective
              meta-aesthetic for autonomous creative systems and make some
              comments about the explainability of creativity and of
              explainability itself.",
  year     =  2018,
  language = "en"
}

@ARTICLE{Boden1998-yn,
  title    = "Creativity and artificial intelligence",
  author   = "Boden, Margaret A",
  journal  = "Artif. Intell.",
  volume   =  103,
  number   =  1,
  pages    = "347--356",
  abstract = "Creativity is a fundamental feature of human intelligence, and a
              challenge for AI. AI techniques can be used to create new ideas in
              three ways: by producing novel combinations of familiar ideas; by
              exploring the potential of conceptual spaces; and by making
              transformations that enable the generation of previously
              impossible ideas. AI will have less difficulty in modelling the
              generation of new ideas than in automating their evaluation.",
  series   = "Artificial Intelligence 40 years later",
  month    =  aug,
  year     =  1998,
  language = "en"
}

@ARTICLE{Smith2017-kw,
  title    = "The {Machine} as {Artist}: {An} {Introduction}",
  author   = "Smith, Glenn W and Leymarie, Frederic Fol",
  journal  = "Arts Health",
  volume   =  6,
  number   =  2,
  pages    =  5,
  abstract = "With the understanding that art and technology are continuing to
              experience an historic and rapidly intensifying rapprochement—but
              with the understanding as well that accounts thereof have tended
              to be constrained by scientific/engineering rigor on the one hand,
              or have tended to swing to the opposite extreme—it is the goal of
              this special issue of Arts to provide an opportunity for artists,
              humanists, scientists, and engineers to consider this development
              from the broader perspective which it deserves, while at the same
              time retaining a focus on what must surely be the emerging core of
              our subject: the state of the art in mechatronics and computation
              is such that we can now begin to speak comfortably of the machine
              as artist—and we can begin to hope, as well, that an aesthetic
              sensitivity on the part of the machine might help lead to a
              friendlier and more sensitive machine intelligence in general.",
  month    =  jun,
  year     =  2017,
  keywords = "artificial intelligence, aesthetics, art, embodiment, empathy,
              science, technology",
  language = "en"
}

@ARTICLE{Dunbar2009-kg,
  title    = "Creativity {Evaluation} through {Latent} {Semantic} {Analysis}",
  author   = "Dunbar, Kevin and Forster, Eve",
  journal  = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  volume   =  31,
  number   =  31,
  abstract = "Author(s): Dunbar, Kevin; Forster, Eve",
  year     =  2009,
  language = "en"
}

@ARTICLE{Allen1999-sr,
  title   = "Mixed-initiative interaction",
  author  = "Allen, J E and Guinn, C I and Horvtz, E",
  journal = "IEEE Intelligent Systems and their Applications",
  volume  =  14,
  number  =  5,
  pages   = "14--23",
  year    =  1999
}

@ARTICLE{Roberts2019-ym,
  title    = "A {Hierarchical} {Latent} {Vector} {Model} for {Learning}
              {Long}-{Term} {Structure} in {Music}",
  author   = "Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne,
              Curtis and Eck, Douglas",
  journal  = "arXiv:1803. 05428 [cs, eess, stat]",
  abstract = "The Variational Autoencoder (VAE) has proven to be an effective
              model for producing semantically meaningful latent representations
              for natural data. However, it has thus far seen limited
              application to sequential data, and, as we demonstrate, existing
              recurrent VAE models have difficulty modeling sequences with
              long-term structure. To address this issue, we propose the use of
              a hierarchical decoder, which first outputs embeddings for
              subsequences of the input and then uses these embeddings to
              generate each subsequence independently. This structure encourages
              the model to utilize its latent code, thereby avoiding the
              ``posterior collapse'' problem, which remains an issue for
              recurrent VAEs. We apply this architecture to modeling sequences
              of musical notes and find that it exhibits dramatically better
              sampling, interpolation, and reconstruction performance than a
              ``flat'' baseline model. An implementation of our ``MusicVAE'' is
              available online at http://g.co/magenta/musicvae-code.",
  month    =  nov,
  year     =  2019,
  keywords = "Computer Science - Machine Learning, Statistics - Machine
              Learning, Computer Science - Sound, Electrical Engineering and
              Systems Science - Audio and Speech Processing"
}

@ARTICLE{Engel2017-ig,
  title    = "Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet}
              {Autoencoders}",
  author   = "Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman,
              Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad",
  journal  = "arXiv:1704. 01279 [cs]",
  abstract = "Generative models in vision have seen rapid progress due to
              algorithmic improvements and the availability of high-quality
              image datasets. In this paper, we offer contributions in both
              these areas to enable similar progress in audio modeling. First,
              we detail a powerful new WaveNet-style autoencoder model that
              conditions an autoregressive decoder on temporal codes learned
              from the raw audio waveform. Second, we introduce NSynth, a
              large-scale and high-quality dataset of musical notes that is an
              order of magnitude larger than comparable public datasets. Using
              NSynth, we demonstrate improved qualitative and quantitative
              performance of the WaveNet autoencoder over a well-tuned spectral
              autoencoder baseline. Finally, we show that the model learns a
              manifold of embeddings that allows for morphing between
              instruments, meaningfully interpolating in timbre to create new
              types of sounds that are realistic and expressive.",
  month    =  apr,
  year     =  2017,
  keywords = "Computer Science - Machine Learning, Computer Science - Artificial
              Intelligence, Computer Science - Sound"
}

@ARTICLE{Lubart2005-zi,
  title    = "How can computers be partners in the creative process:
              {Classification} and commentary on the {Special} {Issue}",
  author   = "Lubart, Todd",
  journal  = "Int. J. Hum. Comput. Stud.",
  volume   =  63,
  number   =  4,
  pages    = "365--369",
  abstract = "The different ways that computers can be involved in creative work
              are examined. A classification based on four categories of
              human–computer interaction to promote creativity is proposed:
              computers may facilitate (a) the management of creative work, (b)
              communication between individuals collaborating on creative
              projects, (c) the use of creativity enhancement techniques, (d)
              the creative act through integrated human–computer cooperation
              during idea production. The papers in the Special Issue are
              discussed according to this classification. Issues to be
              considered in future work on human–computer interactions for
              promoting creativity are discussed.",
  series   = "Computer support for creativity",
  month    =  oct,
  year     =  2005,
  keywords = "Creativity, Human–computer interaction",
  language = "en"
}

@ARTICLE{Brown2020-js,
  title    = "Language {Models} are {Few}-{Shot} {Learners}",
  author   = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
              Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan,
              Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and
              Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen
              and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler,
              Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse,
              Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz
              and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner,
              Christopher and McCandlish, Sam and Radford, Alec and Sutskever,
              Ilya and Amodei, Dario",
  journal  = "arXiv:2005. 14165 [cs]",
  abstract = "Recent work has demonstrated substantial gains on many NLP tasks
              and benchmarks by pre-training on a large corpus of text followed
              by fine-tuning on a specific task. While typically task-agnostic
              in architecture, this method still requires task-specific
              fine-tuning datasets of thousands or tens of thousands of
              examples. By contrast, humans can generally perform a new language
              task from only a few examples or from simple instructions -
              something which current NLP systems still largely struggle to do.
              Here we show that scaling up language models greatly improves
              task-agnostic, few-shot performance, sometimes even reaching
              competitiveness with prior state-of-the-art fine-tuning
              approaches. Specifically, we train GPT-3, an autoregressive
              language model with 175 billion parameters, 10x more than any
              previous non-sparse language model, and test its performance in
              the few-shot setting. For all tasks, GPT-3 is applied without any
              gradient updates or fine-tuning, with tasks and few-shot
              demonstrations specified purely via text interaction with the
              model. GPT-3 achieves strong performance on many NLP datasets,
              including translation, question-answering, and cloze tasks, as
              well as several tasks that require on-the-fly reasoning or domain
              adaptation, such as unscrambling words, using a novel word in a
              sentence, or performing 3-digit arithmetic. At the same time, we
              also identify some datasets where GPT-3's few-shot learning still
              struggles, as well as some datasets where GPT-3 faces
              methodological issues related to training on large web corpora.
              Finally, we find that GPT-3 can generate samples of news articles
              which human evaluators have difficulty distinguishing from
              articles written by humans. We discuss broader societal impacts of
              this finding and of GPT-3 in general.",
  month    =  jul,
  year     =  2020,
  keywords = "Computer Science - Computation and Language"
}

@ARTICLE{Radford2019-yy,
  title    = "Language {Models} are {Unsupervised} {Multitask} {Learners}",
  author   = "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
              Amodei, Dario and Sutskever, Ilya",
  pages    =  24,
  abstract = "Natural language processing tasks, such as question answering,
              machine translation, reading comprehension, and summarization, are
              typically approached with supervised learning on taskspeciﬁc
              datasets. We demonstrate that language models begin to learn these
              tasks without any explicit supervision when trained on a new
              dataset of millions of webpages called WebText. When conditioned
              on a document plus questions, the answers generated by the
              language model reach 55 F1 on the CoQA dataset - matching or
              exceeding the performance of 3 out of 4 baseline systems without
              using the 127,000+ training examples. The capacity of the language
              model is essential to the success of zero-shot task transfer and
              increasing it improves performance in a log-linear fashion across
              tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer
              that achieves state of the art results on 7 out of 8 tested
              language modeling datasets in a zero-shot setting but still
              underﬁts WebText. Samples from the model reﬂect these improvements
              and contain coherent paragraphs of text. These ﬁndings suggest a
              promising path towards building language processing systems which
              learn to perform tasks from their naturally occurring
              demonstrations.",
  year     =  2019,
  language = "en"
}

@INPROCEEDINGS{Roberts2018-wn,
  title     = "Magenta.js: {A} {JavaScript} {API} for {Augmenting} {Creativity}
               with {Deep} {Learning}",
  author    = "Roberts, Adam and Hawthorne, Curtis and Simon, Ian",
  booktitle = "Joint {Workshop} on {Machine} {Learning} for {Music} ({ICML})",
  year      =  2018
}

@MISC{Engelbart1962-ir,
  title  = "Augmenting {Human} {Intellect}",
  author = "Engelbart, Douglas",
  year   =  1962
}

@ARTICLE{Shneiderman2020-wm,
  title    = "Bridging the {Gap} {Between} {Ethics} and {Practice}: {Guidelines}
              for {Reliable}, {Safe}, and {Trustworthy} {Human}-centered {AI}
              {Systems}",
  author   = "Shneiderman, Ben",
  journal  = "ACM Transactions on Interactive Intelligent Systems",
  volume   =  10,
  number   =  4,
  pages    = "26:1--26:31",
  abstract = "This article attempts to bridge the gap between widely discussed
              ethical principles of Human-centered AI (HCAI) and practical steps
              for effective governance. Since HCAI systems are developed and
              implemented in multiple organizational structures, I propose 15
              recommendations at three levels of governance: team, organization,
              and industry. The recommendations are intended to increase the
              reliability, safety, and trustworthiness of HCAI systems: (1)
              reliable systems based on sound software engineering practices,
              (2) safety culture through business management strategies, and (3)
              trustworthy certification by independent oversight. Software
              engineering practices within teams include audit trails to enable
              analysis of failures, software engineering workflows, verification
              and validation testing, bias testing to enhance fairness, and
              explainable user interfaces. The safety culture within
              organizations comes from management strategies that include
              leadership commitment to safety, hiring and training oriented to
              safety, extensive reporting of failures and near misses, internal
              review boards for problems and future plans, and alignment with
              industry standard practices. The trustworthiness certification
              comes from industry-wide efforts that include government
              interventions and regulation, accounting firms conducting external
              audits, insurance companies compensating for failures,
              non-governmental and civil society organizations advancing design
              principles, and professional organizations and research institutes
              developing standards, policies, and novel ideas. The larger goal
              of effective governance is to limit the dangers and increase the
              benefits of HCAI to individuals, organizations, and society.",
  month    =  oct,
  year     =  2020,
  keywords = "Artificial Intelligence, design, Human-centered AI, Human-Computer
              Interaction, independent oversight, management strategies,
              reliable, safe, software engineering practices, trustworthy"
}

@ARTICLE{Shneiderman1997-pu,
  title   = "Direct manipulation vs. interface agents",
  author  = "Shneiderman, Ben and Maes, Pattie",
  journal = "Interactions",
  volume  =  4,
  number  =  6,
  pages   = "42--61",
  month   =  nov,
  year    =  1997
}

@ARTICLE{Guzdial2019-gr,
  title    = "An {Interaction} {Framework} for {Studying} {Co}-{Creative} {AI}",
  author   = "Guzdial, Matthew and Riedl, Mark",
  journal  = "arXiv:1903. 09709 [cs]",
  abstract = "Machine learning has been applied to a number of creative,
              design-oriented tasks. However, it remains unclear how to best
              empower human users with these machine learning approaches,
              particularly those users without technical expertise. In this
              paper we propose a general framework for turn-based interaction
              between human users and AI agents designed to support human
              creativity, called \{co-creative systems\}. The framework can be
              used to better understand the space of possible designs of
              co-creative systems and reveal future research directions. We
              demonstrate how to apply this framework in conjunction with a pair
              of recent human subject studies, comparing between the four
              human-AI systems employed in these studies and generating
              hypotheses towards future studies.",
  month    =  mar,
  year     =  2019,
  keywords = "Computer Science - Human-Computer Interaction, Computer Science -
              Artificial Intelligence"
}

@ARTICLE{Shneiderman2007-yh,
  title    = "Creativity support tools: accelerating discovery and innovation",
  author   = "Shneiderman, Ben",
  journal  = "Commun. ACM",
  volume   =  50,
  number   =  12,
  pages    = "20--32",
  abstract = "How can designers of programming interfaces, interactive tools,
              and rich social environments enable more people to be more
              creative more often?",
  month    =  dec,
  year     =  2007
}

@INPROCEEDINGS{Horvitz1999-wh,
  title     = "Principles of mixed-initiative user interfaces",
  author    = "Horvitz, Eric",
  booktitle = "Proceedings of the {SIGCHI} conference on {Human} {Factors} in
               {Computing} {Systems}",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "159--166",
  abstract  = "Recent debate has centered on the relative promise of focusing
               user-interface research on developing new metaphors and tools
               that enhance users abilities to directly manipulate objects
               versus directing effort toward developing interface agents that
               provide automation. In this paper, we review principles that show
               promise for allowing engineers to enhance human-computer
               interaction through an elegant coupling of automated services
               with direct manipulation. Key ideas will be highlighted in terms
               of the Lookout system for scheduling and meeting management.",
  series    = "CHI '99",
  month     =  may,
  year      =  1999,
  keywords  = "decision theory, direct manipulaton, intelligent agents,
               probability, UI design, user modeling"
}

@ARTICLE{Zhu2018-yc,
  title    = "Generative {Visual} {Manipulation} on the {Natural} {Image}
              {Manifold}",
  author   = "Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros,
              Alexei A",
  journal  = "arXiv:1609. 03552 [cs]",
  abstract = "Realistic image manipulation is challenging because it requires
              modifying the image appearance in a user-controlled way, while
              preserving the realism of the result. Unless the user has
              considerable artistic skill, it is easy to ``fall off'' the
              manifold of natural images while editing. In this paper, we
              propose to learn the natural image manifold directly from data
              using a generative adversarial neural network. We then define a
              class of image editing operations, and constrain their output to
              lie on that learned manifold at all times. The model automatically
              adjusts the output keeping all edits as realistic as possible. All
              our manipulations are expressed in terms of constrained
              optimization and are applied in near-real time. We evaluate our
              algorithm on the task of realistic photo manipulation of shape and
              color. The presented method can further be used for changing one
              image to look like the other, as well as generating novel imagery
              from scratch based on user's scribbles.",
  month    =  dec,
  year     =  2018,
  keywords = "Computer Science - Computer Vision and Pattern Recognition"
}

@ARTICLE{Carter2017-br,
  title    = "Using {Artificial} {Intelligence} to {Augment} {Human}
              {Intelligence}",
  author   = "Carter, Shan and Nielsen, Michael",
  journal  = "Distill",
  volume   =  2,
  number   =  12,
  pages    = "e9",
  abstract = "By creating user interfaces which let us work with the
              representations inside machine learning models, we can give people
              new tools for reasoning.",
  month    =  dec,
  year     =  2017,
  language = "en"
}

@ARTICLE{Karimi2018-wi,
  title    = "Evaluating {Creativity} in {Computational} {Co}-{Creative}
              {Systems}",
  author   = "Karimi, Pegah and Grace, Kazjon and Maher, Mary Lou and Davis,
              Nicholas",
  journal  = "arXiv:1807. 09886 [cs]",
  abstract = "This paper provides a framework for evaluating creativity in
              co-creative systems: those that involve computer programs
              collaborating with human users on creative tasks. We situate
              co-creative systems within a broader context of computational
              creativity and explain the unique qualities of these systems. We
              present four main questions that can guide evaluation in
              co-creative systems: Who is evaluating the creativity, what is
              being evaluated, when does evaluation occur and how the evaluation
              is performed. These questions provide a framework for comparing
              how existing co-creative systems evaluate creativity, and we apply
              them to examples of co-creative systems in art, humor, games and
              robotics. We conclude that existing co-creative systems tend to
              focus on evaluating the user experience. Adopting evaluation
              methods from autonomous creative systems may lead to co-creative
              systems that are self-aware and intentional.",
  month    =  jul,
  year     =  2018,
  keywords = "Computer Science - Human-Computer Interaction, Computer Science -
              Artificial Intelligence"
}

@ARTICLE{Tierney2002-xp,
  title    = "Creative {Self}-{Efficacy}: {Its} {Potential} {Antecedents} and
              {Relationship} to {Creative} {Performance}",
  author   = "Tierney, Pamela and Farmer, Steven M",
  journal  = "Acad. Manage. J.",
  volume   =  45,
  number   =  6,
  pages    = "1137--1148",
  abstract = "Using data from two different firms, this study tested a new
              construct, creative self-efficacy, tapping employees' belief) that
              they can be creative in their work roles. Results support the
              discriminant validity of the construct and indicate that job
              tenure, job self-efficacy, supervisor behavior, and job complexity
              contribute to creative efficacy beliefs. Creative self-efficacy
              also predicted creative performance beyond the predictive effects
              of job self-efficacy. Differences in results between white-collar
              and blue-collar samples suggest considerations for both theory and
              practice.",
  month    =  dec,
  year     =  2002
}

@ARTICLE{Lamb2018-ir,
  title    = "Evaluating {Computational} {Creativity}: {An} {Interdisciplinary}
              {Tutorial}",
  author   = "Lamb, Carolyn and Brown, Daniel G and Clarke, Charles L A",
  journal  = "ACM Computing Surveys",
  volume   =  51,
  number   =  2,
  pages    = "28:1--28:34",
  abstract = "This article is a tutorial for researchers who are designing
              software to perform a creative task and want to evaluate their
              system using interdisciplinary theories of creativity. Researchers
              who study human creativity have a great deal to offer
              computational creativity. We summarize perspectives from
              psychology, philosophy, cognitive science, and computer science as
              to how creativity can be measured both in humans and in computers.
              We survey how these perspectives have been used in computational
              creativity research and make recommendations for how they should
              be used.",
  month    =  feb,
  year     =  2018,
  keywords = "Computational aesthetics, computational creativity, digital art"
}

@ARTICLE{Kantosalo2016-hg,
  title    = "From {Isolation} to {Involvement}: {Adapting} {Machine}
              {Creativity} {Software} to {Support} {Human}-{Computer}
              {Co}-{Creation}",
  author   = "Kantosalo, Anna and Toivanen, Jukka M and Xiao, Ping and Toivonen,
              Hannu",
  pages    =  7,
  abstract = "This paper investigates how to transform machine creativity
              systems into interactive tools that support human-computer
              co-creation. We use three case studies to identify common issues
              in this transformation, under the perspective of User-Centered
              Design. We also analyse the interactivity and creative behavior of
              the three platforms in terms of Wiggins’ formalization of
              creativity as a search. We arrive at the conclusion that adapting
              creative software for supporting human-computer cocreation
              requires redesigning some major aspects of the software, which
              guides our on-going project of building an interactive poetry
              composition tool.",
  year     =  2016,
  language = "en"
}

@ARTICLE{Goodfellow2014-jz,
  title    = "Generative {Adversarial} {Networks}",
  author   = "Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu,
              Bing and Warde-Farley, David and Ozair, Sherjil and Courville,
              Aaron and Bengio, Yoshua",
  journal  = "arXiv:1406. 2661 [cs, stat]",
  abstract = "We propose a new framework for estimating generative models via an
              adversarial process, in which we simultaneously train two models:
              a generative model G that captures the data distribution, and a
              discriminative model D that estimates the probability that a
              sample came from the training data rather than G. The training
              procedure for G is to maximize the probability of D making a
              mistake. This framework corresponds to a minimax two-player game.
              In the space of arbitrary functions G and D, a unique solution
              exists, with G recovering the training data distribution and D
              equal to 1/2 everywhere. In the case where G and D are defined by
              multilayer perceptrons, the entire system can be trained with
              backpropagation. There is no need for any Markov chains or
              unrolled approximate inference networks during either training or
              generation of samples. Experiments demonstrate the potential of
              the framework through qualitative and quantitative evaluation of
              the generated samples.",
  month    =  jun,
  year     =  2014,
  keywords = "Computer Science - Machine Learning, Statistics - Machine Learning"
}

@ARTICLE{Shneiderman2020-je,
  title     = "Bridging the Gap Between Ethics and Practice: Guidelines for
               Reliable, Safe, and Trustworthy Human-centered {AI} Systems",
  author    = "Shneiderman, Ben",
  journal   = "ACM Trans. Interact. Intell. Syst.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  10,
  number    =  4,
  pages     = "1--31",
  abstract  = "This article attempts to bridge the gap between widely discussed
               ethical principles of Human-centered AI (HCAI) and practical
               steps for effective governance. Since HCAI systems are developed
               and implemented in multiple organizational structures, I propose
               15 recommendations at three levels of governance: team,
               organization, and industry. The recommendations are intended to
               increase the reliability, safety, and trustworthiness of HCAI
               systems: (1) reliable systems based on sound software engineering
               practices, (2) safety culture through business management
               strategies, and (3) trustworthy certification by independent
               oversight. Software engineering practices within teams include
               audit trails to enable analysis of failures, software engineering
               workflows, verification and validation testing, bias testing to
               enhance fairness, and explainable user interfaces. The safety
               culture within organizations comes from management strategies
               that include leadership commitment to safety, hiring and training
               oriented to safety, extensive reporting of failures and near
               misses, internal review boards for problems and future plans, and
               alignment with industry standard practices. The trustworthiness
               certification comes from industry-wide efforts that include
               government interventions and regulation, accounting firms
               conducting external audits, insurance companies compensating for
               failures, non-governmental and civil society organizations
               advancing design principles, and professional organizations and
               research institutes developing standards, policies, and novel
               ideas. The larger goal of effective governance is to limit the
               dangers and increase the benefits of HCAI to individuals,
               organizations, and society.",
  month     =  oct,
  year      =  2020,
  keywords  = "Human-Computer Interaction, trustworthy, independent oversight,
               safe, design, Artificial Intelligence, software engineering
               practices, reliable, management strategies, Human-centered AI"
}

@ARTICLE{Huang2018-pc,
  title   = "Music Transformer: Generating Music with Long-Term Structure",
  author  = "Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and
             Shazeer, Noam and Hawthorne, Curtis and Dai, Andrew M and Hoffman,
             Matthew D and Eck, Douglas",
  journal = "arXiv preprint arXiv:1809. 04281",
  year    =  2018
}

@ARTICLE{Muller2020-nv,
  title    = "Mixed Initiative Generative {AI} Interfaces: An Analytic Framework
              for Generative {AI} Applications",
  author   = "Muller, Michael and Weisz, Justin D and Geyer, Werner",
  journal  = "computationalcreativity.net",
  abstract = "Recent advances in deep generative models have enabled a broad
              range of use cases, from drug design to music synthesis. Many of
              these applications will require a collaborative effort between
              humans who steer the generative process, and generative models to
              reach the desired outputs. However, our expressive power to
              describe interactions with these models has not kept pace. We
              review frameworks for mixed initiative user interfaces (Horvitz
              1999) and mixed initiative creative interfaces (Deterding et al.
              2017) and identify gaps due to …",
  year     =  2020
}

@ARTICLE{Yannakakis2014-zs,
  title     = "Mixed-initiative co-creativity",
  author    = "Yannakakis, Georgios N and Liapis, Antonios and Alexopoulos,
               Constantine",
  publisher = "Foundations of Digital Games",
  abstract  = "Creating and designing with a machine: do we merely create
               together (co-create) or can a machine truly foster our creativity
               as human creators? When does such co-creation foster the
               co-creativity of both humans and machines? This paper
               investigates the simultaneous and/or iterative process of human
               and computational creators in a mixed-initiative fashion within
               the context of game design and attempts to draw from both theory
               and praxis towards answering the above questions. For this
               purpose, we first discuss the strong links between
               mixed-initiative co-creation and theories of human and
               computational creativity. We then introduce an assessment
               methodology of mixed-initiative co-creativity and, as a proof of
               concept, evaluate Sentient Sketchbook as a co-creation tool for
               game design. Core findings suggest that tools such as Sentient
               Sketchbook are not mere game authoring systems or mere enablers
               of creation but, instead, foster human creativity and realize
               mixed-initiative co-creativity.",
  year      =  2014,
  keywords  = "Artificial intelligence; Algorithms; Lateral thinking;
               conferenceObject",
  language  = "en"
}

@INPROCEEDINGS{Hoffmann2016-mg,
  title     = "On Modeling Human-Computer Co-Creativity",
  author    = "Hoffmann, Oliver",
  booktitle = "Knowledge, Information and Creativity Support Systems",
  publisher = "Springer International Publishing",
  pages     = "37--48",
  abstract  = "Do we have a scientific model of creativity as emerging from
               contributions of computer users, computer systems and their
               interaction? Such a model would require describing the creative
               process in general, conditions for human creativity, the added
               value of human-computer cooperation as well as the role and power
               of computing. All of these topics have been the subject of
               research, but they have been addressed in different research
               communities. Potential obstacles for combining research results
               from research fields such as knowledge engineering and creativity
               research and properties of a general model of Human-Computer
               Co-Creativity are discussed.",
  year      =  2016
}

@INPROCEEDINGS{Davis2013-jy,
  title     = "Human-Computer Co-Creativity: Blending Human and Computational
               Creativity",
  author    = "Davis, Nicholas Mark",
  booktitle = "Ninth Artificial Intelligence and Interactive Digital
               Entertainment Conference",
  publisher = "aaai.org",
  abstract  = "This paper describes a thesis exploring how computer programs can
               collaborate as equals in the artistic creative process. The
               proposed system, CoCo Sketch, encodes some rudimentary stylistic
               rules of abstract sketching and music theory to contribute
               supplemental lines and music while the user sketches. We describe
               a three-part research method that includes defining rudimentary
               stylistic rules for abstract line drawing, exploring the
               interaction design for artistic improvisation with a computer,
               and evaluating how CoCo Sketch affects the artistic creative
               process. We report on the initial results of early investigations
               into artistic style that describe cognitive, perceptual, and
               behavioral processes used in abstract artists making.",
  month     =  nov,
  year      =  2013,
  language  = "en"
}

@INPROCEEDINGS{Karimi2020-cf,
  title     = "Creative sketching partner: an analysis of human-{AI}
               co-creativity",
  author    = "Karimi, Pegah and Rezwana, Jeba and Siddiqui, Safat and Maher,
               Mary Lou and Dehbozorgi, Nasrin",
  booktitle = "Proceedings of the 25th International Conference on Intelligent
               User Interfaces",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "221--230",
  abstract  = "The creative sketching partner (CSP) is a proof of concept
               intelligent interface to inspire designers while sketching in
               response to a specified design task. With this interactive system
               we are studying the effect of an AI model of visual and
               conceptual similarity for selecting the Al's sketch response as
               an inspiration to the current state of the user's sketch.
               Specifically, we are interested in the user's behavior and
               response to an AI partner when engaged in a design task. By
               developing deep learning models of the sketches from a
               large-scale dataset, the user can control the amount of visual
               and conceptual similarity of the AI response when requesting
               inspiration from the CSP. We conducted a study with 50 design
               students to examine the participants' interaction behavior and
               their self reports. The participants' behavior maps into clusters
               that are co-related with three types of design creativity:
               combinatorial, exploratory, and transformational. Our findings
               demonstrate that the tool can facilitate ideation and overcome
               design fixation. In addition, analysis suggests that inspiration
               related to conceptual similarity is more associated with
               transformational creativity and inspiration related to visual
               similarity occurs more frequently during the detailed stages of
               design and is more prevalent with combinatorial creativity.",
  series    = "IUI '20",
  month     =  mar,
  year      =  2020,
  keywords  = "co-creativity, design creativity, collaboration, sketching"
}

@ARTICLE{Crandall2018-ev,
  title     = "Cooperating with machines",
  author    = "Crandall, Jacob W and Oudah, Mayada and {Tennom} and
               Ishowo-Oloko, Fatimah and Abdallah, Sherief and Bonnefon,
               Jean-François and Cebrian, Manuel and Shariff, Azim and Goodrich,
               Michael A and Rahwan, Iyad",
  journal   = "Nat. Commun.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  9,
  number    =  1,
  pages     = "1--12",
  abstract  = "Since Alan Turing envisioned artificial intelligence, technical
               progress has often been measured by the ability to defeat humans
               in zero-sum encounters (e.g., Chess, Poker, or Go). Less
               attention has been given to scenarios in which human–machine
               cooperation is beneficial but non-trivial, such as scenarios in
               which human and machine preferences are neither fully aligned nor
               fully in conflict. Cooperation does not require sheer
               computational power, but instead is facilitated by intuition,
               cultural norms, emotions, signals, and pre-evolved dispositions.
               Here, we develop an algorithm that combines a state-of-the-art
               reinforcement-learning algorithm with mechanisms for signaling.
               We show that this algorithm can cooperate with people and other
               algorithms at levels that rival human cooperation in a variety of
               two-player repeated stochastic games. These results indicate that
               general human–machine cooperation is achievable using a
               non-trivial, but ultimately simple, set of algorithmic
               mechanisms. Artificial intelligence is now superior to humans in
               many fully competitive games, such as Chess, Go, and Poker. Here
               the authors develop a machine-learning algorithm that can
               cooperate effectively with humans when cooperation is beneficial
               but nontrivial, something humans are remarkably good at.",
  month     =  dec,
  year      =  2018,
  language  = "en"
}

@MISC{noauthor_undated-gh,
  title        = "Interfaces as a Scarce Resource - {LessWrong}",
  abstract     = "Outline: • * The first three sections (Don Norman’s Fridge,
                  Interface Design, and When And Why Is It Hard?) cover what we
                  mean by “interface”, what it looks like for interfaces to be
                  scarce, and the…",
  howpublished = "\url{https://www.lesswrong.com/posts/hyShz2ABiKX56j5tJ/interfaces-as-a-scarce-resource}",
  note         = "Accessed: 2022-2-22"
}

@ARTICLE{Dafoe2021-in,
  title    = "Cooperative {AI}: machines must learn to find common ground",
  author   = "Dafoe, Allan and Bachrach, Yoram and Hadfield, Gillian and
              Horvitz, Eric and Larson, Kate and Graepel, Thore",
  journal  = "Nature",
  volume   =  593,
  number   =  7857,
  pages    = "33--36",
  month    =  may,
  year     =  2021,
  keywords = "Computer science; Human behaviour; Machine learning; Society;
              Sociology; Technology",
  language = "en"
}

@ARTICLE{Nichol2021-ne,
  title         = "{GLIDE}: Towards photorealistic image generation and editing
                   with text-guided diffusion models",
  author        = "Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and
                   Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and
                   Sutskever, Ilya and Chen, Mark",
  journal       = "arXiv [cs.CV]",
  abstract      = "Diffusion models have recently been shown to generate
                   high-quality synthetic images, especially when paired with a
                   guidance technique to trade off diversity for fidelity. We
                   explore diffusion models for the problem of text-conditional
                   image synthesis and compare two different guidance
                   strategies: CLIP guidance and classifier-free guidance. We
                   find that the latter is preferred by human evaluators for
                   both photorealism and caption similarity, and often produces
                   photorealistic samples. Samples from a 3.5 billion parameter
                   text-conditional diffusion model using classifier-free
                   guidance are favored by human evaluators to those from
                   DALL-E, even when the latter uses expensive CLIP reranking.
                   Additionally, we find that our models can be fine-tuned to
                   perform image inpainting, enabling powerful text-driven image
                   editing. We train a smaller model on a filtered dataset and
                   release the code and weights at
                   https://github.com/openai/glide-text2im.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INCOLLECTION{Cason2017-zr,
  title     = "Synchronization to music as a tool for enhancing non-verbal
               communication in people with neurological diseases",
  author    = "Cason, Nia and Schiaratura, Loris and Samson, Séverine",
  booktitle = "The Routledge Companion to Embodied Music Interaction",
  publisher = "Routledge",
  pages     = "304--312",
  year      =  2017
}

@MISC{Mota2017-lo,
  title   = "Gestural Interactions in Ensemble Performance",
  author  = "Mota, Davi and Loureiro, Mauricio and Laboissière, Rafael",
  journal = "The Routledge Companion to Embodied Music Interaction",
  pages   = "177--185",
  year    =  2017
}

@MISC{Spada2017-hc,
  title   = "Coupling Music and Motion",
  author  = "Spada, Danilo and Bigand, Emmanuel",
  journal = "The Routledge Companion to Embodied Music Interaction",
  pages   = "261--268",
  year    =  2017
}

@MISC{Moran2017-yw,
  title   = "Agency in Embodied Music Interaction",
  author  = "Moran, Nikki",
  journal = "The Routledge Companion to Embodied Music Interaction",
  pages   = "105--112",
  year    =  2017
}

@ARTICLE{Mitchell2021-yj,
  title         = "Why {AI} is Harder Than We Think",
  author        = "Mitchell, Melanie",
  journal       = "arXiv [cs.AI]",
  abstract      = "Since its beginning in the 1950s, the field of artificial
                   intelligence has cycled several times between periods of
                   optimistic predictions and massive investment (``AI spring'')
                   and periods of disappointment, loss of confidence, and
                   reduced funding (``AI winter''). Even with today's seemingly
                   fast pace of AI breakthroughs, the development of
                   long-promised technologies such as self-driving cars,
                   housekeeping robots, and conversational companions has turned
                   out to be much harder than many people expected. One reason
                   for these repeating cycles is our limited understanding of
                   the nature and complexity of intelligence itself. In this
                   paper I describe four fallacies in common assumptions made by
                   AI researchers, which can lead to overconfident predictions
                   about the field. I conclude by discussing the open questions
                   spurred by these fallacies, including the age-old challenge
                   of imbuing machines with humanlike common sense.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Roddy_undated-ck,
  title    = "Signal to Noise Loops: A Cybernetic Approach to Musical
              Performance with Smart City Data and Generative Music Techniques",
  author   = "Roddy, Stephen",
  abstract = "This article introduces the Signal to Noise Loops project which
              consisted of a series of performances and installations that took
              place worldwide between 2017 and 2021. The project utilised open
              data from a network of Internet of Things sensors placed around
              Dublin City in the context of experimental music performance and
              composition. This was underpinned by a theoretical framework from
              the field of Cybernetics which united and integrated methods and
              approaches from the wide-ranging fields of Data-Driven Music,
              Generative Music, Rhythmanalysis and Smart Cities Research."
}

@INPROCEEDINGS{Bray2016-ff,
  title     = "Applying core interaction design principles to computational
               creativity",
  author    = "Bray, Liam and Bown, Oliver",
  booktitle = "Proceedings of the seventh international conference on
               computational creativity",
  pages     = "93--97",
  year      =  2016
}

@BOOK{Bown2021-os,
  title     = "Beyond the Creative Species: Making Machines That Make Art and
               Music",
  author    = "Bown, Oliver",
  publisher = "MIT Press",
  abstract  = "A multidisciplinary introduction to the field of computational
               creativity, analyzing the impact of advanced generative
               technologies on art and music.As algorithms get smarter, what
               role will computers play in the creation of music, art, and other
               cultural artifacts? Will they be able to create such things from
               the ground up, and will such creations be meaningful? In Beyond
               the Creative Species, Oliver Bown offers a multidisciplinary
               examination of computational creativity, analyzing the impact of
               advanced generative technologies on art and music. Drawing on a
               wide range of disciplines, including artificial intelligence and
               machine learning, design, social theory, the psychology of
               creativity, and creative practice research, Bown argues that to
               understand computational creativity, we must not only consider
               what computationally creative algorithms actually do, but also
               examine creative artistic activity itself.",
  month     =  feb,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Bown2015-ig,
  title     = "Attributing Creative Agency: Are we doing it right?",
  author    = "Bown, O",
  journal   = "ICCC",
  publisher = "axon.cs.byu.edu",
  abstract  = "When contemplating the creativity of computational systems, a
               host of factors have been taken into consideration, many of which
               people have attempted to measure or otherwise operationalise:
               novelty, value, P-creativity versus H-creativity, exploration
               versus …",
  year      =  2015
}

@BOOK{Alemi2020-kl,
  title     = "The Amazing Journey of Reason: from {DNA} to Artificial
               Intelligence",
  author    = "Alemi, Mario",
  publisher = "Springer International Publishing",
  address   = "Cham, Switzerland",
  abstract  = "The Amazing Journey analyzes the latest results in chemistry,
               biology, neuroscience, anthropology and sociology under the light
               of the evolution of intelligence, seen as the ability of
               processing information.",
  series    = "SpringerBriefs in Computer Science",
  year      =  2020
}
